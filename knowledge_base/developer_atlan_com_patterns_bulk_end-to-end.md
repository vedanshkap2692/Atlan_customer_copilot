# End-to-end bulk update¶
(source: https://developer.atlan.com/patterns/bulk/end-to-end/)

Overview

Getting started

Common tasks

Asset-specific

Governance structures

Reference

ToolkitsToolkitsPackagesPackagesRunning exampleDefine via templateRender your packageDevelop your logicTest your logicRelease (GA)Widget referenceTypedefsTypedefsRunning exampleDefine via templateRender your modelTest your modelBind the SDKsWrite integration testTest baseline UXRelease (GA)TestingTesting

ToolkitsToolkitsPackagesPackagesRunning exampleDefine via templateRender your packageDevelop your logicTest your logicRelease (GA)Widget referenceTypedefsTypedefsRunning exampleDefine via templateRender your modelTest your modelBind the SDKsWrite integration testTest baseline UXRelease (GA)TestingTesting

PackagesPackagesRunning exampleDefine via templateRender your packageDevelop your logicTest your logicRelease (GA)Widget reference

Running example

Define via template

Render your package

Develop your logic

Test your logic

Release (GA)

Widget reference

TypedefsTypedefsRunning exampleDefine via templateRender your modelTest your modelBind the SDKsWrite integration testTest baseline UXRelease (GA)

Running example

Define via template

Render your model

Test your model

Bind the SDKs

Write integration test

Test baseline UX

Release (GA)

TestingTesting

Overview

Getting startedGetting startedOther important conceptsDocumentation conventionsIntegration optionsIntegration optionsCLIdbtJavaPythonKotlinScalaClojureGoEventsRaw REST APISite map

Other important concepts

Documentation conventions

Integration optionsIntegration optionsCLIdbtJavaPythonKotlinScalaClojureGoEventsRaw REST API

CLI

dbt

Java

Python

Kotlin

Scala

Clojure

Go

Events

Raw REST API

Site map

Common tasksCommon tasksCommon asset actionsCommon asset actionsCertify assetsManage announcementsChange descriptionChange ownersTag (classify) assetsChange custom metadataLink terms to assetsLink domains to assetsManage asset READMEsAdd asset resourcesManage asset relationships with attributesAsset CRUD operationsAsset CRUD operationsCreate an assetRetrieve an assetUpdate an assetDelete an assetFind and apply suggestionsRestore an assetReview changes to an assetReview accesses of an assetGet all assets that...Get all assets that...Search for assetsSearch examplesLineageLineageManage lineageTraverse lineageBulk updatesBulk updatesCombine multiple operationsUpdate multiple assetsEnd-to-end bulk updateEnd-to-end bulk updateTable of contentsStep-by-step1. Find the assets2. Build-up your changes3. Save them in batchesPipeliningEvent handlingEvent handlingWebhook <> LambdaWebhook <> LambdaSet up LambdaCode your logicDeploy your codeSet up webhookManage your webhook

Common asset actionsCommon asset actionsCertify assetsManage announcementsChange descriptionChange ownersTag (classify) assetsChange custom metadataLink terms to assetsLink domains to assetsManage asset READMEsAdd asset resourcesManage asset relationships with attributes

Certify assets

Manage announcements

Change description

Change owners

Tag (classify) assets

Change custom metadata

Link terms to assets

Link domains to assets

Manage asset READMEs

Add asset resources

Manage asset relationships with attributes

Asset CRUD operationsAsset CRUD operationsCreate an assetRetrieve an assetUpdate an assetDelete an assetFind and apply suggestionsRestore an assetReview changes to an assetReview accesses of an asset

Create an asset

Retrieve an asset

Update an asset

Delete an asset

Find and apply suggestions

Restore an asset

Review changes to an asset

Review accesses of an asset

Get all assets that...Get all assets that...Search for assetsSearch examples

Search for assets

Search examples

LineageLineageManage lineageTraverse lineage

Manage lineage

Traverse lineage

Bulk updatesBulk updatesCombine multiple operationsUpdate multiple assetsEnd-to-end bulk updateEnd-to-end bulk updateTable of contentsStep-by-step1. Find the assets2. Build-up your changes3. Save them in batchesPipelining

Combine multiple operations

Update multiple assets

End-to-end bulk updateEnd-to-end bulk updateTable of contentsStep-by-step1. Find the assets2. Build-up your changes3. Save them in batchesPipelining

Step-by-step1. Find the assets2. Build-up your changes3. Save them in batches

1. Find the assets

2. Build-up your changes

3. Save them in batches

Pipelining

Event handlingEvent handlingWebhook <> LambdaWebhook <> LambdaSet up LambdaCode your logicDeploy your codeSet up webhookManage your webhook

Webhook <> LambdaWebhook <> LambdaSet up LambdaCode your logicDeploy your codeSet up webhookManage your webhook

Set up Lambda

Code your logic

Deploy your code

Set up webhook

Manage your webhook

Asset-specificAsset-specificGlossary operationsGlossary operationsCreate objectsRetrieval by nameCreate a hierarchyCategorize termsTraverse category hierarchyCreating assetsCreating assetsManage relational assetsManage cube assetsManage object store assetsManage object store assetsManage AWS S3 assetsManage Azure Data Lake Storage assetsManage Google Cloud Storage assetsManage BI assetsManage BI assetsManage Google Data Studio assetsManage Preset assetsManage Superset assetsManage API assetsManage file assetsManage Airflow assetsManage Kafka assetsManage Azure Event Hub assetsManage App assetsManage AI assetsManage Insights assetsManage QuickSight assetsManage DocumentDB assetsManage Data Quality assetsManage Data Quality assetsManage Data Quality rulesConnector types and iconsData meshData meshManage data domainsManage data productsData contractsData contractsManage data contracts (via CLI)Manage data contracts (via SDK)Profiling and popularityProfiling and popularityManage column profilingManage popularity

Glossary operationsGlossary operationsCreate objectsRetrieval by nameCreate a hierarchyCategorize termsTraverse category hierarchy

Create objects

Retrieval by name

Create a hierarchy

Categorize terms

Traverse category hierarchy

Creating assetsCreating assetsManage relational assetsManage cube assetsManage object store assetsManage object store assetsManage AWS S3 assetsManage Azure Data Lake Storage assetsManage Google Cloud Storage assetsManage BI assetsManage BI assetsManage Google Data Studio assetsManage Preset assetsManage Superset assetsManage API assetsManage file assetsManage Airflow assetsManage Kafka assetsManage Azure Event Hub assetsManage App assetsManage AI assetsManage Insights assetsManage QuickSight assetsManage DocumentDB assetsManage Data Quality assetsManage Data Quality assetsManage Data Quality rulesConnector types and icons

Manage relational assets

Manage cube assets

Manage object store assetsManage object store assetsManage AWS S3 assetsManage Azure Data Lake Storage assetsManage Google Cloud Storage assets

Manage AWS S3 assets

Manage Azure Data Lake Storage assets

Manage Google Cloud Storage assets

Manage BI assetsManage BI assetsManage Google Data Studio assetsManage Preset assetsManage Superset assets

Manage Google Data Studio assets

Manage Preset assets

Manage Superset assets

Manage API assets

Manage file assets

Manage Airflow assets

Manage Kafka assets

Manage Azure Event Hub assets

Manage App assets

Manage AI assets

Manage Insights assets

Manage QuickSight assets

Manage DocumentDB assets

Manage Data Quality assetsManage Data Quality assetsManage Data Quality rules

Manage Data Quality rules

Connector types and icons

Data meshData meshManage data domainsManage data products

Manage data domains

Manage data products

Data contractsData contractsManage data contracts (via CLI)Manage data contracts (via SDK)

Manage data contracts (via CLI)

Manage data contracts (via SDK)

Profiling and popularityProfiling and popularityManage column profilingManage popularity

Manage column profiling

Manage popularity

Governance structuresGovernance structuresCustom metadataCustom metadataCreate custom metadataRetrieve custom metadataUpdate custom metadataDelete custom metadataManage badgesManage options (enumerations)Tag managementTag managementManage Atlan tagsMonitor propagationAccess controlAccess controlManage personasManage purposesManage policiesAccess eventsAPI token managementRun queries on an assetUsers and groupsUsers and groupsCreate users and groupsRetrieve users and groupsUpdate users and groupsDelete users and groupsManage SSO group mappingPackages and workflowsPackages and workflowsManage workflowsManage workflow schedulesSupported packagesSupported packagesAthena assetsAsset importAsset export (basic)API token connection adminBigQuery assetsConnection deleteConfluent Kafka assetsdbt assetsDynamoDB assetsDatabricks assetsDatabricks minerFivetran enrichmentGlue assetsLooker assetsLineage builderLineage generator (no transformation)MongoDB assetsOracle assetsPostgres assetsPowerBI assetsRedshift assetsRelational assets builderSnowflake assetsSnowflake minerSigma assetsSQL Server assetsTableau assetsFile managementFile management

Custom metadataCustom metadataCreate custom metadataRetrieve custom metadataUpdate custom metadataDelete custom metadataManage badgesManage options (enumerations)

Create custom metadata

Retrieve custom metadata

Update custom metadata

Delete custom metadata

Manage badges

Manage options (enumerations)

Tag managementTag managementManage Atlan tagsMonitor propagation

Manage Atlan tags

Monitor propagation

Access controlAccess controlManage personasManage purposesManage policiesAccess eventsAPI token managementRun queries on an asset

Manage personas

Manage purposes

Manage policies

Access events

API token management

Run queries on an asset

Users and groupsUsers and groupsCreate users and groupsRetrieve users and groupsUpdate users and groupsDelete users and groupsManage SSO group mapping

Create users and groups

Retrieve users and groups

Update users and groups

Delete users and groups

Manage SSO group mapping

Packages and workflowsPackages and workflowsManage workflowsManage workflow schedulesSupported packagesSupported packagesAthena assetsAsset importAsset export (basic)API token connection adminBigQuery assetsConnection deleteConfluent Kafka assetsdbt assetsDynamoDB assetsDatabricks assetsDatabricks minerFivetran enrichmentGlue assetsLooker assetsLineage builderLineage generator (no transformation)MongoDB assetsOracle assetsPostgres assetsPowerBI assetsRedshift assetsRelational assets builderSnowflake assetsSnowflake minerSigma assetsSQL Server assetsTableau assets

Manage workflows

Manage workflow schedules

Supported packagesSupported packagesAthena assetsAsset importAsset export (basic)API token connection adminBigQuery assetsConnection deleteConfluent Kafka assetsdbt assetsDynamoDB assetsDatabricks assetsDatabricks minerFivetran enrichmentGlue assetsLooker assetsLineage builderLineage generator (no transformation)MongoDB assetsOracle assetsPostgres assetsPowerBI assetsRedshift assetsRelational assets builderSnowflake assetsSnowflake minerSigma assetsSQL Server assetsTableau assets

Athena assets

Asset import

Asset export (basic)

API token connection admin

BigQuery assets

Connection delete

Confluent Kafka assets

dbt assets

DynamoDB assets

Databricks assets

Databricks miner

Fivetran enrichment

Glue assets

Looker assets

Lineage builder

Lineage generator (no transformation)

MongoDB assets

Oracle assets

Postgres assets

PowerBI assets

Redshift assets

Relational assets builder

Snowflake assets

Snowflake miner

Sigma assets

SQL Server assets

Tableau assets

File managementFile management

ReferenceReferenceSearchingSearchingQueryingQueryingTerm-level queriesFull text queriesRank feature queriesCompound queriesSearchable fieldsSearchable fieldsCommon search fieldsGlossary-specific search fieldsLimiting detailsSorting search resultsPaging search resultsAggregating search resultsEventsEventsEvent triggersEvent triggersAsset is createdAsset is updatedAsset is deletedCustom metadata is addedCustom metadata is removedAsset is taggedAsset is untaggedLineage is createdEvent typesEvent typesENTITY_CREATEENTITY_UPDATEENTITY_DELETEBUSINESS_ATTRIBUTE_UPDATECLASSIFICATION_ADDCLASSIFICATION_DELETESpecificationsSpecificationsData contract specOpenLineage specTypesTypesCoreCoreReferenceableAssetConnectionCatalogTagTagAttachmentAccess controlAccess controlPersonaPurposeAuthPolicyAuthServiceBusinessPolicyBusinessPolicyExceptionBusinessPolicyIncidentBusinessPolicyLogIncidentLineageLineageColumnProcessBIProcessResourcesResourcesLinkFileReadmeReadmeTemplateBadgeWorkflowsWorkflowsWorkflowWorkflowRunTaskStructsStructsActionAuthPolicyConditionAuthPolicyValidityScheduleAwsCloudWatchMetricAwsTagAzureTagBadgeConditionBusinessPolicyRuleByocSsoConfigColumnValueFrequencyMapDbtMetricFilterDbtJobRunGoogleLabelGoogleTagHistogramKafkaTopicConsumptionMCRuleComparisonMCRuleSchedulePopularityInsightsSourceTagAttachmentSourceTagAttachmentValueSourceTagAttributeStarredDetailsEnumerationsEnumerationsAdfActivityStateADLSAccessTierADLSAccountStatusADLSEncryptionTypesADLSLeaseStateADLSLeaseStatusADLSObjectArchiveStatusADLSObjectTypeADLSPerformanceADLSProvisionStateADLSReplicationTypeADLSStorageKindAPIQueryParamTypeEnumatlas_operationAtlasGlossaryCategoryTypeAtlasGlossaryTermTypeAtlasGlossaryTypeAtlasGlossaryTermAssignmentStatusAtlasGlossaryTermRelationshipStatusAuthPolicyCategoryAuthPolicyResourceCategoryAuthPolicyTypecertificate_statusDataGlossaryDataProductCriticalityDataProductSensitivityDataProductStatusDataProductVisibilityDomoCardTypeDynamoDBSecondaryIndexProjectionTypeDynamoDBStatusfile_typeFivetranConnectorStatusFivetranProcessStatusgoogle_datastudio_asset_typeicon_typeincident_severitykafka_topic_cleanup_policykafka_topic_compression_typematillion_job_typeModelCardinalityTypeMongoDBCollectionValidationActionMongoDBCollectionValidationLevelOpenLineageRunStatepowerbi_endorsementquery_username_strategyquick_sight_analysis_statusquick_sight_dataset_field_typequick_sight_dataset_import_modequick_sight_folder_typeSchemaRegistrySchemaCompatibilitySchemaRegistrySchemaTypeSourceCostUnitTypetable_typeWorkflowRunStatusWorkflowRunTypeWorkflowStatusWorkflowTypeAbstractionsAbstractionsBICloudInsightObjectStoreEventStoreDataQualityMetricNoSQLSchemaRegistryGlossaryGlossaryAtlasGlossaryAtlasGlossaryCategoryAtlasGlossaryTermData meshData meshDataDomainDataProductDataContractStakeholderStakeholderTitleRelational databasesRelational databasesDatabaseSchemaTableViewMaterialisedViewColumnQueryTablePartitionCalculationViewBigqueryTagDatabricksUnityCatalogTagSnowflakeDynamicTableSnowflakePipeSnowflakeStreamSnowflakeTagProcedureFunctionSQLQuery organizationQuery organizationNamespaceCollectionFolderCubesCubesCubeCubeDimensionCubeHierarchyCubeFieldAPIsAPIsAPIPathAPISpecAPIObjectAPIQueryAPIFieldAirflowAirflowAirflowDagAirflowTaskAmazonAmazonAmazon DynamoDBAmazon DynamoDBDynamoDBTableDynamoDBSecondaryIndexDynamoDBGlobalSecondaryIndexDynamoDBLocalSecondaryIndexAWS S3AWS S3S3BucketS3ObjectAmazon QuickSightAmazon QuickSightQuickSightAnalysisQuickSightAnalysisVisualQuickSightDashboardQuickSightDashboardVisualQuickSightDatasetQuickSightDatasetFieldQuickSightFolderAnaplanAnaplanAnaplanWorkspaceAnaplanAppAnaplanPageAnaplanModelAnaplanModuleAnaplanListAnaplanSystemDimensionAnaplanDimensionAnaplanLineItemAnaplanViewAnomaloAnomaloAnomaloCheckAppAppApplicationApplicationFieldMicrosoft AzureMicrosoft AzureAzure Data FactoryAzure Data FactoryAdfActivityAdfDataflowAdfDatasetAdfLinkedserviceAdfPipelineAzure Data Lake StorageAzure Data Lake StorageADLSAccountADLSContainerADLSObjectAzure Event HubAzure Event HubAzureEventHubAzureEventHubConsumerGroupAzure Service BusAzure Service BusAzureServiceBusNamespaceAzureServiceBusSchemaAzureServiceBusTopicCosmos DBCosmos DBCosmosMongoDBAccountCosmosMongoDBCollectionCosmosMongoDBDatabaseCogniteCogniteCognite3DModelCogniteAssetCogniteEventCogniteFileCogniteSequenceCogniteTimeseriesCustomCustomCustomEntityDataverseDataverseDataverseEntityDataverseAttributedbtdbtDbtColumnProcessDbtMetricDbtModelDbtModelColumnDbtProcessDbtSourceDbtTagDbtTestDomoDomoDomoCardDomoDashboardDomoDatasetDomoDatasetColumnDocumentDBDocumentDBDocumentDBCollectionDocumentDBDatabaseFivetranFivetranFivetranConnectorGoogleGoogleGoogle Cloud StorageGoogle Cloud StorageGCSBucketGCSObjectGoogle Data StudioGoogle Data StudioDataStudioAssetIBMIBMIBMCognosCognosCognosDashboardCognosDatasourceCognosExplorationCognosFileCognosFolderCognosModuleCognosPackageCognosReportKafkaKafkaKafkaConsumerGroupKafkaTopicLookerLookerLookerDashboardLookerExploreLookerFieldLookerFolderLookerLookLookerModelLookerProjectLookerQueryLookerTileLookerViewMatillionMatillionMatillionComponentMatillionGroupMatillionJobMatillionProjectMetabaseMetabaseMetabaseCollectionMetabaseDashboardMetabaseQuestionMicroStrategyMicroStrategyMicroStrategyAttributeMicroStrategyCubeMicroStrategyDocumentMicroStrategyDossierMicroStrategyFactMicroStrategyMetricMicroStrategyProjectMicroStrategyReportMicroStrategyVisualizationModeModeModeChartModeCollectionModeQueryModeReportModeWorkspaceModelsModelsModelAttributeModelAttributeAssociationModelDataModelModelEntityModelEntityAssociationModelVersionMongoDBMongoDBMongoDBCollectionMongDBDatabaseMonte CarloMonte CarloMCIncidentMCMonitorPower BIPower BIPower BIPowerBIColumnPowerBIDashboardPowerBIDataflowPowerBIDataflowEntityColumnPowerBIDatasetPowerBIDatasourcePowerBIMeasurePowerBIPagePowerBIReportPowerBITablePowerBITilePowerBIWorkspacePresetPresetPresetChartPresetDashboardPresetDatasetPresetWorkspaceQlikQlikQlikAppQlikChartQlikDatasetQlikSheetQlikSpaceQlikStreamRedashRedashRedashDashboardRedashQueryRedashVisualizationSalesforceSalesforceSalesforceDashboardSalesforceFieldSalesforceObjectSalesforceOrganizationSalesforceReportSaaSSigmaSigmaSigmaWorkbookSigmaPageSigmaDataElementSigmaDataElementFieldSigmaDatasetSigmaDatasetColumnSisenseSisenseSisenseDashboardSisenseDatamodelSisenseDatamodelTableSisenseFolderSisenseWidgetSodaSodaSodaCheckSparkSparkSparkJobSupersetSupersetSupersetChartSupersetDashboardSupersetDatasetTableauTableauTableauCalculatedFieldTableauDashboardTableauDatasourceTableauDatasourceFieldTableauFlowTableauMetricTableauProjectTableauSiteTableauWorkbookTableauWorksheetThoughtSpotThoughtSpotThoughtspotAnswerThoughtspotColumnThoughtspotDashletThoughtspotLiveboardThoughtspotTableThoughtspotViewThoughtspotWorksheetEndpoints

SearchingSearchingQueryingQueryingTerm-level queriesFull text queriesRank feature queriesCompound queriesSearchable fieldsSearchable fieldsCommon search fieldsGlossary-specific search fieldsLimiting detailsSorting search resultsPaging search resultsAggregating search results

QueryingQueryingTerm-level queriesFull text queriesRank feature queriesCompound queries

Term-level queries

Full text queries

Rank feature queries

Compound queries

Searchable fieldsSearchable fieldsCommon search fieldsGlossary-specific search fields

Common search fields

Glossary-specific search fields

Limiting details

Sorting search results

Paging search results

Aggregating search results

EventsEventsEvent triggersEvent triggersAsset is createdAsset is updatedAsset is deletedCustom metadata is addedCustom metadata is removedAsset is taggedAsset is untaggedLineage is createdEvent typesEvent typesENTITY_CREATEENTITY_UPDATEENTITY_DELETEBUSINESS_ATTRIBUTE_UPDATECLASSIFICATION_ADDCLASSIFICATION_DELETE

Event triggersEvent triggersAsset is createdAsset is updatedAsset is deletedCustom metadata is addedCustom metadata is removedAsset is taggedAsset is untaggedLineage is created

Asset is created

Asset is updated

Asset is deleted

Custom metadata is added

Custom metadata is removed

Asset is tagged

Asset is untagged

Lineage is created

Event typesEvent typesENTITY_CREATEENTITY_UPDATEENTITY_DELETEBUSINESS_ATTRIBUTE_UPDATECLASSIFICATION_ADDCLASSIFICATION_DELETE

ENTITY_CREATE

ENTITY_UPDATE

ENTITY_DELETE

BUSINESS_ATTRIBUTE_UPDATE

CLASSIFICATION_ADD

CLASSIFICATION_DELETE

SpecificationsSpecificationsData contract specOpenLineage spec

Data contract spec

OpenLineage spec

TypesTypesCoreCoreReferenceableAssetConnectionCatalogTagTagAttachmentAccess controlAccess controlPersonaPurposeAuthPolicyAuthServiceBusinessPolicyBusinessPolicyExceptionBusinessPolicyIncidentBusinessPolicyLogIncidentLineageLineageColumnProcessBIProcessResourcesResourcesLinkFileReadmeReadmeTemplateBadgeWorkflowsWorkflowsWorkflowWorkflowRunTaskStructsStructsActionAuthPolicyConditionAuthPolicyValidityScheduleAwsCloudWatchMetricAwsTagAzureTagBadgeConditionBusinessPolicyRuleByocSsoConfigColumnValueFrequencyMapDbtMetricFilterDbtJobRunGoogleLabelGoogleTagHistogramKafkaTopicConsumptionMCRuleComparisonMCRuleSchedulePopularityInsightsSourceTagAttachmentSourceTagAttachmentValueSourceTagAttributeStarredDetailsEnumerationsEnumerationsAdfActivityStateADLSAccessTierADLSAccountStatusADLSEncryptionTypesADLSLeaseStateADLSLeaseStatusADLSObjectArchiveStatusADLSObjectTypeADLSPerformanceADLSProvisionStateADLSReplicationTypeADLSStorageKindAPIQueryParamTypeEnumatlas_operationAtlasGlossaryCategoryTypeAtlasGlossaryTermTypeAtlasGlossaryTypeAtlasGlossaryTermAssignmentStatusAtlasGlossaryTermRelationshipStatusAuthPolicyCategoryAuthPolicyResourceCategoryAuthPolicyTypecertificate_statusDataGlossaryDataProductCriticalityDataProductSensitivityDataProductStatusDataProductVisibilityDomoCardTypeDynamoDBSecondaryIndexProjectionTypeDynamoDBStatusfile_typeFivetranConnectorStatusFivetranProcessStatusgoogle_datastudio_asset_typeicon_typeincident_severitykafka_topic_cleanup_policykafka_topic_compression_typematillion_job_typeModelCardinalityTypeMongoDBCollectionValidationActionMongoDBCollectionValidationLevelOpenLineageRunStatepowerbi_endorsementquery_username_strategyquick_sight_analysis_statusquick_sight_dataset_field_typequick_sight_dataset_import_modequick_sight_folder_typeSchemaRegistrySchemaCompatibilitySchemaRegistrySchemaTypeSourceCostUnitTypetable_typeWorkflowRunStatusWorkflowRunTypeWorkflowStatusWorkflowTypeAbstractionsAbstractionsBICloudInsightObjectStoreEventStoreDataQualityMetricNoSQLSchemaRegistryGlossaryGlossaryAtlasGlossaryAtlasGlossaryCategoryAtlasGlossaryTermData meshData meshDataDomainDataProductDataContractStakeholderStakeholderTitleRelational databasesRelational databasesDatabaseSchemaTableViewMaterialisedViewColumnQueryTablePartitionCalculationViewBigqueryTagDatabricksUnityCatalogTagSnowflakeDynamicTableSnowflakePipeSnowflakeStreamSnowflakeTagProcedureFunctionSQLQuery organizationQuery organizationNamespaceCollectionFolderCubesCubesCubeCubeDimensionCubeHierarchyCubeFieldAPIsAPIsAPIPathAPISpecAPIObjectAPIQueryAPIFieldAirflowAirflowAirflowDagAirflowTaskAmazonAmazonAmazon DynamoDBAmazon DynamoDBDynamoDBTableDynamoDBSecondaryIndexDynamoDBGlobalSecondaryIndexDynamoDBLocalSecondaryIndexAWS S3AWS S3S3BucketS3ObjectAmazon QuickSightAmazon QuickSightQuickSightAnalysisQuickSightAnalysisVisualQuickSightDashboardQuickSightDashboardVisualQuickSightDatasetQuickSightDatasetFieldQuickSightFolderAnaplanAnaplanAnaplanWorkspaceAnaplanAppAnaplanPageAnaplanModelAnaplanModuleAnaplanListAnaplanSystemDimensionAnaplanDimensionAnaplanLineItemAnaplanViewAnomaloAnomaloAnomaloCheckAppAppApplicationApplicationFieldMicrosoft AzureMicrosoft AzureAzure Data FactoryAzure Data FactoryAdfActivityAdfDataflowAdfDatasetAdfLinkedserviceAdfPipelineAzure Data Lake StorageAzure Data Lake StorageADLSAccountADLSContainerADLSObjectAzure Event HubAzure Event HubAzureEventHubAzureEventHubConsumerGroupAzure Service BusAzure Service BusAzureServiceBusNamespaceAzureServiceBusSchemaAzureServiceBusTopicCosmos DBCosmos DBCosmosMongoDBAccountCosmosMongoDBCollectionCosmosMongoDBDatabaseCogniteCogniteCognite3DModelCogniteAssetCogniteEventCogniteFileCogniteSequenceCogniteTimeseriesCustomCustomCustomEntityDataverseDataverseDataverseEntityDataverseAttributedbtdbtDbtColumnProcessDbtMetricDbtModelDbtModelColumnDbtProcessDbtSourceDbtTagDbtTestDomoDomoDomoCardDomoDashboardDomoDatasetDomoDatasetColumnDocumentDBDocumentDBDocumentDBCollectionDocumentDBDatabaseFivetranFivetranFivetranConnectorGoogleGoogleGoogle Cloud StorageGoogle Cloud StorageGCSBucketGCSObjectGoogle Data StudioGoogle Data StudioDataStudioAssetIBMIBMIBMCognosCognosCognosDashboardCognosDatasourceCognosExplorationCognosFileCognosFolderCognosModuleCognosPackageCognosReportKafkaKafkaKafkaConsumerGroupKafkaTopicLookerLookerLookerDashboardLookerExploreLookerFieldLookerFolderLookerLookLookerModelLookerProjectLookerQueryLookerTileLookerViewMatillionMatillionMatillionComponentMatillionGroupMatillionJobMatillionProjectMetabaseMetabaseMetabaseCollectionMetabaseDashboardMetabaseQuestionMicroStrategyMicroStrategyMicroStrategyAttributeMicroStrategyCubeMicroStrategyDocumentMicroStrategyDossierMicroStrategyFactMicroStrategyMetricMicroStrategyProjectMicroStrategyReportMicroStrategyVisualizationModeModeModeChartModeCollectionModeQueryModeReportModeWorkspaceModelsModelsModelAttributeModelAttributeAssociationModelDataModelModelEntityModelEntityAssociationModelVersionMongoDBMongoDBMongoDBCollectionMongDBDatabaseMonte CarloMonte CarloMCIncidentMCMonitorPower BIPower BIPower BIPowerBIColumnPowerBIDashboardPowerBIDataflowPowerBIDataflowEntityColumnPowerBIDatasetPowerBIDatasourcePowerBIMeasurePowerBIPagePowerBIReportPowerBITablePowerBITilePowerBIWorkspacePresetPresetPresetChartPresetDashboardPresetDatasetPresetWorkspaceQlikQlikQlikAppQlikChartQlikDatasetQlikSheetQlikSpaceQlikStreamRedashRedashRedashDashboardRedashQueryRedashVisualizationSalesforceSalesforceSalesforceDashboardSalesforceFieldSalesforceObjectSalesforceOrganizationSalesforceReportSaaSSigmaSigmaSigmaWorkbookSigmaPageSigmaDataElementSigmaDataElementFieldSigmaDatasetSigmaDatasetColumnSisenseSisenseSisenseDashboardSisenseDatamodelSisenseDatamodelTableSisenseFolderSisenseWidgetSodaSodaSodaCheckSparkSparkSparkJobSupersetSupersetSupersetChartSupersetDashboardSupersetDatasetTableauTableauTableauCalculatedFieldTableauDashboardTableauDatasourceTableauDatasourceFieldTableauFlowTableauMetricTableauProjectTableauSiteTableauWorkbookTableauWorksheetThoughtSpotThoughtSpotThoughtspotAnswerThoughtspotColumnThoughtspotDashletThoughtspotLiveboardThoughtspotTableThoughtspotViewThoughtspotWorksheet

CoreCoreReferenceableAssetConnectionCatalogTagTagAttachmentAccess controlAccess controlPersonaPurposeAuthPolicyAuthServiceBusinessPolicyBusinessPolicyExceptionBusinessPolicyIncidentBusinessPolicyLogIncidentLineageLineageColumnProcessBIProcessResourcesResourcesLinkFileReadmeReadmeTemplateBadgeWorkflowsWorkflowsWorkflowWorkflowRunTaskStructsStructsActionAuthPolicyConditionAuthPolicyValidityScheduleAwsCloudWatchMetricAwsTagAzureTagBadgeConditionBusinessPolicyRuleByocSsoConfigColumnValueFrequencyMapDbtMetricFilterDbtJobRunGoogleLabelGoogleTagHistogramKafkaTopicConsumptionMCRuleComparisonMCRuleSchedulePopularityInsightsSourceTagAttachmentSourceTagAttachmentValueSourceTagAttributeStarredDetailsEnumerationsEnumerationsAdfActivityStateADLSAccessTierADLSAccountStatusADLSEncryptionTypesADLSLeaseStateADLSLeaseStatusADLSObjectArchiveStatusADLSObjectTypeADLSPerformanceADLSProvisionStateADLSReplicationTypeADLSStorageKindAPIQueryParamTypeEnumatlas_operationAtlasGlossaryCategoryTypeAtlasGlossaryTermTypeAtlasGlossaryTypeAtlasGlossaryTermAssignmentStatusAtlasGlossaryTermRelationshipStatusAuthPolicyCategoryAuthPolicyResourceCategoryAuthPolicyTypecertificate_statusDataGlossaryDataProductCriticalityDataProductSensitivityDataProductStatusDataProductVisibilityDomoCardTypeDynamoDBSecondaryIndexProjectionTypeDynamoDBStatusfile_typeFivetranConnectorStatusFivetranProcessStatusgoogle_datastudio_asset_typeicon_typeincident_severitykafka_topic_cleanup_policykafka_topic_compression_typematillion_job_typeModelCardinalityTypeMongoDBCollectionValidationActionMongoDBCollectionValidationLevelOpenLineageRunStatepowerbi_endorsementquery_username_strategyquick_sight_analysis_statusquick_sight_dataset_field_typequick_sight_dataset_import_modequick_sight_folder_typeSchemaRegistrySchemaCompatibilitySchemaRegistrySchemaTypeSourceCostUnitTypetable_typeWorkflowRunStatusWorkflowRunTypeWorkflowStatusWorkflowTypeAbstractionsAbstractionsBICloudInsightObjectStoreEventStoreDataQualityMetricNoSQLSchemaRegistry

Referenceable

Asset

Connection

Catalog

Tag

TagAttachment

Access controlAccess controlPersonaPurposeAuthPolicyAuthService

Persona

Purpose

AuthPolicy

AuthService

BusinessPolicy

BusinessPolicyException

BusinessPolicyIncident

BusinessPolicyLog

Incident

LineageLineageColumnProcessBIProcess

ColumnProcess

BIProcess

ResourcesResourcesLinkFileReadmeReadmeTemplateBadge

Link

File

Readme

ReadmeTemplate

Badge

WorkflowsWorkflowsWorkflowWorkflowRunTask

Workflow

WorkflowRun

Task

StructsStructsActionAuthPolicyConditionAuthPolicyValidityScheduleAwsCloudWatchMetricAwsTagAzureTagBadgeConditionBusinessPolicyRuleByocSsoConfigColumnValueFrequencyMapDbtMetricFilterDbtJobRunGoogleLabelGoogleTagHistogramKafkaTopicConsumptionMCRuleComparisonMCRuleSchedulePopularityInsightsSourceTagAttachmentSourceTagAttachmentValueSourceTagAttributeStarredDetails

Action

AuthPolicyCondition

AuthPolicyValiditySchedule

AwsCloudWatchMetric

AwsTag

AzureTag

BadgeCondition

BusinessPolicyRule

ByocSsoConfig

ColumnValueFrequencyMap

DbtMetricFilter

DbtJobRun

GoogleLabel

GoogleTag

Histogram

KafkaTopicConsumption

MCRuleComparison

MCRuleSchedule

PopularityInsights

SourceTagAttachment

SourceTagAttachmentValue

SourceTagAttribute

StarredDetails

EnumerationsEnumerationsAdfActivityStateADLSAccessTierADLSAccountStatusADLSEncryptionTypesADLSLeaseStateADLSLeaseStatusADLSObjectArchiveStatusADLSObjectTypeADLSPerformanceADLSProvisionStateADLSReplicationTypeADLSStorageKindAPIQueryParamTypeEnumatlas_operationAtlasGlossaryCategoryTypeAtlasGlossaryTermTypeAtlasGlossaryTypeAtlasGlossaryTermAssignmentStatusAtlasGlossaryTermRelationshipStatusAuthPolicyCategoryAuthPolicyResourceCategoryAuthPolicyTypecertificate_statusDataGlossaryDataProductCriticalityDataProductSensitivityDataProductStatusDataProductVisibilityDomoCardTypeDynamoDBSecondaryIndexProjectionTypeDynamoDBStatusfile_typeFivetranConnectorStatusFivetranProcessStatusgoogle_datastudio_asset_typeicon_typeincident_severitykafka_topic_cleanup_policykafka_topic_compression_typematillion_job_typeModelCardinalityTypeMongoDBCollectionValidationActionMongoDBCollectionValidationLevelOpenLineageRunStatepowerbi_endorsementquery_username_strategyquick_sight_analysis_statusquick_sight_dataset_field_typequick_sight_dataset_import_modequick_sight_folder_typeSchemaRegistrySchemaCompatibilitySchemaRegistrySchemaTypeSourceCostUnitTypetable_typeWorkflowRunStatusWorkflowRunTypeWorkflowStatusWorkflowType

AdfActivityState

ADLSAccessTier

ADLSAccountStatus

ADLSEncryptionTypes

ADLSLeaseState

ADLSLeaseStatus

ADLSObjectArchiveStatus

ADLSObjectType

ADLSPerformance

ADLSProvisionState

ADLSReplicationType

ADLSStorageKind

APIQueryParamTypeEnum

atlas_operation

AtlasGlossaryCategoryType

AtlasGlossaryTermType

AtlasGlossaryType

AtlasGlossaryTermAssignmentStatus

AtlasGlossaryTermRelationshipStatus

AuthPolicyCategory

AuthPolicyResourceCategory

AuthPolicyType

certificate_status

DataGlossary

DataProductCriticality

DataProductSensitivity

DataProductStatus

DataProductVisibility

DomoCardType

DynamoDBSecondaryIndexProjectionType

DynamoDBStatus

file_type

FivetranConnectorStatus

FivetranProcessStatus

google_datastudio_asset_type

icon_type

incident_severity

kafka_topic_cleanup_policy

kafka_topic_compression_type

matillion_job_type

ModelCardinalityType

MongoDBCollectionValidationAction

MongoDBCollectionValidationLevel

OpenLineageRunState

powerbi_endorsement

query_username_strategy

quick_sight_analysis_status

quick_sight_dataset_field_type

quick_sight_dataset_import_mode

quick_sight_folder_type

SchemaRegistrySchemaCompatibility

SchemaRegistrySchemaType

SourceCostUnitType

table_type

WorkflowRunStatus

WorkflowRunType

WorkflowStatus

WorkflowType

AbstractionsAbstractionsBICloudInsightObjectStoreEventStoreDataQualityMetricNoSQLSchemaRegistry

BI

Cloud

Insight

ObjectStore

EventStore

DataQuality

Metric

NoSQL

SchemaRegistry

GlossaryGlossaryAtlasGlossaryAtlasGlossaryCategoryAtlasGlossaryTerm

AtlasGlossary

AtlasGlossaryCategory

AtlasGlossaryTerm

Data meshData meshDataDomainDataProductDataContractStakeholderStakeholderTitle

DataDomain

DataProduct

DataContract

Stakeholder

StakeholderTitle

Relational databasesRelational databasesDatabaseSchemaTableViewMaterialisedViewColumnQueryTablePartitionCalculationViewBigqueryTagDatabricksUnityCatalogTagSnowflakeDynamicTableSnowflakePipeSnowflakeStreamSnowflakeTagProcedureFunctionSQL

Database

Schema

Table

View

MaterialisedView

Column

Query

TablePartition

CalculationView

BigqueryTag

DatabricksUnityCatalogTag

SnowflakeDynamicTable

SnowflakePipe

SnowflakeStream

SnowflakeTag

Procedure

Function

SQL

Query organizationQuery organizationNamespaceCollectionFolder

Namespace

Collection

Folder

CubesCubesCubeCubeDimensionCubeHierarchyCubeField

Cube

CubeDimension

CubeHierarchy

CubeField

APIsAPIsAPIPathAPISpecAPIObjectAPIQueryAPIField

APIPath

APISpec

APIObject

APIQuery

APIField

AirflowAirflowAirflowDagAirflowTask

AirflowDag

AirflowTask

AmazonAmazonAmazon DynamoDBAmazon DynamoDBDynamoDBTableDynamoDBSecondaryIndexDynamoDBGlobalSecondaryIndexDynamoDBLocalSecondaryIndexAWS S3AWS S3S3BucketS3ObjectAmazon QuickSightAmazon QuickSightQuickSightAnalysisQuickSightAnalysisVisualQuickSightDashboardQuickSightDashboardVisualQuickSightDatasetQuickSightDatasetFieldQuickSightFolder

Amazon DynamoDBAmazon DynamoDBDynamoDBTableDynamoDBSecondaryIndexDynamoDBGlobalSecondaryIndexDynamoDBLocalSecondaryIndex

DynamoDBTable

DynamoDBSecondaryIndex

DynamoDBGlobalSecondaryIndex

DynamoDBLocalSecondaryIndex

AWS S3AWS S3S3BucketS3Object

S3Bucket

S3Object

Amazon QuickSightAmazon QuickSightQuickSightAnalysisQuickSightAnalysisVisualQuickSightDashboardQuickSightDashboardVisualQuickSightDatasetQuickSightDatasetFieldQuickSightFolder

QuickSightAnalysis

QuickSightAnalysisVisual

QuickSightDashboard

QuickSightDashboardVisual

QuickSightDataset

QuickSightDatasetField

QuickSightFolder

AnaplanAnaplanAnaplanWorkspaceAnaplanAppAnaplanPageAnaplanModelAnaplanModuleAnaplanListAnaplanSystemDimensionAnaplanDimensionAnaplanLineItemAnaplanView

AnaplanWorkspace

AnaplanApp

AnaplanPage

AnaplanModel

AnaplanModule

AnaplanList

AnaplanSystemDimension

AnaplanDimension

AnaplanLineItem

AnaplanView

AnomaloAnomaloAnomaloCheck

AnomaloCheck

AppAppApplicationApplicationField

Application

ApplicationField

Microsoft AzureMicrosoft AzureAzure Data FactoryAzure Data FactoryAdfActivityAdfDataflowAdfDatasetAdfLinkedserviceAdfPipelineAzure Data Lake StorageAzure Data Lake StorageADLSAccountADLSContainerADLSObjectAzure Event HubAzure Event HubAzureEventHubAzureEventHubConsumerGroupAzure Service BusAzure Service BusAzureServiceBusNamespaceAzureServiceBusSchemaAzureServiceBusTopicCosmos DBCosmos DBCosmosMongoDBAccountCosmosMongoDBCollectionCosmosMongoDBDatabase

Azure Data FactoryAzure Data FactoryAdfActivityAdfDataflowAdfDatasetAdfLinkedserviceAdfPipeline

AdfActivity

AdfDataflow

AdfDataset

AdfLinkedservice

AdfPipeline

Azure Data Lake StorageAzure Data Lake StorageADLSAccountADLSContainerADLSObject

ADLSAccount

ADLSContainer

ADLSObject

Azure Event HubAzure Event HubAzureEventHubAzureEventHubConsumerGroup

AzureEventHub

AzureEventHubConsumerGroup

Azure Service BusAzure Service BusAzureServiceBusNamespaceAzureServiceBusSchemaAzureServiceBusTopic

AzureServiceBusNamespace

AzureServiceBusSchema

AzureServiceBusTopic

Cosmos DBCosmos DBCosmosMongoDBAccountCosmosMongoDBCollectionCosmosMongoDBDatabase

CosmosMongoDBAccount

CosmosMongoDBCollection

CosmosMongoDBDatabase

CogniteCogniteCognite3DModelCogniteAssetCogniteEventCogniteFileCogniteSequenceCogniteTimeseries

Cognite3DModel

CogniteAsset

CogniteEvent

CogniteFile

CogniteSequence

CogniteTimeseries

CustomCustomCustomEntity

CustomEntity

DataverseDataverseDataverseEntityDataverseAttribute

DataverseEntity

DataverseAttribute

dbtdbtDbtColumnProcessDbtMetricDbtModelDbtModelColumnDbtProcessDbtSourceDbtTagDbtTest

DbtColumnProcess

DbtMetric

DbtModel

DbtModelColumn

DbtProcess

DbtSource

DbtTag

DbtTest

DomoDomoDomoCardDomoDashboardDomoDatasetDomoDatasetColumn

DomoCard

DomoDashboard

DomoDataset

DomoDatasetColumn

DocumentDBDocumentDBDocumentDBCollectionDocumentDBDatabase

DocumentDBCollection

DocumentDBDatabase

FivetranFivetranFivetranConnector

FivetranConnector

GoogleGoogleGoogle Cloud StorageGoogle Cloud StorageGCSBucketGCSObjectGoogle Data StudioGoogle Data StudioDataStudioAsset

Google Cloud StorageGoogle Cloud StorageGCSBucketGCSObject

GCSBucket

GCSObject

Google Data StudioGoogle Data StudioDataStudioAsset

DataStudioAsset

IBMIBMIBMCognosCognosCognosDashboardCognosDatasourceCognosExplorationCognosFileCognosFolderCognosModuleCognosPackageCognosReport

CognosCognosCognosDashboardCognosDatasourceCognosExplorationCognosFileCognosFolderCognosModuleCognosPackageCognosReport

CognosDashboard

CognosDatasource

CognosExploration

CognosFile

CognosFolder

CognosModule

CognosPackage

CognosReport

KafkaKafkaKafkaConsumerGroupKafkaTopic

KafkaConsumerGroup

KafkaTopic

LookerLookerLookerDashboardLookerExploreLookerFieldLookerFolderLookerLookLookerModelLookerProjectLookerQueryLookerTileLookerView

LookerDashboard

LookerExplore

LookerField

LookerFolder

LookerLook

LookerModel

LookerProject

LookerQuery

LookerTile

LookerView

MatillionMatillionMatillionComponentMatillionGroupMatillionJobMatillionProject

MatillionComponent

MatillionGroup

MatillionJob

MatillionProject

MetabaseMetabaseMetabaseCollectionMetabaseDashboardMetabaseQuestion

MetabaseCollection

MetabaseDashboard

MetabaseQuestion

MicroStrategyMicroStrategyMicroStrategyAttributeMicroStrategyCubeMicroStrategyDocumentMicroStrategyDossierMicroStrategyFactMicroStrategyMetricMicroStrategyProjectMicroStrategyReportMicroStrategyVisualization

MicroStrategyAttribute

MicroStrategyCube

MicroStrategyDocument

MicroStrategyDossier

MicroStrategyFact

MicroStrategyMetric

MicroStrategyProject

MicroStrategyReport

MicroStrategyVisualization

ModeModeModeChartModeCollectionModeQueryModeReportModeWorkspace

ModeChart

ModeCollection

ModeQuery

ModeReport

ModeWorkspace

ModelsModelsModelAttributeModelAttributeAssociationModelDataModelModelEntityModelEntityAssociationModelVersion

ModelAttribute

ModelAttributeAssociation

ModelDataModel

ModelEntity

ModelEntityAssociation

ModelVersion

MongoDBMongoDBMongoDBCollectionMongDBDatabase

MongoDBCollection

MongDBDatabase

Monte CarloMonte CarloMCIncidentMCMonitor

MCIncident

MCMonitor

Power BIPower BIPower BIPowerBIColumnPowerBIDashboardPowerBIDataflowPowerBIDataflowEntityColumnPowerBIDatasetPowerBIDatasourcePowerBIMeasurePowerBIPagePowerBIReportPowerBITablePowerBITilePowerBIWorkspace

PowerBIColumn

PowerBIDashboard

PowerBIDataflow

PowerBIDataflowEntityColumn

PowerBIDataset

PowerBIDatasource

PowerBIMeasure

PowerBIPage

PowerBIReport

PowerBITable

PowerBITile

PowerBIWorkspace

PresetPresetPresetChartPresetDashboardPresetDatasetPresetWorkspace

PresetChart

PresetDashboard

PresetDataset

PresetWorkspace

QlikQlikQlikAppQlikChartQlikDatasetQlikSheetQlikSpaceQlikStream

QlikApp

QlikChart

QlikDataset

QlikSheet

QlikSpace

QlikStream

RedashRedashRedashDashboardRedashQueryRedashVisualization

RedashDashboard

RedashQuery

RedashVisualization

SalesforceSalesforceSalesforceDashboardSalesforceFieldSalesforceObjectSalesforceOrganizationSalesforceReportSaaS

SalesforceDashboard

SalesforceField

SalesforceObject

SalesforceOrganization

SalesforceReport

SaaS

SigmaSigmaSigmaWorkbookSigmaPageSigmaDataElementSigmaDataElementFieldSigmaDatasetSigmaDatasetColumn

SigmaWorkbook

SigmaPage

SigmaDataElement

SigmaDataElementField

SigmaDataset

SigmaDatasetColumn

SisenseSisenseSisenseDashboardSisenseDatamodelSisenseDatamodelTableSisenseFolderSisenseWidget

SisenseDashboard

SisenseDatamodel

SisenseDatamodelTable

SisenseFolder

SisenseWidget

SodaSodaSodaCheck

SodaCheck

SparkSparkSparkJob

SparkJob

SupersetSupersetSupersetChartSupersetDashboardSupersetDataset

SupersetChart

SupersetDashboard

SupersetDataset

TableauTableauTableauCalculatedFieldTableauDashboardTableauDatasourceTableauDatasourceFieldTableauFlowTableauMetricTableauProjectTableauSiteTableauWorkbookTableauWorksheet

TableauCalculatedField

TableauDashboard

TableauDatasource

TableauDatasourceField

TableauFlow

TableauMetric

TableauProject

TableauSite

TableauWorkbook

TableauWorksheet

ThoughtSpotThoughtSpotThoughtspotAnswerThoughtspotColumnThoughtspotDashletThoughtspotLiveboardThoughtspotTableThoughtspotViewThoughtspotWorksheet

ThoughtspotAnswer

ThoughtspotColumn

ThoughtspotDashlet

ThoughtspotLiveboard

ThoughtspotTable

ThoughtspotView

ThoughtspotWorksheet

Endpoints

Step-by-step1. Find the assets2. Build-up your changes3. Save them in batches

1. Find the assets

2. Build-up your changes

3. Save them in batches

Pipelining

Running example

To walk through this using an example, and to compare and contrast the approaches, imagine you want to:

Mark all views (including materialized views) in a particular schema as verified, unless they already have some certificate.

Change the owner of the same views.



## Step-by-step¶
(source: https://developer.atlan.com/patterns/bulk/end-to-end/)

The usual end-to-end pattern for updating many assets efficiently involves three steps:

Finding the assets you want to update.

Applying your updates to each asset (in-memory).

Sending those changes to Atlan (in batches).

You can do each of these steps in sequence, for example:



### 1. Find the assets¶
(source: https://developer.atlan.com/patterns/bulk/end-to-end/)

1.4.01.1.0

You start by first finding the assets you want to update. This is usually best done through asearch. (For other common examples, have a look at thesearch snippets.)

123456789101112

StringschemaQN="default/snowflake/1662194632/MYDB/MY_SCH";// (1)IndexSearchRequestfindViews=client.assets.select()// (2).where(Asset.QUALIFIED_NAME.startsWith(schemaQN))// (3).where(Asset.TYPE_NAME.in(List.of(View.TYPE_NAME,MaterializedView.TYPE_NAME)))// (4).whereNot(Asset.CERTIFICATE_STATUS.hasAnyValue())// (5).pageSize(100)// (6).includeOnResults(Asset.DESCRIPTION)// (7).includeOnResults(Asset.CERTIFICATE_STATUS).includeOnResults(Asset.OWNER_USERS).toRequest();// (8)IndexSearchResponseresponse=findViews.search();// (9)

StringschemaQN="default/snowflake/1662194632/MYDB/MY_SCH";// (1)IndexSearchRequestfindViews=client.assets.select()// (2).where(Asset.QUALIFIED_NAME.startsWith(schemaQN))// (3).where(Asset.TYPE_NAME.in(List.of(View.TYPE_NAME,MaterializedView.TYPE_NAME)))// (4).whereNot(Asset.CERTIFICATE_STATUS.hasAnyValue())// (5).pageSize(100)// (6).includeOnResults(Asset.DESCRIPTION)// (7).includeOnResults(Asset.CERTIFICATE_STATUS).includeOnResults(Asset.OWNER_USERS).toRequest();// (8)IndexSearchResponseresponse=findViews.search();// (9)

ThequalifiedNameof every view starts with thequalifiedNameof its parent (schema), so we can limit the results to a particular schema by using thequalifiedName.

ThequalifiedNameof every view starts with thequalifiedNameof its parent (schema), so we can limit the results to a particular schema by using thequalifiedName.

qualifiedName

qualifiedName

qualifiedName

To start building up a query with multiple conditions, you can use theselect()helper on any client'sassetsmember.

To start building up a query with multiple conditions, you can use theselect()helper on any client'sassetsmember.

select()

assets

You can chainwhere()methods to define all the conditions the search results must match. You can use the static constants within any given type to select a particular attribute (likeQUALIFIED_NAMEin this example), and then limit results to only those assets whosequalifiedNamestarts with thequalifiedNameof the schema (by using thestartsWith()predicate). In this example, that means only assets that are within this particular schema will be returned as results.

You can chainwhere()methods to define all the conditions the search results must match. You can use the static constants within any given type to select a particular attribute (likeQUALIFIED_NAMEin this example), and then limit results to only those assets whosequalifiedNamestarts with thequalifiedNameof the schema (by using thestartsWith()predicate). In this example, that means only assets that are within this particular schema will be returned as results.

where()

QUALIFIED_NAME

qualifiedName

qualifiedName

startsWith()

Since there could be tables, views, materialized views and columns in this schema — but you only want views and materialized views — you can use theAsset.TYPE_NAME.in()method to restrict results to only views and materialized views.

Since there could be tables, views, materialized views and columns in this schema — but you only want views and materialized views — you can use theAsset.TYPE_NAME.in()method to restrict results to only views and materialized views.

Asset.TYPE_NAME.in()

Since you only want to update views that do not already have a certificate, you can further limit the results using thewhereNot()method. This will exclude any assets where a certificate alreadyhasAnyValue().

Since you only want to update views that do not already have a certificate, you can further limit the results using thewhereNot()method. This will exclude any assets where a certificate alreadyhasAnyValue().

whereNot()

hasAnyValue()

Here you can play around with different page sizes, to further limit API calls by retrieving more results per page.

Here you can play around with different page sizes, to further limit API calls by retrieving more results per page.

Add as many attributes as needed. Each attribute you add here will ensure that detail is included in each search result. So in this example, every view will include its description, certificate, and individual owners. (Limit these attributes to the minimum you need about each view to do your intended work.)

Add as many attributes as needed. Each attribute you add here will ensure that detail is included in each search result. So in this example, every view will include its description, certificate, and individual owners. (Limit these attributes to the minimum you need about each view to do your intended work.)

You can translate the object you've built up into various outputs, for example immediately calculating a count of how many results match or streaming them directly for processing. In this case, thetoRequest()method will give us the resulting set of criteria back as a complete index search request.

You can translate the object you've built up into various outputs, for example immediately calculating a count of how many results match or streaming them directly for processing. In this case, thetoRequest()method will give us the resulting set of criteria back as a complete index search request.

toRequest()

You can then execute the search based on the request.

You can then execute the search based on the request.

12345678910111213141516171819202122

frompyatlan.client.atlanimportAtlanClientfrompyatlan.client.assetimportBatchfrompyatlan.errorsimportAtlanErrorfrompyatlan.model.enumsimportCertificateStatusfrompyatlan.model.fluent_searchimportCompoundQuery,FluentSearchfrompyatlan.model.assetsimportAsset,View,MaterialisedViewclient=AtlanClient()schema_qualified_name="default/snowflake/1662194632/MYDB/MY_SCH"# (1)find_views=(FluentSearch()# (2).where(Asset.QUALIFIED_NAME.startswith(schema_qualified_name))# (3).where(CompoundQuery.asset_types([View,MaterialisedView]))# (4).where(CompoundQuery.active_assets()).where_not(Asset.CERTIFICATE_STATUS.has_any_value())# (5).page_size(100)# (6).include_on_results(Asset.DESCRIPTION)# (7).include_on_results(Asset.CERTIFICATE_STATUS).include_on_results(Asset.OWNER_USERS)).to_request()# (8)response=client.asset.search(find_views)# (9)

frompyatlan.client.atlanimportAtlanClientfrompyatlan.client.assetimportBatchfrompyatlan.errorsimportAtlanErrorfrompyatlan.model.enumsimportCertificateStatusfrompyatlan.model.fluent_searchimportCompoundQuery,FluentSearchfrompyatlan.model.assetsimportAsset,View,MaterialisedViewclient=AtlanClient()schema_qualified_name="default/snowflake/1662194632/MYDB/MY_SCH"# (1)find_views=(FluentSearch()# (2).where(Asset.QUALIFIED_NAME.startswith(schema_qualified_name))# (3).where(CompoundQuery.asset_types([View,MaterialisedView]))# (4).where(CompoundQuery.active_assets()).where_not(Asset.CERTIFICATE_STATUS.has_any_value())# (5).page_size(100)# (6).include_on_results(Asset.DESCRIPTION)# (7).include_on_results(Asset.CERTIFICATE_STATUS).include_on_results(Asset.OWNER_USERS)).to_request()# (8)response=client.asset.search(find_views)# (9)

Thequalified_nameof every view starts with thequalified_nameof its parent (schema), so we can limit the results to a particular schema by using thequalified_name.

Thequalified_nameof every view starts with thequalified_nameof its parent (schema), so we can limit the results to a particular schema by using thequalified_name.

qualified_name

qualified_name

qualified_name

To start building up a query with multiple conditions, you can use aFluentSearch()object.

To start building up a query with multiple conditions, you can use aFluentSearch()object.

FluentSearch()

You can chainwhere()methods to define all the conditions the search results must match. You can use the class variables within any given type to select a particular attribute (likeQUALIFIED_NAMEin this example), and then limit results to only those assets whosequalified_namestarts with thequalified_nameof the schema (by using thestartswith()predicate). In this example, that means only assets that are within this particular schema will be returned as results.

You can chainwhere()methods to define all the conditions the search results must match. You can use the class variables within any given type to select a particular attribute (likeQUALIFIED_NAMEin this example), and then limit results to only those assets whosequalified_namestarts with thequalified_nameof the schema (by using thestartswith()predicate). In this example, that means only assets that are within this particular schema will be returned as results.

where()

QUALIFIED_NAME

qualified_name

qualified_name

startswith()

Since there could be tables, views, materialized views and columns in this schema — but you only want views and materialized views — you can use theCompoundQuery.asset_types()helper method to restrict results to only views and materialized views.

Since there could be tables, views, materialized views and columns in this schema — but you only want views and materialized views — you can use theCompoundQuery.asset_types()helper method to restrict results to only views and materialized views.

CompoundQuery.asset_types()

Since you only want to update views that do not already have a certificate, you can further limit the results using thewhere_not()method. This will exclude any assets where a certificate alreadyhas_any_value().

Since you only want to update views that do not already have a certificate, you can further limit the results using thewhere_not()method. This will exclude any assets where a certificate alreadyhas_any_value().

where_not()

has_any_value()

Here you can play around with different page sizes, to further limit API calls by retrieving more results per page.

Here you can play around with different page sizes, to further limit API calls by retrieving more results per page.

Add as many attributes as needed. Each attribute you add here will ensure that detail is included in each search result. So in this example, every view will include its description, certificate, and individual owners. (Limit these attributes to the minimum you need about each view to do your intended work.)

Add as many attributes as needed. Each attribute you add here will ensure that detail is included in each search result. So in this example, every view will include its description, certificate, and individual owners. (Limit these attributes to the minimum you need about each view to do your intended work.)

You can translate the object you've built up into various outputs, for example immediately calculating a count of how many results match or executing the query to start processing results directly. In this case, theto_request()method will give us the resulting set of criteria back as a complete index search request.

You can translate the object you've built up into various outputs, for example immediately calculating a count of how many results match or executing the query to start processing results directly. In this case, theto_request()method will give us the resulting set of criteria back as a complete index search request.

to_request()

You can then execute the search based on the request.

You can then execute the search based on the request.

123456789101112

valschemaQN="default/snowflake/1662194632/MYDB/MY_SCH"// (1)valfindViews=client.assets.select()// (2).where(Asset.QUALIFIED_NAME.startsWith(schemaQN))// (3).where(Asset.TYPE_NAME.`in`(listOf(View.TYPE_NAME,MaterializedView.TYPE_NAME)))// (4).whereNot(Asset.CERTIFICATE_STATUS.hasAnyValue())// (5).pageSize(100)// (6).includeOnResults(Asset.DESCRIPTION)// (7).includeOnResults(Asset.CERTIFICATE_STATUS).includeOnResults(Asset.OWNER_USERS).toRequest()// (8)valresponse=findViews.search()// (9)

valschemaQN="default/snowflake/1662194632/MYDB/MY_SCH"// (1)valfindViews=client.assets.select()// (2).where(Asset.QUALIFIED_NAME.startsWith(schemaQN))// (3).where(Asset.TYPE_NAME.`in`(listOf(View.TYPE_NAME,MaterializedView.TYPE_NAME)))// (4).whereNot(Asset.CERTIFICATE_STATUS.hasAnyValue())// (5).pageSize(100)// (6).includeOnResults(Asset.DESCRIPTION)// (7).includeOnResults(Asset.CERTIFICATE_STATUS).includeOnResults(Asset.OWNER_USERS).toRequest()// (8)valresponse=findViews.search()// (9)

ThequalifiedNameof every view starts with thequalifiedNameof its parent (schema), so we can limit the results to a particular schema by using thequalifiedName.

ThequalifiedNameof every view starts with thequalifiedNameof its parent (schema), so we can limit the results to a particular schema by using thequalifiedName.

qualifiedName

qualifiedName

qualifiedName

To start building up a query with multiple conditions, you can use theselect()helper on any client'sassetsmember.

To start building up a query with multiple conditions, you can use theselect()helper on any client'sassetsmember.

select()

assets

You can chainwhere()methods to define all the conditions the search results must match. You can use the static constants within any given type to select a particular attribute (likeQUALIFIED_NAMEin this example), and then limit results to only those assets whosequalifiedNamestarts with thequalifiedNameof the schema (by using thestartsWith()predicate). In this example, that means only assets that are within this particular schema will be returned as results.

You can chainwhere()methods to define all the conditions the search results must match. You can use the static constants within any given type to select a particular attribute (likeQUALIFIED_NAMEin this example), and then limit results to only those assets whosequalifiedNamestarts with thequalifiedNameof the schema (by using thestartsWith()predicate). In this example, that means only assets that are within this particular schema will be returned as results.

where()

QUALIFIED_NAME

qualifiedName

qualifiedName

startsWith()

Since there could be tables, views, materialized views and columns in this schema — but you only want views and materialized views — you can use theAsset.TYPE_NAME.in()helper method to restrict results to only views and materialized views.

Since there could be tables, views, materialized views and columns in this schema — but you only want views and materialized views — you can use theAsset.TYPE_NAME.in()helper method to restrict results to only views and materialized views.

Asset.TYPE_NAME.in()

Since you only want to update views that do not already have a certificate, you can further limit the results using thewhereNot()method. This will exclude any assets where a certificate alreadyhasAnyValue().

Since you only want to update views that do not already have a certificate, you can further limit the results using thewhereNot()method. This will exclude any assets where a certificate alreadyhasAnyValue().

whereNot()

hasAnyValue()

Here you can play around with different page sizes, to further limit API calls by retrieving more results per page.

Here you can play around with different page sizes, to further limit API calls by retrieving more results per page.

Add as many attributes as needed. Each attribute you add here will ensure that detail is included in each search result. So in this example, every view will include its description, certificate, and individual owners. (Limit these attributes to the minimum you need about each view to do your intended work.)

Add as many attributes as needed. Each attribute you add here will ensure that detail is included in each search result. So in this example, every view will include its description, certificate, and individual owners. (Limit these attributes to the minimum you need about each view to do your intended work.)

You can translate the object you've built up into various outputs, for example immediately calculating a count of how many results match or streaming them directly for processing. In this case, thetoRequest()method will give us the resulting set of criteria back as a complete index search request.

You can translate the object you've built up into various outputs, for example immediately calculating a count of how many results match or streaming them directly for processing. In this case, thetoRequest()method will give us the resulting set of criteria back as a complete index search request.

toRequest()

You can then execute the search based on the request.

You can then execute the search based on the request.

12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758

{"dsl":{// (1)"query":{"bool":{// (2)"filter":[// (3){"prefix":{// (4)"qualifiedName":{"value":"default/snowflake/1662194632/MYDB/MY_SCH"}}},{"terms":{// (5)"__typeName.keyword":["View","MaterialisedView"]}},{"term":{// (6)"__state":{"value":"ACTIVE"}}}],"must_not":[// (7){"exists":{"field":"certificateStatus"}}]}},"sort":[// (8){"__guid":{"order":"asc"}}],"from":0,// (9)"size":100,"track_total_hits":true},"attributes":[// (10)"description","certificateStatus","ownerUsers"],"suppressLogs":true,"showSearchScore":false,"excludeMeanings":false,"excludeClassifications":false}

{"dsl":{// (1)"query":{"bool":{// (2)"filter":[// (3){"prefix":{// (4)"qualifiedName":{"value":"default/snowflake/1662194632/MYDB/MY_SCH"}}},{"terms":{// (5)"__typeName.keyword":["View","MaterialisedView"]}},{"term":{// (6)"__state":{"value":"ACTIVE"}}}],"must_not":[// (7){"exists":{"field":"certificateStatus"}}]}},"sort":[// (8){"__guid":{"order":"asc"}}],"from":0,// (9)"size":100,"track_total_hits":true},"attributes":[// (10)"description","certificateStatus","ownerUsers"],"suppressLogs":true,"showSearchScore":false,"excludeMeanings":false,"excludeClassifications":false}

Run a search to find the views and materialized views.

Run a search to find the views and materialized views.

To start building up a query with multiple conditions, you can use aboolquery in Elasticsearch.

To start building up a query with multiple conditions, you can use aboolquery in Elasticsearch.

bool

You can use thefiltercriteria to define all the conditions the search results must match in a binary way (either matches or doesn't). This avoids the need to calculate a score for each result.

You can use thefiltercriteria to define all the conditions the search results must match in a binary way (either matches or doesn't). This avoids the need to calculate a score for each result.

filter

ThequalifiedNameof every view starts with thequalifiedNameof its parent (schema), so we can limit the results to a particular schema by using thequalifiedName.

ThequalifiedNameof every view starts with thequalifiedNameof its parent (schema), so we can limit the results to a particular schema by using thequalifiedName.

qualifiedName

qualifiedName

qualifiedName

Since there could be tables, views, materialized views and columns in this schema — but you only want views and materialized views — you can use an exact match on multiple types to restrict results to only views and materialized views.

Since there could be tables, views, materialized views and columns in this schema — but you only want views and materialized views — you can use an exact match on multiple types to restrict results to only views and materialized views.

Searches by default will returnallassets that are found — whether active or archived (soft-deleted). In most cases, you probably only want the active ones.

Searches by default will returnallassets that are found — whether active or archived (soft-deleted). In most cases, you probably only want the active ones.

Since you only want to update views that do not already have a certificate, you can further limit the results using themust_notclause. This will exclude any assets that already have a certificate present.

Since you only want to update views that do not already have a certificate, you can further limit the results using themust_notclause. This will exclude any assets that already have a certificate present.

must_not

When paging through results, you should specify a sort to give a stable set of results across pages. The most reliable sort will be by GUID of the asset, as this is guaranteed to be unique for every asset.

When paging through results, you should specify a sort to give a stable set of results across pages. The most reliable sort will be by GUID of the asset, as this is guaranteed to be unique for every asset.

Here you can play around with different page sizes, to further limit API calls by retrieving more results per page.

Here you can play around with different page sizes, to further limit API calls by retrieving more results per page.

Add as many attributes as needed. Each attribute you add here will ensure that detail is included in each search result. So in this example, every view will include its description, certificate, and individual owners. (Limit these attributes to the minimum you need about each column to do your intended work.)

Add as many attributes as needed. Each attribute you add here will ensure that detail is included in each search result. So in this example, every view will include its description, certificate, and individual owners. (Limit these attributes to the minimum you need about each column to do your intended work.)



### 2. Build-up your changes¶
(source: https://developer.atlan.com/patterns/bulk/end-to-end/)

3.0.01.10.5

Next, you iterate through those results and make the changes you want to each one. Use themultiple operations patternto make multiple changes to each asset.

131415161718192021

AssetBatchbatch=newAssetBatch(client,20);// (1)try{for(Assetresult:response){// (2)Assetrevised=result.trimToRequired()// (3).certificateStatus(CertificateStatus.VERIFIED)// (4).ownerUser("jsmith").build();batch.add(revised);// (5)}

AssetBatchbatch=newAssetBatch(client,20);// (1)try{for(Assetresult:response){// (2)Assetrevised=result.trimToRequired()// (3).certificateStatus(CertificateStatus.VERIFIED)// (4).ownerUser("jsmith").build();batch.add(revised);// (5)}

Create a batch of assets to build-up the changes across multiple assets before applying those changes in Atlan itself.The first parameter defines the Atlan tenant on which the batch will be processedThe second specifies the maximum number of assets to build-up before sending them across to AtlanAdditional parametersBy default (using only the options above) no classifications or custom metadata will be added or changed on the assets in each batch. To also include classifications and custom metadata, you need to use these additional parameters:A third parameter oftrueto replace all classifications on the assets in the batch, which would include removing classifications if none are provided for the assets in the batch itself (orfalseif you still want to ignore classifications)A fourth parameter to control how custom metadata should be handled for the assets:IGNOREany custom metadata changes in the batch,OVERWRITEto replace all custom metadata with what's provided in the batch (including removing custom metadata that already exists on an asset), orMERGEto only add or update custom metadata based on what's in the batch (leaving other existing custom metadata unchanged)a fifth parameter to control whether failures should be captured across batches (true) or ignored (false)a sixth parameter to control whether the batch should only attempt to update assets that already exist (true) or also create assets if they do not yet exist (false)a seventh parameter to control whether details about each created and updated asset across batches should be tracked (true) or ignored (false) — counts will always be keptan eighth parameter to control whether the matching for determining whether an asset already exists should be done in a case-insensitive way (true) or case-sensitively (false)a ninth parameter to control what kind of assets to create, if not running inupdateOnlymode: partial assets (only available in lineage), or full assetsa tenth parameter to control whether the matching for determining whether an asset already exists should be done strictly according to the data type specified (false), or if tables, views and materialized views should be treated interchangeably (true)

Create a batch of assets to build-up the changes across multiple assets before applying those changes in Atlan itself.

The first parameter defines the Atlan tenant on which the batch will be processed

The second specifies the maximum number of assets to build-up before sending them across to Atlan

By default (using only the options above) no classifications or custom metadata will be added or changed on the assets in each batch. To also include classifications and custom metadata, you need to use these additional parameters:

A third parameter oftrueto replace all classifications on the assets in the batch, which would include removing classifications if none are provided for the assets in the batch itself (orfalseif you still want to ignore classifications)

true

false

A fourth parameter to control how custom metadata should be handled for the assets:IGNOREany custom metadata changes in the batch,OVERWRITEto replace all custom metadata with what's provided in the batch (including removing custom metadata that already exists on an asset), orMERGEto only add or update custom metadata based on what's in the batch (leaving other existing custom metadata unchanged)

IGNORE

OVERWRITE

MERGE

a fifth parameter to control whether failures should be captured across batches (true) or ignored (false)

true

false

a sixth parameter to control whether the batch should only attempt to update assets that already exist (true) or also create assets if they do not yet exist (false)

true

false

a seventh parameter to control whether details about each created and updated asset across batches should be tracked (true) or ignored (false) — counts will always be kept

true

false

an eighth parameter to control whether the matching for determining whether an asset already exists should be done in a case-insensitive way (true) or case-sensitively (false)

true

false

a ninth parameter to control what kind of assets to create, if not running inupdateOnlymode: partial assets (only available in lineage), or full assets

updateOnly

a tenth parameter to control whether the matching for determining whether an asset already exists should be done strictly according to the data type specified (false), or if tables, views and materialized views should be treated interchangeably (true)

false

true

This is the pattern for iterating through all results (across pages) covered in theSearching for assetsportion of the SDK documentation.

This is the pattern for iterating through all results (across pages) covered in theSearching for assetsportion of the SDK documentation.

Every asset implements thetrimToRequired()method, which gives you a builder containing only the bare minimum information needed to update that asset.Limit your asset to only what you intend to updateWhen you send an update to Atlan, it will only attempt to change the information you send in your request — leaving any information not in your request as-is (unchanged) on the asset in Atlan. By usingtrimToRequired()you can remove all information you do not want to update, and then chain on only the details youdowant to update.

Every asset implements thetrimToRequired()method, which gives you a builder containing only the bare minimum information needed to update that asset.

trimToRequired()

Limit your asset to only what you intend to update

When you send an update to Atlan, it will only attempt to change the information you send in your request — leaving any information not in your request as-is (unchanged) on the asset in Atlan. By usingtrimToRequired()you can remove all information you do not want to update, and then chain on only the details youdowant to update.

trimToRequired()

In this running example, you are updating the certificate to verified and setting a new owner — so you simply chain those updates onto the trimmed builder.

In this running example, you are updating the certificate to verified and setting a new owner — so you simply chain those updates onto the trimmed builder.

You can then add your (in-memory) modified asset to the batch.Auto-saves as it goesAs long as the number of assets built-up is below the maximum batch size specified when creating the batch, this will simply continue to build up the batch. As soon as you hit the size limit for the batch, though, this same method will call thesave()operation to batch-update all of those assets in a single API call.Remember to flushSince your loop could finish before you reach another full batch, you must always remember toflush()the batch. This will send any remaining assets that were queued up, when the size of the queue did not yet reach the full batch size.

You can then add your (in-memory) modified asset to the batch.

Auto-saves as it goes

As long as the number of assets built-up is below the maximum batch size specified when creating the batch, this will simply continue to build up the batch. As soon as you hit the size limit for the batch, though, this same method will call thesave()operation to batch-update all of those assets in a single API call.

save()

Remember to flush

Since your loop could finish before you reach another full batch, you must always remember toflush()the batch. This will send any remaining assets that were queued up, when the size of the queue did not yet reach the full batch size.

flush()

232425262728293031323334353637383940

batch=Batch(# (1)client=client,max_size=20,replace_atlan_tags=False,custom_metadata_handling=CustomMetadataHandling.IGNORE,capture_failures=False,update_only=False,track=False,case_insensitive=False,table_view_agnostic=False,creation_handling=AssetCreationHandling.FULL,)try:forassetinresponse:# (2)revised=asset.trim_to_required()# (3)revised.certificate_status=CertificateStatus.VERIFIED# (4)revised.owner_users={"jsmith"}batch.add(asset)# (5)

batch=Batch(# (1)client=client,max_size=20,replace_atlan_tags=False,custom_metadata_handling=CustomMetadataHandling.IGNORE,capture_failures=False,update_only=False,track=False,case_insensitive=False,table_view_agnostic=False,creation_handling=AssetCreationHandling.FULL,)try:forassetinresponse:# (2)revised=asset.trim_to_required()# (3)revised.certificate_status=CertificateStatus.VERIFIED# (4)revised.owner_users={"jsmith"}batch.add(asset)# (5)

Create a batch of assets to accumulate changes across multiple 
assets before applying those changes in Atlan itself. TheBatch()takes the following parameters:client: an instance ofAssetClient.max_size: the maximum size of each batch to be processed (per API call).Additional optional parametersBy default (using only the options above) no classifications or custom metadata will be added or changed on the assets in each batch. To also include classifications and custom metadata, you need to use these additional parameters:replace_atlan_tags(default: False): IfTruereplace all classifications (tags) on the assets in the batch, which would include removing classifications (tags) if none are provided for the assets in the batch itself (orFalseif you still want to ignore classifications)custom_metadata_handling(default: CustomMetadataHandling.IGNORE): control how custom metadata should be handled for the assets:IGNOREany custom metadata changes in the batch,OVERWRITEto replace all custom metadata with what's provided in the batch (including removing custom metadata that already exists on an asset), orMERGEto only add or update custom metadata based on what's in the batch (leaving other existing custom metadata unchanged)capture_failures(default: False): control whether failures should be captured across batches (True) or ignored (False)update_only(default: False): control whether the batch should only attempt to update assets that already exist (True) or also create assets if they do not yet exist (False)track(default: False): control whether details about each created and updated asset across batches should be tracked (True) or ignored (False) — counts will always be keptcase_insensitive(default: False): control whether the matching for determining whether an asset already exists should be done in a case-insensitive way (True) or case-sensitively (False)creation_handling(default: AssetCreationHandling.FULL): control what kind of assets to create, if not running inupdate_onlymode;PARTIALassets (only available in lineage), orFULLassetstable_view_agnostic(default: False): control whether the matching for determining whether an asset already exists should be done strictly according to the data type specified (False), or if tables, views and materialized views should be treated interchangeably (True)

Create a batch of assets to accumulate changes across multiple 
assets before applying those changes in Atlan itself. TheBatch()takes the following parameters:

Batch()

client: an instance ofAssetClient.

client

AssetClient

max_size: the maximum size of each batch to be processed (per API call).

max_size

By default (using only the options above) no classifications or custom metadata will be added or changed on the assets in each batch. To also include classifications and custom metadata, you need to use these additional parameters:

replace_atlan_tags(default: False): IfTruereplace all classifications (tags) on the assets in the batch, which would include removing classifications (tags) if none are provided for the assets in the batch itself (orFalseif you still want to ignore classifications)

replace_atlan_tags

True

False

custom_metadata_handling(default: CustomMetadataHandling.IGNORE): control how custom metadata should be handled for the assets:IGNOREany custom metadata changes in the batch,OVERWRITEto replace all custom metadata with what's provided in the batch (including removing custom metadata that already exists on an asset), orMERGEto only add or update custom metadata based on what's in the batch (leaving other existing custom metadata unchanged)

custom_metadata_handling

IGNORE

OVERWRITE

MERGE

capture_failures(default: False): control whether failures should be captured across batches (True) or ignored (False)

capture_failures

True

False

update_only(default: False): control whether the batch should only attempt to update assets that already exist (True) or also create assets if they do not yet exist (False)

update_only

True

False

track(default: False): control whether details about each created and updated asset across batches should be tracked (True) or ignored (False) — counts will always be kept

track

True

False

case_insensitive(default: False): control whether the matching for determining whether an asset already exists should be done in a case-insensitive way (True) or case-sensitively (False)

case_insensitive

True

False

creation_handling(default: AssetCreationHandling.FULL): control what kind of assets to create, if not running inupdate_onlymode;PARTIALassets (only available in lineage), orFULLassets

creation_handling

update_only

PARTIAL

FULL

table_view_agnostic(default: False): control whether the matching for determining whether an asset already exists should be done strictly according to the data type specified (False), or if tables, views and materialized views should be treated interchangeably (True)

table_view_agnostic

False

True

This is the pattern for iterating through all results (across pages) covered in theSearching for assetsportion of the SDK documentation.

This is the pattern for iterating through all results (across pages) covered in theSearching for assetsportion of the SDK documentation.

Every asset implements thetrim_to_required()method, which gives you an object containing only the bare minimum information needed to update that asset.Limit your asset to only what you intend to updateWhen you send an update to Atlan, it will only attempt to change the information you send in your request — leaving any information not in your request as-is (unchanged) on the asset in Atlan. By usingtrimToRequired()you can remove all information you do not want to update, and then chain on only the details youdowant to update.

Every asset implements thetrim_to_required()method, which gives you an object containing only the bare minimum information needed to update that asset.

trim_to_required()

Limit your asset to only what you intend to update

When you send an update to Atlan, it will only attempt to change the information you send in your request — leaving any information not in your request as-is (unchanged) on the asset in Atlan. By usingtrimToRequired()you can remove all information you do not want to update, and then chain on only the details youdowant to update.

trimToRequired()

In this running example, you are updating the certificate to verified and setting a new owner — so you simply add those updates onto the trimmed object.

In this running example, you are updating the certificate to verified and setting a new owner — so you simply add those updates onto the trimmed object.

You can then add your (in-memory) modified asset to the batch.Auto-saves as it goesAs long as the number of assets built-up is below the maximum batch size specified when creating the batch, this will simply continue to build up the batch. As soon as you hit the size limit for the batch, though, this same method will call thesave()operation to batch-update all of those assets in a single API call.Remember to flushSince your loop could finish before you reach another full batch, you must always remember toflush()the batch. This will send any remaining assets that were queued up, when the size of the queue did not yet reach the full batch size.

You can then add your (in-memory) modified asset to the batch.

Auto-saves as it goes

As long as the number of assets built-up is below the maximum batch size specified when creating the batch, this will simply continue to build up the batch. As soon as you hit the size limit for the batch, though, this same method will call thesave()operation to batch-update all of those assets in a single API call.

save()

Remember to flush

Since your loop could finish before you reach another full batch, you must always remember toflush()the batch. This will send any remaining assets that were queued up, when the size of the queue did not yet reach the full batch size.

flush()

131415161718192021

valbatch=AssetBatch(client,20)// (1)try{for(resultinresponse){// (2)valrevised=result.trimToRequired()// (3).certificateStatus(CertificateStatus.VERIFIED)// (4).ownerUser("jsmith").build()batch.add(revised)// (5)}

valbatch=AssetBatch(client,20)// (1)try{for(resultinresponse){// (2)valrevised=result.trimToRequired()// (3).certificateStatus(CertificateStatus.VERIFIED)// (4).ownerUser("jsmith").build()batch.add(revised)// (5)}

Create a batch of assets to build-up the changes across multiple assets before applying those changes in Atlan itself.The first parameter defines the Atlan tenant on which the batch will be processedThe second specifies the maximum number of assets to build-up before sending them across to AtlanAdditional parametersBy default (using only the options above) no classifications or custom metadata will be added or changed on the assets in each batch. To also include classifications and custom metadata, you need to use these additional parameters:A third parameter oftrueto replace all classifications on the assets in the batch, which would include removing classifications if none are provided for the assets in the batch itself (orfalseif you still want to ignore classifications)A fourth parameter to control how custom metadata should be handled for the assets:IGNOREany custom metadata changes in the batch,OVERWRITEto replace all custom metadata with what's provided in the batch (including removing custom metadata that already exists on an asset), orMERGEto only add or update custom metadata based on what's in the batch (leaving other existing custom metadata unchanged)a fifth parameter to control whether failures should be captured across batches (true) or ignored (false)a sixth parameter to control whether the batch should only attempt to update assets that already exist (true) or also create assets if they do not yet exist (false)a seventh parameter to control whether details about each created and updated asset across batches should be tracked (true) or ignored (false) — counts will always be keptan eighth parameter to control whether the matching for determining whether an asset already exists should be done in a case-insensitive way (true) or case-sensitively (false)a ninth parameter to control what kind of assets to create, if not running inupdateOnlymode: partial assets (only available in lineage), or full assetsa tenth parameter to control whether the matching for determining whether an asset already exists should be done strictly according to the data type specified (false), or if tables, views and materialized views should be treated interchangeably (true)

Create a batch of assets to build-up the changes across multiple assets before applying those changes in Atlan itself.

The first parameter defines the Atlan tenant on which the batch will be processed

The second specifies the maximum number of assets to build-up before sending them across to Atlan

By default (using only the options above) no classifications or custom metadata will be added or changed on the assets in each batch. To also include classifications and custom metadata, you need to use these additional parameters:

A third parameter oftrueto replace all classifications on the assets in the batch, which would include removing classifications if none are provided for the assets in the batch itself (orfalseif you still want to ignore classifications)

true

false

A fourth parameter to control how custom metadata should be handled for the assets:IGNOREany custom metadata changes in the batch,OVERWRITEto replace all custom metadata with what's provided in the batch (including removing custom metadata that already exists on an asset), orMERGEto only add or update custom metadata based on what's in the batch (leaving other existing custom metadata unchanged)

IGNORE

OVERWRITE

MERGE

a fifth parameter to control whether failures should be captured across batches (true) or ignored (false)

true

false

a sixth parameter to control whether the batch should only attempt to update assets that already exist (true) or also create assets if they do not yet exist (false)

true

false

a seventh parameter to control whether details about each created and updated asset across batches should be tracked (true) or ignored (false) — counts will always be kept

true

false

an eighth parameter to control whether the matching for determining whether an asset already exists should be done in a case-insensitive way (true) or case-sensitively (false)

true

false

a ninth parameter to control what kind of assets to create, if not running inupdateOnlymode: partial assets (only available in lineage), or full assets

updateOnly

a tenth parameter to control whether the matching for determining whether an asset already exists should be done strictly according to the data type specified (false), or if tables, views and materialized views should be treated interchangeably (true)

false

true

This is the pattern for iterating through all results (across pages) covered in theSearching for assetsportion of the SDK documentation.

This is the pattern for iterating through all results (across pages) covered in theSearching for assetsportion of the SDK documentation.

Every asset implements thetrimToRequired()method, which gives you a builder containing only the bare minimum information needed to update that asset.Limit your asset to only what you intend to updateWhen you send an update to Atlan, it will only attempt to change the information you send in your request — leaving any information not in your request as-is (unchanged) on the asset in Atlan. By usingtrimToRequired()you can remove all information you do not want to update, and then chain on only the details youdowant to update.

Every asset implements thetrimToRequired()method, which gives you a builder containing only the bare minimum information needed to update that asset.

trimToRequired()

Limit your asset to only what you intend to update

When you send an update to Atlan, it will only attempt to change the information you send in your request — leaving any information not in your request as-is (unchanged) on the asset in Atlan. By usingtrimToRequired()you can remove all information you do not want to update, and then chain on only the details youdowant to update.

trimToRequired()

In this running example, you are updating the certificate to verified and setting a new owner — so you simply chain those updates onto the trimmed builder.

In this running example, you are updating the certificate to verified and setting a new owner — so you simply chain those updates onto the trimmed builder.

You can then add your (in-memory) modified asset to the batch.Auto-saves as it goesAs long as the number of assets built-up is below the maximum batch size specified when creating the batch, this will simply continue to build up the batch. As soon as you hit the size limit for the batch, though, this same method will call thesave()operation to batch-update all of those assets in a single API call.Remember to flushSince your loop could finish before you reach another full batch, you must always remember toflush()the batch. This will send any remaining assets that were queued up, when the size of the queue did not yet reach the full batch size.

You can then add your (in-memory) modified asset to the batch.

Auto-saves as it goes

As long as the number of assets built-up is below the maximum batch size specified when creating the batch, this will simply continue to build up the batch. As soon as you hit the size limit for the batch, though, this same method will call thesave()operation to batch-update all of those assets in a single API call.

save()

Remember to flush

Since your loop could finish before you reach another full batch, you must always remember toflush()the batch. This will send any remaining assets that were queued up, when the size of the queue did not yet reach the full batch size.

flush()

Up to your own code

There are no API calls to make to change the results in-memory. How you implement this will be entirely up to how you are writing your code.



### 3. Save them in batches¶
(source: https://developer.atlan.com/patterns/bulk/end-to-end/)

3.0.01.1.0

Finally, send the changes you have queued up in batches. Use themultiple assets patternto update multiple assets at the same time.

22232425

batch.flush();// (1)}catch(AtlanExceptione){// (2)}

batch.flush();// (1)}catch(AtlanExceptione){// (2)}

TheAssetBatch'sadd()method used in the previous step will automatically save as its internal queue of assets reaches a full batch size.Remember to flushHowever, since your loop could finish before you reach another full batch, you must always remember toflush()the batch. This will send any remaining assets that were queued up.

TheAssetBatch'sadd()method used in the previous step will automatically save as its internal queue of assets reaches a full batch size.

AssetBatch

add()

Remember to flush

However, since your loop could finish before you reach another full batch, you must always remember toflush()the batch. This will send any remaining assets that were queued up.

flush()

Both the.add()and.flush()operations of theAssetBatchcould send a request over to Atlan. Either can therefore also run into trouble and raise an error through anAtlanException. It is up to you to handle such potential errors as you see fit.

Both the.add()and.flush()operations of theAssetBatchcould send a request over to Atlan. Either can therefore also run into trouble and raise an error through anAtlanException. It is up to you to handle such potential errors as you see fit.

.add()

.flush()

AssetBatch

AtlanException

303132

batch.flush()# (1)exceptAtlanErroraserr:...# (2)

batch.flush()# (1)exceptAtlanErroraserr:...# (2)

TheBatch'sadd()method used in the previous step will automatically save as its internal queue of assets reaches a full batch size.Remember to flushHowever, since your loop could finish before you reach another full batch, you must always remember toflush()the batch. This will send any remaining assets that were queued up.

TheBatch'sadd()method used in the previous step will automatically save as its internal queue of assets reaches a full batch size.

Batch

add()

Remember to flush

However, since your loop could finish before you reach another full batch, you must always remember toflush()the batch. This will send any remaining assets that were queued up.

flush()

Both the.add()and.flush()operations of theBatchcould send a request over to Atlan. Either can therefore also run into trouble and raise an error through anAtlanError. It is up to you to handle such potential errors as you see fit.

Both the.add()and.flush()operations of theBatchcould send a request over to Atlan. Either can therefore also run into trouble and raise an error through anAtlanError. It is up to you to handle such potential errors as you see fit.

.add()

.flush()

Batch

AtlanError

22232425

batch.flush()// (1)}catch(e:AtlanException){// (2)}

batch.flush()// (1)}catch(e:AtlanException){// (2)}

TheAssetBatch'sadd()method used in the previous step will automatically save as its internal queue of assets reaches a full batch size.Remember to flushHowever, since your loop could finish before you reach another full batch, you must always remember toflush()the batch. This will send any remaining assets that were queued up.

TheAssetBatch'sadd()method used in the previous step will automatically save as its internal queue of assets reaches a full batch size.

AssetBatch

add()

Remember to flush

However, since your loop could finish before you reach another full batch, you must always remember toflush()the batch. This will send any remaining assets that were queued up.

flush()

Both the.add()and.flush()operations of theAssetBatchcould send a request over to Atlan. Either can therefore also run into trouble and raise an error through anAtlanException. It is up to you to handle such potential errors as you see fit.

Both the.add()and.flush()operations of theAssetBatchcould send a request over to Atlan. Either can therefore also run into trouble and raise an error through anAtlanException. It is up to you to handle such potential errors as you see fit.

.add()

.flush()

AssetBatch

AtlanException

12345678910111213141516171819202122

{"entities":[// (1){"typeName":"View",// (2)"attributes":{"name":"VIEW1",// (3)"qualifiedName":"default/snowflake/1662194632/MYDB/MY_SCH/VIEW1","certificateStatus":"VERIFIED",// (4)"ownerUsers":["jsmith"]}},{// (5)"typeName":"MaterialisedView","attributes":{"name":"MVIEW2","qualifiedName":"default/snowflake/1662194632/MYDB/MY_SCH/MVIEW2","certificateStatus":"VERIFIED","ownerUsers":["jsmith"]}}]}

{"entities":[// (1){"typeName":"View",// (2)"attributes":{"name":"VIEW1",// (3)"qualifiedName":"default/snowflake/1662194632/MYDB/MY_SCH/VIEW1","certificateStatus":"VERIFIED",// (4)"ownerUsers":["jsmith"]}},{// (5)"typeName":"MaterialisedView","attributes":{"name":"MVIEW2","qualifiedName":"default/snowflake/1662194632/MYDB/MY_SCH/MVIEW2","certificateStatus":"VERIFIED","ownerUsers":["jsmith"]}}]}

All details must still be included in an outerentitiesarray.

entities

You need to specify the type for each asset you are updating.

You need to specify other required attributes for each asset, such as its name and qualifiedName.

Add on any other attributes or relationships you want to set on the asset, such as in the running example a verified certificate and new individual owner.

Add another object to the payload to represent another asset that should be updated by the same API call. Once again specify all the required information for that kind of asset, and any of the details for attributes or relationships you want to set.



## Pipelining¶
(source: https://developer.atlan.com/patterns/bulk/end-to-end/)

3.0.01.10.5

Alternatively, when using an SDK, you can pipeline these operations together. The pipeline will run just as efficiently as the step-by-step approach above:

Pushing down the criteria to run as a search on Atlan

Lazily-fetching each page of results

Batching up and bulk-saving changes

12345678910111213141516171819202122232425

StringschemaQN="default/snowflake/1662194632/MYDB/MY_SCH";// (1)try(ParallelBatchbatch=newParallelBatch(client,20)){// (2)client.assets.select()// (3).where(Asset.QUALIFIED_NAME.startsWith(schemaQN))// (4).where(Asset.TYPE_NAME.in(List.of(View.TYPE_NAME,MaterializedView.TYPE_NAME)))// (5).whereNot(Asset.CERTIFICATE_STATUS.hasAnyValue())// (6).pageSize(100)// (7).includeOnResults(Asset.DESCRIPTION)// (8).includeOnResults(Asset.CERTIFICATE_STATUS).includeOnResults(Asset.OWNER_USERS).stream(true)// (9).forEach(result->{// (10)try{batch.add(result.trimToRequired()// (11).certificateStatus(CertificateStatus.VERIFIED)// (12).ownerUser("jsmith").build());// (13)}catch(AtlanExceptione){// (14)log.error("Unable to update: {}",result.getQualifiedName());}});batch.flush();// (15)log.info("Created: {}",batch.getCreated().size());log.info("Updated: {}",batch.getUpdated().size());}

StringschemaQN="default/snowflake/1662194632/MYDB/MY_SCH";// (1)try(ParallelBatchbatch=newParallelBatch(client,20)){// (2)client.assets.select()// (3).where(Asset.QUALIFIED_NAME.startsWith(schemaQN))// (4).where(Asset.TYPE_NAME.in(List.of(View.TYPE_NAME,MaterializedView.TYPE_NAME)))// (5).whereNot(Asset.CERTIFICATE_STATUS.hasAnyValue())// (6).pageSize(100)// (7).includeOnResults(Asset.DESCRIPTION)// (8).includeOnResults(Asset.CERTIFICATE_STATUS).includeOnResults(Asset.OWNER_USERS).stream(true)// (9).forEach(result->{// (10)try{batch.add(result.trimToRequired()// (11).certificateStatus(CertificateStatus.VERIFIED)// (12).ownerUser("jsmith").build());// (13)}catch(AtlanExceptione){// (14)log.error("Unable to update: {}",result.getQualifiedName());}});batch.flush();// (15)log.info("Created: {}",batch.getCreated().size());log.info("Updated: {}",batch.getUpdated().size());}

ThequalifiedNameof every view starts with thequalifiedNameof its parent (schema), so we can limit the results to a particular schema by using thequalifiedName.

ThequalifiedNameof every view starts with thequalifiedNameof its parent (schema), so we can limit the results to a particular schema by using thequalifiedName.

qualifiedName

qualifiedName

qualifiedName

Create a batch of assets to build-up the changes across multiple assets before applying those changes in Atlan itself. When parallel-processing (see further notes on thestream(true)) you need to use a parallel-capableParallelBatch:The first parameter defines the Atlan tenant on which the batch will be processedThe second specifies the maximum number of assets to build-up before sending them across to AtlanAdditional parametersBy default (using only the options above) no classifications or custom metadata will be added or changed on the assets in each batch. To also include classifications and custom metadata, you need to use these additional parameters:A third parameter oftrueto replace all classifications on the assets in the batch, which would include removing classifications if none are provided for the assets in the batch itself (orfalseif you still want to ignore classifications)A fourth parameter to control how custom metadata should be handled for the assets:IGNOREany custom metadata changes in the batch,OVERWRITEto replace all custom metadata with what's provided in the batch (including removing custom metadata that already exists on an asset), orMERGEto only add or update custom metadata based on what's in the batch (leaving other existing custom metadata unchanged)a fifth parameter to control whether failures should be captured across batches (true) or ignored (false)a sixth parameter to control whether the batch should only attempt to update assets that already exist (true) or also create assets if they do not yet exist (false)a seventh parameter to control whether details about each created and updated asset across batches should be tracked (true) or ignored (false) — counts will always be keptan eighth parameter to control whether the matching for determining whether an asset already exists should be done in a case-insensitive way (true) or case-sensitively (false)a ninth parameter to control what kind of assets to create, if not running inupdateOnlymode: partial assets (only available in lineage), or full assetsa tenth parameter to control whether the matching for determining whether an asset already exists should be done strictly according to the data type specified (false), or if tables, views and materialized views should be treated interchangeably (true)

Create a batch of assets to build-up the changes across multiple assets before applying those changes in Atlan itself. When parallel-processing (see further notes on thestream(true)) you need to use a parallel-capableParallelBatch:

stream(true)

ParallelBatch

The first parameter defines the Atlan tenant on which the batch will be processed

The second specifies the maximum number of assets to build-up before sending them across to Atlan

By default (using only the options above) no classifications or custom metadata will be added or changed on the assets in each batch. To also include classifications and custom metadata, you need to use these additional parameters:

A third parameter oftrueto replace all classifications on the assets in the batch, which would include removing classifications if none are provided for the assets in the batch itself (orfalseif you still want to ignore classifications)

true

false

A fourth parameter to control how custom metadata should be handled for the assets:IGNOREany custom metadata changes in the batch,OVERWRITEto replace all custom metadata with what's provided in the batch (including removing custom metadata that already exists on an asset), orMERGEto only add or update custom metadata based on what's in the batch (leaving other existing custom metadata unchanged)

IGNORE

OVERWRITE

MERGE

a fifth parameter to control whether failures should be captured across batches (true) or ignored (false)

true

false

a sixth parameter to control whether the batch should only attempt to update assets that already exist (true) or also create assets if they do not yet exist (false)

true

false

a seventh parameter to control whether details about each created and updated asset across batches should be tracked (true) or ignored (false) — counts will always be kept

true

false

an eighth parameter to control whether the matching for determining whether an asset already exists should be done in a case-insensitive way (true) or case-sensitively (false)

true

false

a ninth parameter to control what kind of assets to create, if not running inupdateOnlymode: partial assets (only available in lineage), or full assets

updateOnly

a tenth parameter to control whether the matching for determining whether an asset already exists should be done strictly according to the data type specified (false), or if tables, views and materialized views should be treated interchangeably (true)

false

true

You can then start defining a pipeline directly against the client'sassetsby using theselect()method.Including archived (soft-deleted) assetsSearches by default will returnallassets that are found — whether active or archived (soft-deleted). In most cases, you probably only want the active ones, so this is the default behavior ofselect(). Sending intrueto thisselect()method will start the pipeline toincludeany archived (soft-deleted) assets in the results, if you do want them.

You can then start defining a pipeline directly against the client'sassetsby using theselect()method.

assets

select()

Including archived (soft-deleted) assets

Searches by default will returnallassets that are found — whether active or archived (soft-deleted). In most cases, you probably only want the active ones, so this is the default behavior ofselect(). Sending intrueto thisselect()method will start the pipeline toincludeany archived (soft-deleted) assets in the results, if you do want them.

select()

true

select()

You can chain as manywhere()methods as you want to define all the conditions the search results must match. You can use the static constants within any given type to select a particular attribute (likeQUALIFIED_NAMEin this example), and then limit results to only those assets whosequalifiedNamestarts with thequalifiedNameof the schema (by using thestartsWith()predicate). In this example, that means only assets that are within this particular schema will be returned as results.

You can chain as manywhere()methods as you want to define all the conditions the search results must match. You can use the static constants within any given type to select a particular attribute (likeQUALIFIED_NAMEin this example), and then limit results to only those assets whosequalifiedNamestarts with thequalifiedNameof the schema (by using thestartsWith()predicate). In this example, that means only assets that are within this particular schema will be returned as results.

where()

QUALIFIED_NAME

qualifiedName

qualifiedName

startsWith()

Since there could be tables, views, materialized views and columns in this schema — but you only want views and materialized views — you can use theAsset.TYPE_NAME.in()method to restrict results to only views and materialized views.

Since there could be tables, views, materialized views and columns in this schema — but you only want views and materialized views — you can use theAsset.TYPE_NAME.in()method to restrict results to only views and materialized views.

Asset.TYPE_NAME.in()

Since you only want to update views that do not already have a certificate, you can further limit the results using thewhereNot()method. This will exclude any assets where a certificate alreadyhasAnyValue().

Since you only want to update views that do not already have a certificate, you can further limit the results using thewhereNot()method. This will exclude any assets where a certificate alreadyhasAnyValue().

whereNot()

hasAnyValue()

(Optional) You can play around with different page sizes, to further limit API calls by retrieving more results per page.

(Optional) You can play around with different page sizes, to further limit API calls by retrieving more results per page.

Add as many attributes as needed. Each attribute you add here will ensure that detail is included in each search result. So in this example, every view will include its description, certificate, and individual owners. (Limit these attributes to the minimum you need about each view to do your intended work.)

Add as many attributes as needed. Each attribute you add here will ensure that detail is included in each search result. So in this example, every view will include its description, certificate, and individual owners. (Limit these attributes to the minimum you need about each view to do your intended work.)

Once you have defined the criteria for your pipeline, call thestream()method to push-down the pipeline to Atlan. This will:Create a search that combines all the criteria you have specified.Run that search against Atlan to produce the first page of results.Page through the results by lazily fetching each subsequent page as you iterate through them. (So if you use alimit()on the stream, for example, you can break out before retrieving all pages.)Can also run in parallel threadsYou can also parallel-stream the results by passingtrueto thestream()method. This will spawn multiple threads that each independently process a page of results and combine the results in parallel. While this can be significantly faster for processing many results, keep in mind if you are collecting the results into any structure that structure must be thread-safe. (For example, you'll need to use things likeConcurrentHashMaprather than justHashMap, and to useParallelBatchrather thanAssetBatchif making changes.)

Once you have defined the criteria for your pipeline, call thestream()method to push-down the pipeline to Atlan. This will:

stream()

Create a search that combines all the criteria you have specified.

Run that search against Atlan to produce the first page of results.

Page through the results by lazily fetching each subsequent page as you iterate through them. (So if you use alimit()on the stream, for example, you can break out before retrieving all pages.)

limit()

Can also run in parallel threads

You can also parallel-stream the results by passingtrueto thestream()method. This will spawn multiple threads that each independently process a page of results and combine the results in parallel. While this can be significantly faster for processing many results, keep in mind if you are collecting the results into any structure that structure must be thread-safe. (For example, you'll need to use things likeConcurrentHashMaprather than justHashMap, and to useParallelBatchrather thanAssetBatchif making changes.)

true

stream()

ConcurrentHashMap

HashMap

ParallelBatch

AssetBatch

For each result, you can then carry out your changes and submit them into the batch.

For each result, you can then carry out your changes and submit them into the batch.

Every asset implements thetrimToRequired()method, which gives you a builder containing only the bare minimum information needed to update that asset.Limit your asset to only what you intend to updateWhen you send an update to Atlan, it will only attempt to change the information you send in your request — leaving any information not in your request as-is (unchanged) on the asset in Atlan. By usingtrimToRequired()you can remove all information you do not want to update, and then chain on only the details youdowant to update.

Every asset implements thetrimToRequired()method, which gives you a builder containing only the bare minimum information needed to update that asset.

trimToRequired()

Limit your asset to only what you intend to update

When you send an update to Atlan, it will only attempt to change the information you send in your request — leaving any information not in your request as-is (unchanged) on the asset in Atlan. By usingtrimToRequired()you can remove all information you do not want to update, and then chain on only the details youdowant to update.

trimToRequired()

In this running example, you are updating the certificate to verified and setting a new owner — so you simply chain those updates onto the trimmed builder.

In this running example, you are updating the certificate to verified and setting a new owner — so you simply chain those updates onto the trimmed builder.

You can then add your (in-memory) modified asset to the batch.Auto-saves as it goesAs long as the number of assets built-up is below the maximum batch size specified when creating the batch, this will simply continue to build up the batch. As soon as you hit the size limit for the batch, though, this same method will call thesave()operation to batch-update all of those assets in a single API call.Remember to flushSince your loop could finish before you reach another full batch, you must always remember toflush()the batch. This will send any remaining assets that were queued up, when the size of the queue did not yet reach the full batch size.

You can then add your (in-memory) modified asset to the batch.

Auto-saves as it goes

As long as the number of assets built-up is below the maximum batch size specified when creating the batch, this will simply continue to build up the batch. As soon as you hit the size limit for the batch, though, this same method will call thesave()operation to batch-update all of those assets in a single API call.

save()

Remember to flush

Since your loop could finish before you reach another full batch, you must always remember toflush()the batch. This will send any remaining assets that were queued up, when the size of the queue did not yet reach the full batch size.

flush()

Both the.add()and.flush()operations of theAssetBatchcould send a request over to Atlan. Either can therefore also run into trouble and raise an error through anAtlanException. It is up to you to handle such potential errors as you see fit.

Both the.add()and.flush()operations of theAssetBatchcould send a request over to Atlan. Either can therefore also run into trouble and raise an error through anAtlanException. It is up to you to handle such potential errors as you see fit.

.add()

.flush()

AssetBatch

AtlanException

TheAssetBatch'sadd()method used in the previous step will automatically save as its internal queue of assets reaches a full batch size.Remember to flushHowever, since your loop could finish before you reach another full batch, you must always remember toflush()the batch. This will send any remaining assets that were queued up.

TheAssetBatch'sadd()method used in the previous step will automatically save as its internal queue of assets reaches a full batch size.

AssetBatch

add()

Remember to flush

However, since your loop could finish before you reach another full batch, you must always remember toflush()the batch. This will send any remaining assets that were queued up.

flush()

123456789101112131415161718192021222324252627282930313233343536373839404142434445464748

importloggingfrompyatlan.client.atlanimportAtlanClientfrompyatlan.client.assetimportBatchfrompyatlan.model.enumsimportCertificateStatusfrompyatlan.model.fluent_searchimportCompoundQuery,FluentSearchfrompyatlan.model.assetsimportAsset,View,MaterialisedViewLOGGER=logging.getLogger(__name__)client=AtlanClient()schema_qualified_name="default/snowflake/1662194632/MYDB/MY_SCH"# (1)batch=Batch(# (2)client=client,max_size=20,replace_atlan_tags=False,custom_metadata_handling=CustomMetadataHandling.IGNORE,capture_failures=False,update_only=False,track=False,case_insensitive=False,table_view_agnostic=False,creation_handling=AssetCreationHandling.FULL,)find_views=(FluentSearch()# (3).where(Asset.QUALIFIED_NAME.startswith(schema_qualified_name))# (4).where(CompoundQuery.asset_types([View,MaterialisedView]))# (5).where(CompoundQuery.active_assets()).where_not(Asset.CERTIFICATE_STATUS.has_any_value())# (6).page_size(100)# (7).include_on_results(Asset.DESCRIPTION)# (8).include_on_results(Asset.CERTIFICATE_STATUS).include_on_results(Asset.OWNER_USERS)).to_request()# (9)response=client.asset.search(find_views)# (10)try:forassetinresponse:# (11)revised=asset.trim_to_required()# (12)revised.certificate_status=CertificateStatus.VERIFIED# (13)revised.owner_users={"jsmith"}batch.add(asset)# (14)batch.flush()# (15)LOGGER.info("Created%s",len(batch.created))LOGGER.info("Updated%s",len(batch.updated))exceptAtlanErroraserr:LOGGER.error("Unable to update:%s",asset.qualified_name)

importloggingfrompyatlan.client.atlanimportAtlanClientfrompyatlan.client.assetimportBatchfrompyatlan.model.enumsimportCertificateStatusfrompyatlan.model.fluent_searchimportCompoundQuery,FluentSearchfrompyatlan.model.assetsimportAsset,View,MaterialisedViewLOGGER=logging.getLogger(__name__)client=AtlanClient()schema_qualified_name="default/snowflake/1662194632/MYDB/MY_SCH"# (1)batch=Batch(# (2)client=client,max_size=20,replace_atlan_tags=False,custom_metadata_handling=CustomMetadataHandling.IGNORE,capture_failures=False,update_only=False,track=False,case_insensitive=False,table_view_agnostic=False,creation_handling=AssetCreationHandling.FULL,)find_views=(FluentSearch()# (3).where(Asset.QUALIFIED_NAME.startswith(schema_qualified_name))# (4).where(CompoundQuery.asset_types([View,MaterialisedView]))# (5).where(CompoundQuery.active_assets()).where_not(Asset.CERTIFICATE_STATUS.has_any_value())# (6).page_size(100)# (7).include_on_results(Asset.DESCRIPTION)# (8).include_on_results(Asset.CERTIFICATE_STATUS).include_on_results(Asset.OWNER_USERS)).to_request()# (9)response=client.asset.search(find_views)# (10)try:forassetinresponse:# (11)revised=asset.trim_to_required()# (12)revised.certificate_status=CertificateStatus.VERIFIED# (13)revised.owner_users={"jsmith"}batch.add(asset)# (14)batch.flush()# (15)LOGGER.info("Created%s",len(batch.created))LOGGER.info("Updated%s",len(batch.updated))exceptAtlanErroraserr:LOGGER.error("Unable to update:%s",asset.qualified_name)

ThequalifiedNameof every view starts with thequalifiedNameof its parent (schema), so we can limit the results to a particular schema by using thequalifiedName.

ThequalifiedNameof every view starts with thequalifiedNameof its parent (schema), so we can limit the results to a particular schema by using thequalifiedName.

qualifiedName

qualifiedName

qualifiedName

Create a batch of assets to accumulate changes across multiple
assets before applying those changes in Atlan itself. TheBatch()takes the following parameters:client: an instance ofAssetClient.max_size: the maximum size of each batch to be processed (per API call).Additional optional parametersBy default (using only the options above) no classifications or custom metadata will be added or changed on the assets in each batch. To also include classifications and custom metadata, you need to use these additional parameters:replace_atlan_tags(default: False): IfTruereplace all classifications (tags) on the assets in the batch, which would include removing classifications (tags) if none are provided for the assets in the batch itself (orFalseif you still want to ignore classifications)custom_metadata_handling(default: CustomMetadataHandling.IGNORE): control how custom metadata should be handled for the assets:IGNOREany custom metadata changes in the batch,OVERWRITEto replace all custom metadata with what's provided in the batch (including removing custom metadata that already exists on an asset), orMERGEto only add or update custom metadata based on what's in the batch (leaving other existing custom metadata unchanged)capture_failures(default: False): control whether failures should be captured across batches (True) or ignored (False)update_only(default: False): control whether the batch should only attempt to update assets that already exist (True) or also create assets if they do not yet exist (False)track(default: False): control whether details about each created and updated asset across batches should be tracked (True) or ignored (False) — counts will always be keptcase_insensitive(default: False): control whether the matching for determining whether an asset already exists should be done in a case-insensitive way (True) or case-sensitively (False)creation_handling(default: AssetCreationHandling.FULL): control what kind of assets to create, if not running inupdate_onlymode;PARTIALassets (only available in lineage), orFULLassetstable_view_agnostic(default: False): control whether the matching for determining whether an asset already exists should be done strictly according to the data type specified (False), or if tables, views and materialized views should be treated interchangeably (True)

Create a batch of assets to accumulate changes across multiple
assets before applying those changes in Atlan itself. TheBatch()takes the following parameters:

Batch()

client: an instance ofAssetClient.

client

AssetClient

max_size: the maximum size of each batch to be processed (per API call).

max_size

By default (using only the options above) no classifications or custom metadata will be added or changed on the assets in each batch. To also include classifications and custom metadata, you need to use these additional parameters:

replace_atlan_tags(default: False): IfTruereplace all classifications (tags) on the assets in the batch, which would include removing classifications (tags) if none are provided for the assets in the batch itself (orFalseif you still want to ignore classifications)

replace_atlan_tags

True

False

custom_metadata_handling(default: CustomMetadataHandling.IGNORE): control how custom metadata should be handled for the assets:IGNOREany custom metadata changes in the batch,OVERWRITEto replace all custom metadata with what's provided in the batch (including removing custom metadata that already exists on an asset), orMERGEto only add or update custom metadata based on what's in the batch (leaving other existing custom metadata unchanged)

custom_metadata_handling

IGNORE

OVERWRITE

MERGE

capture_failures(default: False): control whether failures should be captured across batches (True) or ignored (False)

capture_failures

True

False

update_only(default: False): control whether the batch should only attempt to update assets that already exist (True) or also create assets if they do not yet exist (False)

update_only

True

False

track(default: False): control whether details about each created and updated asset across batches should be tracked (True) or ignored (False) — counts will always be kept

track

True

False

case_insensitive(default: False): control whether the matching for determining whether an asset already exists should be done in a case-insensitive way (True) or case-sensitively (False)

case_insensitive

True

False

creation_handling(default: AssetCreationHandling.FULL): control what kind of assets to create, if not running inupdate_onlymode;PARTIALassets (only available in lineage), orFULLassets

creation_handling

update_only

PARTIAL

FULL

table_view_agnostic(default: False): control whether the matching for determining whether an asset already exists should be done strictly according to the data type specified (False), or if tables, views and materialized views should be treated interchangeably (True)

table_view_agnostic

False

True

You can then start defining a pipeline directly using aFluentSearch()object.

You can then start defining a pipeline directly using aFluentSearch()object.

FluentSearch()

You can chain as manywhere()methods as you want to define all the conditions the search results must match. You can use the class variables within any given type to select a particular attribute (likeQUALIFIED_NAMEin this example), and then limit results to only those assets whosequalified_namestarts with thequalified_nameof the schema (by using thestartswith()predicate). In this example, that means only assets that are within this particular schema will be returned as results.

You can chain as manywhere()methods as you want to define all the conditions the search results must match. You can use the class variables within any given type to select a particular attribute (likeQUALIFIED_NAMEin this example), and then limit results to only those assets whosequalified_namestarts with thequalified_nameof the schema (by using thestartswith()predicate). In this example, that means only assets that are within this particular schema will be returned as results.

where()

QUALIFIED_NAME

qualified_name

qualified_name

startswith()

Since there could be tables, views, materialized views and columns in this schema — but you only want views and materialized views — you can use theCompoundQuery.asset_types()helper method to restrict results to only views and materialized views.

Since there could be tables, views, materialized views and columns in this schema — but you only want views and materialized views — you can use theCompoundQuery.asset_types()helper method to restrict results to only views and materialized views.

CompoundQuery.asset_types()

Since you only want to update views that do not already have a certificate, you can further limit the results using thewhere_not()method. This will exclude any assets where a certificate alreadyhas_any_value().

Since you only want to update views that do not already have a certificate, you can further limit the results using thewhere_not()method. This will exclude any assets where a certificate alreadyhas_any_value().

where_not()

has_any_value()

(Optional) You can play around with different page sizes, to further limit API calls by retrieving more results per page.

(Optional) You can play around with different page sizes, to further limit API calls by retrieving more results per page.

Add as many attributes as needed. Each attribute you add here will ensure that detail is included in each search result. So in this example, every view will include its description, certificate, and individual owners. (Limit these attributes to the minimum you need about each view to do your intended work.)

Add as many attributes as needed. Each attribute you add here will ensure that detail is included in each search result. So in this example, every view will include its description, certificate, and individual owners. (Limit these attributes to the minimum you need about each view to do your intended work.)

You can translate the object you've built up into various outputs, for example immediately calculating a count of how many results match or streaming them directly for processing. In this case, thetoRequest()method will give us the resulting set of criteria back as a complete index search request.

You can translate the object you've built up into various outputs, for example immediately calculating a count of how many results match or streaming them directly for processing. In this case, thetoRequest()method will give us the resulting set of criteria back as a complete index search request.

toRequest()

You can then execute the search based on the request.tore all of those details back into a response object.

You can then execute the search based on the request.tore all of those details back into a response object.

For each result, you can then carry out your changes and submit them into the batch.

For each result, you can then carry out your changes and submit them into the batch.

Every asset implements thetrim_to_required()method, which gives you a builder containing only the bare minimum information needed to update that asset.Limit your asset to only what you intend to updateWhen you send an update to Atlan, it will only attempt to change the information you send in your request — leaving any information not in your request as-is (unchanged) on the asset in Atlan. By usingtrim_to_required()you can remove all information you do not want to update, and then chain on only the details youdowant to update.

Every asset implements thetrim_to_required()method, which gives you a builder containing only the bare minimum information needed to update that asset.

trim_to_required()

Limit your asset to only what you intend to update

When you send an update to Atlan, it will only attempt to change the information you send in your request — leaving any information not in your request as-is (unchanged) on the asset in Atlan. By usingtrim_to_required()you can remove all information you do not want to update, and then chain on only the details youdowant to update.

trim_to_required()

In this running example, you are updating the certificate to verified and setting a new owner — so you simply set those updates on the trimmed object.

In this running example, you are updating the certificate to verified and setting a new owner — so you simply set those updates on the trimmed object.

You can then add your (in-memory) modified asset to the batch.

You can then add your (in-memory) modified asset to the batch.

TheBatch'sadd()method used in the previous step will automatically save as its internal queue of assets reaches a full batch size.Remember to flushHowever, since your loop could finish before you reach another full batch, you must always remember toflush()the batch. This will send any remaining assets that were queued up.

TheBatch'sadd()method used in the previous step will automatically save as its internal queue of assets reaches a full batch size.

Batch

add()

Remember to flush

However, since your loop could finish before you reach another full batch, you must always remember toflush()the batch. This will send any remaining assets that were queued up.

flush()

12345678910111213141516171819202122232425

valschemaQN="default/snowflake/1662194632/MYDB/MY_SCH"// (1)ParallelBatch(client,20).use{batch->// (2)client.assets.select()// (3).where(Asset.QUALIFIED_NAME.startsWith(schemaQN))// (4).where(Asset.TYPE_NAME.`in`(listOf(View.TYPE_NAME,MaterializedView.TYPE_NAME)))// (5).whereNot(Asset.CERTIFICATE_STATUS.hasAnyValue())// (6).pageSize(100)// (7).includeOnResults(Asset.DESCRIPTION)// (8).includeOnResults(Asset.CERTIFICATE_STATUS).includeOnResults(Asset.OWNER_USERS).stream(true)// (9).forEach(result->{// (10)try{batch.add(result.trimToRequired()// (11).certificateStatus(CertificateStatus.VERIFIED)// (12).ownerUser("jsmith").build())// (13)}catch(AtlanExceptione){// (14)log.error("Unable to update: {}",result.qualifiedName);}});batch.flush()// (15)log.info("Created: {}",batch.created.size)log.info("Updated: {}",batch.updated.size)}

valschemaQN="default/snowflake/1662194632/MYDB/MY_SCH"// (1)ParallelBatch(client,20).use{batch->// (2)client.assets.select()// (3).where(Asset.QUALIFIED_NAME.startsWith(schemaQN))// (4).where(Asset.TYPE_NAME.`in`(listOf(View.TYPE_NAME,MaterializedView.TYPE_NAME)))// (5).whereNot(Asset.CERTIFICATE_STATUS.hasAnyValue())// (6).pageSize(100)// (7).includeOnResults(Asset.DESCRIPTION)// (8).includeOnResults(Asset.CERTIFICATE_STATUS).includeOnResults(Asset.OWNER_USERS).stream(true)// (9).forEach(result->{// (10)try{batch.add(result.trimToRequired()// (11).certificateStatus(CertificateStatus.VERIFIED)// (12).ownerUser("jsmith").build())// (13)}catch(AtlanExceptione){// (14)log.error("Unable to update: {}",result.qualifiedName);}});batch.flush()// (15)log.info("Created: {}",batch.created.size)log.info("Updated: {}",batch.updated.size)}

ThequalifiedNameof every view starts with thequalifiedNameof its parent (schema), so we can limit the results to a particular schema by using thequalifiedName.

ThequalifiedNameof every view starts with thequalifiedNameof its parent (schema), so we can limit the results to a particular schema by using thequalifiedName.

qualifiedName

qualifiedName

qualifiedName

Create a batch of assets to build-up the changes across multiple assets before applying those changes in Atlan itself. When parallel-processing (see further notes on thestream(true)) you need to use a parallel-capableParallelBatch:The first parameter defines the Atlan tenant on which the batch will be processedThe second specifies the maximum number of assets to build-up before sending them across to AtlanAdditional parametersBy default (using only the options above) no classifications or custom metadata will be added or changed on the assets in each batch. To also include classifications and custom metadata, you need to use these additional parameters:A third parameter oftrueto replace all classifications on the assets in the batch, which would include removing classifications if none are provided for the assets in the batch itself (orfalseif you still want to ignore classifications)A fourth parameter to control how custom metadata should be handled for the assets:IGNOREany custom metadata changes in the batch,OVERWRITEto replace all custom metadata with what's provided in the batch (including removing custom metadata that already exists on an asset), orMERGEto only add or update custom metadata based on what's in the batch (leaving other existing custom metadata unchanged)a fifth parameter to control whether failures should be captured across batches (true) or ignored (false)a sixth parameter to control whether the batch should only attempt to update assets that already exist (true) or also create assets if they do not yet exist (false)a seventh parameter to control whether details about each created and updated asset across batches should be tracked (true) or ignored (false) — counts will always be keptan eighth parameter to control whether the matching for determining whether an asset already exists should be done in a case-insensitive way (true) or case-sensitively (false)a ninth parameter to control what kind of assets to create, if not running inupdateOnlymode: partial assets (only available in lineage), or full assetsa tenth parameter to control whether the matching for determining whether an asset already exists should be done strictly according to the data type specified (false), or if tables, views and materialized views should be treated interchangeably (true)

Create a batch of assets to build-up the changes across multiple assets before applying those changes in Atlan itself. When parallel-processing (see further notes on thestream(true)) you need to use a parallel-capableParallelBatch:

stream(true)

ParallelBatch

The first parameter defines the Atlan tenant on which the batch will be processed

The second specifies the maximum number of assets to build-up before sending them across to Atlan

By default (using only the options above) no classifications or custom metadata will be added or changed on the assets in each batch. To also include classifications and custom metadata, you need to use these additional parameters:

A third parameter oftrueto replace all classifications on the assets in the batch, which would include removing classifications if none are provided for the assets in the batch itself (orfalseif you still want to ignore classifications)

true

false

A fourth parameter to control how custom metadata should be handled for the assets:IGNOREany custom metadata changes in the batch,OVERWRITEto replace all custom metadata with what's provided in the batch (including removing custom metadata that already exists on an asset), orMERGEto only add or update custom metadata based on what's in the batch (leaving other existing custom metadata unchanged)

IGNORE

OVERWRITE

MERGE

a fifth parameter to control whether failures should be captured across batches (true) or ignored (false)

true

false

a sixth parameter to control whether the batch should only attempt to update assets that already exist (true) or also create assets if they do not yet exist (false)

true

false

a seventh parameter to control whether details about each created and updated asset across batches should be tracked (true) or ignored (false) — counts will always be kept

true

false

an eighth parameter to control whether the matching for determining whether an asset already exists should be done in a case-insensitive way (true) or case-sensitively (false)

true

false

a ninth parameter to control what kind of assets to create, if not running inupdateOnlymode: partial assets (only available in lineage), or full assets

updateOnly

a tenth parameter to control whether the matching for determining whether an asset already exists should be done strictly according to the data type specified (false), or if tables, views and materialized views should be treated interchangeably (true)

false

true

You can then start defining a pipeline directly against the client'sassetsby using theselect()method.Including archived (soft-deleted) assetsSearches by default will returnallassets that are found — whether active or archived (soft-deleted). In most cases, you probably only want the active ones, so this is the default behavior ofselect(). Sending intrueto thisselect()method will start the pipeline toincludeany archived (soft-deleted) assets in the results, if you do want them.

You can then start defining a pipeline directly against the client'sassetsby using theselect()method.

assets

select()

Including archived (soft-deleted) assets

Searches by default will returnallassets that are found — whether active or archived (soft-deleted). In most cases, you probably only want the active ones, so this is the default behavior ofselect(). Sending intrueto thisselect()method will start the pipeline toincludeany archived (soft-deleted) assets in the results, if you do want them.

select()

true

select()

You can chain as manywhere()methods as you want to define all the conditions the search results must match. You can use the static constants within any given type to select a particular attribute (likeQUALIFIED_NAMEin this example), and then limit results to only those assets whosequalifiedNamestarts with thequalifiedNameof the schema (by using thestartsWith()predicate). In this example, that means only assets that are within this particular schema will be returned as results.

You can chain as manywhere()methods as you want to define all the conditions the search results must match. You can use the static constants within any given type to select a particular attribute (likeQUALIFIED_NAMEin this example), and then limit results to only those assets whosequalifiedNamestarts with thequalifiedNameof the schema (by using thestartsWith()predicate). In this example, that means only assets that are within this particular schema will be returned as results.

where()

QUALIFIED_NAME

qualifiedName

qualifiedName

startsWith()

Since there could be tables, views, materialized views and columns in this schema — but you only want views and materialized views — you can use theAsset.TYPE_NAME.inhelper method to restrict results to only views and materialized views.

Since there could be tables, views, materialized views and columns in this schema — but you only want views and materialized views — you can use theAsset.TYPE_NAME.inhelper method to restrict results to only views and materialized views.

Asset.TYPE_NAME.in

Since you only want to update views that do not already have a certificate, you can further limit the results using thewhereNot()method. This will exclude any assets where a certificate alreadyhasAnyValue().

Since you only want to update views that do not already have a certificate, you can further limit the results using thewhereNot()method. This will exclude any assets where a certificate alreadyhasAnyValue().

whereNot()

hasAnyValue()

(Optional) You can play around with different page sizes, to further limit API calls by retrieving more results per page.

(Optional) You can play around with different page sizes, to further limit API calls by retrieving more results per page.

Add as many attributes as needed. Each attribute you add here will ensure that detail is included in each search result. So in this example, every view will include its description, certificate, and individual owners. (Limit these attributes to the minimum you need about each view to do your intended work.)

Add as many attributes as needed. Each attribute you add here will ensure that detail is included in each search result. So in this example, every view will include its description, certificate, and individual owners. (Limit these attributes to the minimum you need about each view to do your intended work.)

Once you have defined the criteria for your pipeline, call thestream()method to push-down the pipeline to Atlan. This will:Create a search that combines all the criteria you have specified.Run that search against Atlan to produce the first page of results.Page through the results by lazily fetching each subsequent page as you iterate through them. (So if you use alimit()on the stream, for example, you can break out before retrieving all pages.)Can also run in parallel threadsYou can also parallel-stream the results by passingtrueto thestream()method. This will spawn multiple threads that each independently process a page of results and combine the results in parallel. While this can be significantly faster for processing many results, keep in mind if you are collecting the results into any structure that structure must be thread-safe. (For example, you'll need to use things likeConcurrentHashMaprather than justHashMap, and to useParallelBatchrather thanAssetBatchif making changes.)

Once you have defined the criteria for your pipeline, call thestream()method to push-down the pipeline to Atlan. This will:

stream()

Create a search that combines all the criteria you have specified.

Run that search against Atlan to produce the first page of results.

Page through the results by lazily fetching each subsequent page as you iterate through them. (So if you use alimit()on the stream, for example, you can break out before retrieving all pages.)

limit()

Can also run in parallel threads

You can also parallel-stream the results by passingtrueto thestream()method. This will spawn multiple threads that each independently process a page of results and combine the results in parallel. While this can be significantly faster for processing many results, keep in mind if you are collecting the results into any structure that structure must be thread-safe. (For example, you'll need to use things likeConcurrentHashMaprather than justHashMap, and to useParallelBatchrather thanAssetBatchif making changes.)

true

stream()

ConcurrentHashMap

HashMap

ParallelBatch

AssetBatch

For each result, you can then carry out your changes and submit them into the batch.

For each result, you can then carry out your changes and submit them into the batch.

Every asset implements thetrimToRequired()method, which gives you a builder containing only the bare minimum information needed to update that asset.Limit your asset to only what you intend to updateWhen you send an update to Atlan, it will only attempt to change the information you send in your request — leaving any information not in your request as-is (unchanged) on the asset in Atlan. By usingtrimToRequired()you can remove all information you do not want to update, and then chain on only the details youdowant to update.

Every asset implements thetrimToRequired()method, which gives you a builder containing only the bare minimum information needed to update that asset.

trimToRequired()

Limit your asset to only what you intend to update

When you send an update to Atlan, it will only attempt to change the information you send in your request — leaving any information not in your request as-is (unchanged) on the asset in Atlan. By usingtrimToRequired()you can remove all information you do not want to update, and then chain on only the details youdowant to update.

trimToRequired()

In this running example, you are updating the certificate to verified and setting a new owner — so you simply chain those updates onto the trimmed builder.

In this running example, you are updating the certificate to verified and setting a new owner — so you simply chain those updates onto the trimmed builder.

You can then add your (in-memory) modified asset to the batch.Auto-saves as it goesAs long as the number of assets built-up is below the maximum batch size specified when creating the batch, this will simply continue to build up the batch. As soon as you hit the size limit for the batch, though, this same method will call thesave()operation to batch-update all of those assets in a single API call.Remember to flushSince your loop could finish before you reach another full batch, you must always remember toflush()the batch. This will send any remaining assets that were queued up, when the size of the queue did not yet reach the full batch size.

You can then add your (in-memory) modified asset to the batch.

Auto-saves as it goes

As long as the number of assets built-up is below the maximum batch size specified when creating the batch, this will simply continue to build up the batch. As soon as you hit the size limit for the batch, though, this same method will call thesave()operation to batch-update all of those assets in a single API call.

save()

Remember to flush

Since your loop could finish before you reach another full batch, you must always remember toflush()the batch. This will send any remaining assets that were queued up, when the size of the queue did not yet reach the full batch size.

flush()

Both the.add()and.flush()operations of theAssetBatchcould send a request over to Atlan. Either can therefore also run into trouble and raise an error through anAtlanException. It is up to you to handle such potential errors as you see fit.

Both the.add()and.flush()operations of theAssetBatchcould send a request over to Atlan. Either can therefore also run into trouble and raise an error through anAtlanException. It is up to you to handle such potential errors as you see fit.

.add()

.flush()

AssetBatch

AtlanException

TheAssetBatch'sadd()method used in the previous step will automatically save as its internal queue of assets reaches a full batch size.Remember to flushHowever, since your loop could finish before you reach another full batch, you must always remember toflush()the batch. This will send any remaining assets that were queued up.

TheAssetBatch'sadd()method used in the previous step will automatically save as its internal queue of assets reaches a full batch size.

AssetBatch

add()

Remember to flush

However, since your loop could finish before you reach another full batch, you must always remember toflush()the batch. This will send any remaining assets that were queued up.

flush()

Requires numerous API calls

To implement the same logic purely through raw API calls will require making many calls:

To run the search.

To page through the results.

To batch up a set of assets to update.

To submit each batch of assets to update.



#### Cookie consent
(source: https://developer.atlan.com/patterns/bulk/end-to-end/)

We use cookies to:Anonymously measure page views, andAllow you to give us one-click feedback on any page.We donotcollect or store:Any personally identifiable information.Any information for any (re)marketing purposes.With your consent, you're helping us to make our documentation better 💙

Anonymously measure page views, and

Allow you to give us one-click feedback on any page.

Any personally identifiable information.

Any information for any (re)marketing purposes.

Google Analytics
