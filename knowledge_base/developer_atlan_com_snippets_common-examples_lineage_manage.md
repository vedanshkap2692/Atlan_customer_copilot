# Manage lineageÂ¶
(source: https://developer.atlan.com/snippets/common-examples/lineage/manage/)

Overview

Getting started

Common tasks

Asset-specific

Governance structures

Reference

ToolkitsToolkitsPackagesPackagesRunning exampleDefine via templateRender your packageDevelop your logicTest your logicRelease (GA)Widget referenceTypedefsTypedefsRunning exampleDefine via templateRender your modelTest your modelBind the SDKsWrite integration testTest baseline UXRelease (GA)TestingTesting

ToolkitsToolkitsPackagesPackagesRunning exampleDefine via templateRender your packageDevelop your logicTest your logicRelease (GA)Widget referenceTypedefsTypedefsRunning exampleDefine via templateRender your modelTest your modelBind the SDKsWrite integration testTest baseline UXRelease (GA)TestingTesting

PackagesPackagesRunning exampleDefine via templateRender your packageDevelop your logicTest your logicRelease (GA)Widget reference

Running example

Define via template

Render your package

Develop your logic

Test your logic

Release (GA)

Widget reference

TypedefsTypedefsRunning exampleDefine via templateRender your modelTest your modelBind the SDKsWrite integration testTest baseline UXRelease (GA)

Running example

Define via template

Render your model

Test your model

Bind the SDKs

Write integration test

Test baseline UX

Release (GA)

TestingTesting

Overview

Getting startedGetting startedOther important conceptsDocumentation conventionsIntegration optionsIntegration optionsCLIdbtJavaPythonKotlinScalaClojureGoEventsRaw REST APISite map

Other important concepts

Documentation conventions

Integration optionsIntegration optionsCLIdbtJavaPythonKotlinScalaClojureGoEventsRaw REST API

CLI

dbt

Java

Python

Kotlin

Scala

Clojure

Go

Events

Raw REST API

Site map

Common tasksCommon tasksCommon asset actionsCommon asset actionsCertify assetsManage announcementsChange descriptionChange ownersTag (classify) assetsChange custom metadataLink terms to assetsLink domains to assetsManage asset READMEsAdd asset resourcesManage asset relationships with attributesAsset CRUD operationsAsset CRUD operationsCreate an assetRetrieve an assetUpdate an assetDelete an assetFind and apply suggestionsRestore an assetReview changes to an assetReview accesses of an assetGet all assets that...Get all assets that...Search for assetsSearch examplesLineageLineageManage lineageManage lineageTable of contentsCreate lineage between assetsDirectlyUsing OpenLineageCreate lineage between columnsDirectlyUsing OpenLineageRemove lineage between assetsTraverse lineageBulk updatesBulk updatesCombine multiple operationsUpdate multiple assetsEnd-to-end bulk updateEvent handlingEvent handlingWebhook <> LambdaWebhook <> LambdaSet up LambdaCode your logicDeploy your codeSet up webhookManage your webhook

Common asset actionsCommon asset actionsCertify assetsManage announcementsChange descriptionChange ownersTag (classify) assetsChange custom metadataLink terms to assetsLink domains to assetsManage asset READMEsAdd asset resourcesManage asset relationships with attributes

Certify assets

Manage announcements

Change description

Change owners

Tag (classify) assets

Change custom metadata

Link terms to assets

Link domains to assets

Manage asset READMEs

Add asset resources

Manage asset relationships with attributes

Asset CRUD operationsAsset CRUD operationsCreate an assetRetrieve an assetUpdate an assetDelete an assetFind and apply suggestionsRestore an assetReview changes to an assetReview accesses of an asset

Create an asset

Retrieve an asset

Update an asset

Delete an asset

Find and apply suggestions

Restore an asset

Review changes to an asset

Review accesses of an asset

Get all assets that...Get all assets that...Search for assetsSearch examples

Search for assets

Search examples

LineageLineageManage lineageManage lineageTable of contentsCreate lineage between assetsDirectlyUsing OpenLineageCreate lineage between columnsDirectlyUsing OpenLineageRemove lineage between assetsTraverse lineage

Manage lineageManage lineageTable of contentsCreate lineage between assetsDirectlyUsing OpenLineageCreate lineage between columnsDirectlyUsing OpenLineageRemove lineage between assets

Create lineage between assetsDirectlyUsing OpenLineage

Directly

Using OpenLineage

Create lineage between columnsDirectlyUsing OpenLineage

Directly

Using OpenLineage

Remove lineage between assets

Traverse lineage

Bulk updatesBulk updatesCombine multiple operationsUpdate multiple assetsEnd-to-end bulk update

Combine multiple operations

Update multiple assets

End-to-end bulk update

Event handlingEvent handlingWebhook <> LambdaWebhook <> LambdaSet up LambdaCode your logicDeploy your codeSet up webhookManage your webhook

Webhook <> LambdaWebhook <> LambdaSet up LambdaCode your logicDeploy your codeSet up webhookManage your webhook

Set up Lambda

Code your logic

Deploy your code

Set up webhook

Manage your webhook

Asset-specificAsset-specificGlossary operationsGlossary operationsCreate objectsRetrieval by nameCreate a hierarchyCategorize termsTraverse category hierarchyCreating assetsCreating assetsManage relational assetsManage cube assetsManage object store assetsManage object store assetsManage AWS S3 assetsManage Azure Data Lake Storage assetsManage Google Cloud Storage assetsManage BI assetsManage BI assetsManage Google Data Studio assetsManage Preset assetsManage Superset assetsManage API assetsManage file assetsManage Airflow assetsManage Kafka assetsManage Azure Event Hub assetsManage App assetsManage AI assetsManage Insights assetsManage QuickSight assetsManage DocumentDB assetsManage Data Quality assetsManage Data Quality assetsManage Data Quality rulesConnector types and iconsData meshData meshManage data domainsManage data productsData contractsData contractsManage data contracts (via CLI)Manage data contracts (via SDK)Profiling and popularityProfiling and popularityManage column profilingManage popularity

Glossary operationsGlossary operationsCreate objectsRetrieval by nameCreate a hierarchyCategorize termsTraverse category hierarchy

Create objects

Retrieval by name

Create a hierarchy

Categorize terms

Traverse category hierarchy

Creating assetsCreating assetsManage relational assetsManage cube assetsManage object store assetsManage object store assetsManage AWS S3 assetsManage Azure Data Lake Storage assetsManage Google Cloud Storage assetsManage BI assetsManage BI assetsManage Google Data Studio assetsManage Preset assetsManage Superset assetsManage API assetsManage file assetsManage Airflow assetsManage Kafka assetsManage Azure Event Hub assetsManage App assetsManage AI assetsManage Insights assetsManage QuickSight assetsManage DocumentDB assetsManage Data Quality assetsManage Data Quality assetsManage Data Quality rulesConnector types and icons

Manage relational assets

Manage cube assets

Manage object store assetsManage object store assetsManage AWS S3 assetsManage Azure Data Lake Storage assetsManage Google Cloud Storage assets

Manage AWS S3 assets

Manage Azure Data Lake Storage assets

Manage Google Cloud Storage assets

Manage BI assetsManage BI assetsManage Google Data Studio assetsManage Preset assetsManage Superset assets

Manage Google Data Studio assets

Manage Preset assets

Manage Superset assets

Manage API assets

Manage file assets

Manage Airflow assets

Manage Kafka assets

Manage Azure Event Hub assets

Manage App assets

Manage AI assets

Manage Insights assets

Manage QuickSight assets

Manage DocumentDB assets

Manage Data Quality assetsManage Data Quality assetsManage Data Quality rules

Manage Data Quality rules

Connector types and icons

Data meshData meshManage data domainsManage data products

Manage data domains

Manage data products

Data contractsData contractsManage data contracts (via CLI)Manage data contracts (via SDK)

Manage data contracts (via CLI)

Manage data contracts (via SDK)

Profiling and popularityProfiling and popularityManage column profilingManage popularity

Manage column profiling

Manage popularity

Governance structuresGovernance structuresCustom metadataCustom metadataCreate custom metadataRetrieve custom metadataUpdate custom metadataDelete custom metadataManage badgesManage options (enumerations)Tag managementTag managementManage Atlan tagsMonitor propagationAccess controlAccess controlManage personasManage purposesManage policiesAccess eventsAPI token managementRun queries on an assetUsers and groupsUsers and groupsCreate users and groupsRetrieve users and groupsUpdate users and groupsDelete users and groupsManage SSO group mappingPackages and workflowsPackages and workflowsManage workflowsManage workflow schedulesSupported packagesSupported packagesAthena assetsAsset importAsset export (basic)API token connection adminBigQuery assetsConnection deleteConfluent Kafka assetsdbt assetsDynamoDB assetsDatabricks assetsDatabricks minerFivetran enrichmentGlue assetsLooker assetsLineage builderLineage generator (no transformation)MongoDB assetsOracle assetsPostgres assetsPowerBI assetsRedshift assetsRelational assets builderSnowflake assetsSnowflake minerSigma assetsSQL Server assetsTableau assetsFile managementFile management

Custom metadataCustom metadataCreate custom metadataRetrieve custom metadataUpdate custom metadataDelete custom metadataManage badgesManage options (enumerations)

Create custom metadata

Retrieve custom metadata

Update custom metadata

Delete custom metadata

Manage badges

Manage options (enumerations)

Tag managementTag managementManage Atlan tagsMonitor propagation

Manage Atlan tags

Monitor propagation

Access controlAccess controlManage personasManage purposesManage policiesAccess eventsAPI token managementRun queries on an asset

Manage personas

Manage purposes

Manage policies

Access events

API token management

Run queries on an asset

Users and groupsUsers and groupsCreate users and groupsRetrieve users and groupsUpdate users and groupsDelete users and groupsManage SSO group mapping

Create users and groups

Retrieve users and groups

Update users and groups

Delete users and groups

Manage SSO group mapping

Packages and workflowsPackages and workflowsManage workflowsManage workflow schedulesSupported packagesSupported packagesAthena assetsAsset importAsset export (basic)API token connection adminBigQuery assetsConnection deleteConfluent Kafka assetsdbt assetsDynamoDB assetsDatabricks assetsDatabricks minerFivetran enrichmentGlue assetsLooker assetsLineage builderLineage generator (no transformation)MongoDB assetsOracle assetsPostgres assetsPowerBI assetsRedshift assetsRelational assets builderSnowflake assetsSnowflake minerSigma assetsSQL Server assetsTableau assets

Manage workflows

Manage workflow schedules

Supported packagesSupported packagesAthena assetsAsset importAsset export (basic)API token connection adminBigQuery assetsConnection deleteConfluent Kafka assetsdbt assetsDynamoDB assetsDatabricks assetsDatabricks minerFivetran enrichmentGlue assetsLooker assetsLineage builderLineage generator (no transformation)MongoDB assetsOracle assetsPostgres assetsPowerBI assetsRedshift assetsRelational assets builderSnowflake assetsSnowflake minerSigma assetsSQL Server assetsTableau assets

Athena assets

Asset import

Asset export (basic)

API token connection admin

BigQuery assets

Connection delete

Confluent Kafka assets

dbt assets

DynamoDB assets

Databricks assets

Databricks miner

Fivetran enrichment

Glue assets

Looker assets

Lineage builder

Lineage generator (no transformation)

MongoDB assets

Oracle assets

Postgres assets

PowerBI assets

Redshift assets

Relational assets builder

Snowflake assets

Snowflake miner

Sigma assets

SQL Server assets

Tableau assets

File managementFile management

ReferenceReferenceSearchingSearchingQueryingQueryingTerm-level queriesFull text queriesRank feature queriesCompound queriesSearchable fieldsSearchable fieldsCommon search fieldsGlossary-specific search fieldsLimiting detailsSorting search resultsPaging search resultsAggregating search resultsEventsEventsEvent triggersEvent triggersAsset is createdAsset is updatedAsset is deletedCustom metadata is addedCustom metadata is removedAsset is taggedAsset is untaggedLineage is createdEvent typesEvent typesENTITY_CREATEENTITY_UPDATEENTITY_DELETEBUSINESS_ATTRIBUTE_UPDATECLASSIFICATION_ADDCLASSIFICATION_DELETESpecificationsSpecificationsData contract specOpenLineage specTypesTypesCoreCoreReferenceableAssetConnectionCatalogTagTagAttachmentAccess controlAccess controlPersonaPurposeAuthPolicyAuthServiceBusinessPolicyBusinessPolicyExceptionBusinessPolicyIncidentBusinessPolicyLogIncidentLineageLineageColumnProcessBIProcessResourcesResourcesLinkFileReadmeReadmeTemplateBadgeWorkflowsWorkflowsWorkflowWorkflowRunTaskStructsStructsActionAuthPolicyConditionAuthPolicyValidityScheduleAwsCloudWatchMetricAwsTagAzureTagBadgeConditionBusinessPolicyRuleByocSsoConfigColumnValueFrequencyMapDbtMetricFilterDbtJobRunGoogleLabelGoogleTagHistogramKafkaTopicConsumptionMCRuleComparisonMCRuleSchedulePopularityInsightsSourceTagAttachmentSourceTagAttachmentValueSourceTagAttributeStarredDetailsEnumerationsEnumerationsAdfActivityStateADLSAccessTierADLSAccountStatusADLSEncryptionTypesADLSLeaseStateADLSLeaseStatusADLSObjectArchiveStatusADLSObjectTypeADLSPerformanceADLSProvisionStateADLSReplicationTypeADLSStorageKindAPIQueryParamTypeEnumatlas_operationAtlasGlossaryCategoryTypeAtlasGlossaryTermTypeAtlasGlossaryTypeAtlasGlossaryTermAssignmentStatusAtlasGlossaryTermRelationshipStatusAuthPolicyCategoryAuthPolicyResourceCategoryAuthPolicyTypecertificate_statusDataGlossaryDataProductCriticalityDataProductSensitivityDataProductStatusDataProductVisibilityDomoCardTypeDynamoDBSecondaryIndexProjectionTypeDynamoDBStatusfile_typeFivetranConnectorStatusFivetranProcessStatusgoogle_datastudio_asset_typeicon_typeincident_severitykafka_topic_cleanup_policykafka_topic_compression_typematillion_job_typeModelCardinalityTypeMongoDBCollectionValidationActionMongoDBCollectionValidationLevelOpenLineageRunStatepowerbi_endorsementquery_username_strategyquick_sight_analysis_statusquick_sight_dataset_field_typequick_sight_dataset_import_modequick_sight_folder_typeSchemaRegistrySchemaCompatibilitySchemaRegistrySchemaTypeSourceCostUnitTypetable_typeWorkflowRunStatusWorkflowRunTypeWorkflowStatusWorkflowTypeAbstractionsAbstractionsBICloudInsightObjectStoreEventStoreDataQualityMetricNoSQLSchemaRegistryGlossaryGlossaryAtlasGlossaryAtlasGlossaryCategoryAtlasGlossaryTermData meshData meshDataDomainDataProductDataContractStakeholderStakeholderTitleRelational databasesRelational databasesDatabaseSchemaTableViewMaterialisedViewColumnQueryTablePartitionCalculationViewBigqueryTagDatabricksUnityCatalogTagSnowflakeDynamicTableSnowflakePipeSnowflakeStreamSnowflakeTagProcedureFunctionSQLQuery organizationQuery organizationNamespaceCollectionFolderCubesCubesCubeCubeDimensionCubeHierarchyCubeFieldAPIsAPIsAPIPathAPISpecAPIObjectAPIQueryAPIFieldAirflowAirflowAirflowDagAirflowTaskAmazonAmazonAmazon DynamoDBAmazon DynamoDBDynamoDBTableDynamoDBSecondaryIndexDynamoDBGlobalSecondaryIndexDynamoDBLocalSecondaryIndexAWS S3AWS S3S3BucketS3ObjectAmazon QuickSightAmazon QuickSightQuickSightAnalysisQuickSightAnalysisVisualQuickSightDashboardQuickSightDashboardVisualQuickSightDatasetQuickSightDatasetFieldQuickSightFolderAnaplanAnaplanAnaplanWorkspaceAnaplanAppAnaplanPageAnaplanModelAnaplanModuleAnaplanListAnaplanSystemDimensionAnaplanDimensionAnaplanLineItemAnaplanViewAnomaloAnomaloAnomaloCheckAppAppApplicationApplicationFieldMicrosoft AzureMicrosoft AzureAzure Data FactoryAzure Data FactoryAdfActivityAdfDataflowAdfDatasetAdfLinkedserviceAdfPipelineAzure Data Lake StorageAzure Data Lake StorageADLSAccountADLSContainerADLSObjectAzure Event HubAzure Event HubAzureEventHubAzureEventHubConsumerGroupAzure Service BusAzure Service BusAzureServiceBusNamespaceAzureServiceBusSchemaAzureServiceBusTopicCosmos DBCosmos DBCosmosMongoDBAccountCosmosMongoDBCollectionCosmosMongoDBDatabaseCogniteCogniteCognite3DModelCogniteAssetCogniteEventCogniteFileCogniteSequenceCogniteTimeseriesCustomCustomCustomEntityDataverseDataverseDataverseEntityDataverseAttributedbtdbtDbtColumnProcessDbtMetricDbtModelDbtModelColumnDbtProcessDbtSourceDbtTagDbtTestDomoDomoDomoCardDomoDashboardDomoDatasetDomoDatasetColumnDocumentDBDocumentDBDocumentDBCollectionDocumentDBDatabaseFivetranFivetranFivetranConnectorGoogleGoogleGoogle Cloud StorageGoogle Cloud StorageGCSBucketGCSObjectGoogle Data StudioGoogle Data StudioDataStudioAssetIBMIBMIBMCognosCognosCognosDashboardCognosDatasourceCognosExplorationCognosFileCognosFolderCognosModuleCognosPackageCognosReportKafkaKafkaKafkaConsumerGroupKafkaTopicLookerLookerLookerDashboardLookerExploreLookerFieldLookerFolderLookerLookLookerModelLookerProjectLookerQueryLookerTileLookerViewMatillionMatillionMatillionComponentMatillionGroupMatillionJobMatillionProjectMetabaseMetabaseMetabaseCollectionMetabaseDashboardMetabaseQuestionMicroStrategyMicroStrategyMicroStrategyAttributeMicroStrategyCubeMicroStrategyDocumentMicroStrategyDossierMicroStrategyFactMicroStrategyMetricMicroStrategyProjectMicroStrategyReportMicroStrategyVisualizationModeModeModeChartModeCollectionModeQueryModeReportModeWorkspaceModelsModelsModelAttributeModelAttributeAssociationModelDataModelModelEntityModelEntityAssociationModelVersionMongoDBMongoDBMongoDBCollectionMongDBDatabaseMonte CarloMonte CarloMCIncidentMCMonitorPower BIPower BIPower BIPowerBIColumnPowerBIDashboardPowerBIDataflowPowerBIDataflowEntityColumnPowerBIDatasetPowerBIDatasourcePowerBIMeasurePowerBIPagePowerBIReportPowerBITablePowerBITilePowerBIWorkspacePresetPresetPresetChartPresetDashboardPresetDatasetPresetWorkspaceQlikQlikQlikAppQlikChartQlikDatasetQlikSheetQlikSpaceQlikStreamRedashRedashRedashDashboardRedashQueryRedashVisualizationSalesforceSalesforceSalesforceDashboardSalesforceFieldSalesforceObjectSalesforceOrganizationSalesforceReportSaaSSigmaSigmaSigmaWorkbookSigmaPageSigmaDataElementSigmaDataElementFieldSigmaDatasetSigmaDatasetColumnSisenseSisenseSisenseDashboardSisenseDatamodelSisenseDatamodelTableSisenseFolderSisenseWidgetSodaSodaSodaCheckSparkSparkSparkJobSupersetSupersetSupersetChartSupersetDashboardSupersetDatasetTableauTableauTableauCalculatedFieldTableauDashboardTableauDatasourceTableauDatasourceFieldTableauFlowTableauMetricTableauProjectTableauSiteTableauWorkbookTableauWorksheetThoughtSpotThoughtSpotThoughtspotAnswerThoughtspotColumnThoughtspotDashletThoughtspotLiveboardThoughtspotTableThoughtspotViewThoughtspotWorksheetEndpoints

SearchingSearchingQueryingQueryingTerm-level queriesFull text queriesRank feature queriesCompound queriesSearchable fieldsSearchable fieldsCommon search fieldsGlossary-specific search fieldsLimiting detailsSorting search resultsPaging search resultsAggregating search results

QueryingQueryingTerm-level queriesFull text queriesRank feature queriesCompound queries

Term-level queries

Full text queries

Rank feature queries

Compound queries

Searchable fieldsSearchable fieldsCommon search fieldsGlossary-specific search fields

Common search fields

Glossary-specific search fields

Limiting details

Sorting search results

Paging search results

Aggregating search results

EventsEventsEvent triggersEvent triggersAsset is createdAsset is updatedAsset is deletedCustom metadata is addedCustom metadata is removedAsset is taggedAsset is untaggedLineage is createdEvent typesEvent typesENTITY_CREATEENTITY_UPDATEENTITY_DELETEBUSINESS_ATTRIBUTE_UPDATECLASSIFICATION_ADDCLASSIFICATION_DELETE

Event triggersEvent triggersAsset is createdAsset is updatedAsset is deletedCustom metadata is addedCustom metadata is removedAsset is taggedAsset is untaggedLineage is created

Asset is created

Asset is updated

Asset is deleted

Custom metadata is added

Custom metadata is removed

Asset is tagged

Asset is untagged

Lineage is created

Event typesEvent typesENTITY_CREATEENTITY_UPDATEENTITY_DELETEBUSINESS_ATTRIBUTE_UPDATECLASSIFICATION_ADDCLASSIFICATION_DELETE

ENTITY_CREATE

ENTITY_UPDATE

ENTITY_DELETE

BUSINESS_ATTRIBUTE_UPDATE

CLASSIFICATION_ADD

CLASSIFICATION_DELETE

SpecificationsSpecificationsData contract specOpenLineage spec

Data contract spec

OpenLineage spec

TypesTypesCoreCoreReferenceableAssetConnectionCatalogTagTagAttachmentAccess controlAccess controlPersonaPurposeAuthPolicyAuthServiceBusinessPolicyBusinessPolicyExceptionBusinessPolicyIncidentBusinessPolicyLogIncidentLineageLineageColumnProcessBIProcessResourcesResourcesLinkFileReadmeReadmeTemplateBadgeWorkflowsWorkflowsWorkflowWorkflowRunTaskStructsStructsActionAuthPolicyConditionAuthPolicyValidityScheduleAwsCloudWatchMetricAwsTagAzureTagBadgeConditionBusinessPolicyRuleByocSsoConfigColumnValueFrequencyMapDbtMetricFilterDbtJobRunGoogleLabelGoogleTagHistogramKafkaTopicConsumptionMCRuleComparisonMCRuleSchedulePopularityInsightsSourceTagAttachmentSourceTagAttachmentValueSourceTagAttributeStarredDetailsEnumerationsEnumerationsAdfActivityStateADLSAccessTierADLSAccountStatusADLSEncryptionTypesADLSLeaseStateADLSLeaseStatusADLSObjectArchiveStatusADLSObjectTypeADLSPerformanceADLSProvisionStateADLSReplicationTypeADLSStorageKindAPIQueryParamTypeEnumatlas_operationAtlasGlossaryCategoryTypeAtlasGlossaryTermTypeAtlasGlossaryTypeAtlasGlossaryTermAssignmentStatusAtlasGlossaryTermRelationshipStatusAuthPolicyCategoryAuthPolicyResourceCategoryAuthPolicyTypecertificate_statusDataGlossaryDataProductCriticalityDataProductSensitivityDataProductStatusDataProductVisibilityDomoCardTypeDynamoDBSecondaryIndexProjectionTypeDynamoDBStatusfile_typeFivetranConnectorStatusFivetranProcessStatusgoogle_datastudio_asset_typeicon_typeincident_severitykafka_topic_cleanup_policykafka_topic_compression_typematillion_job_typeModelCardinalityTypeMongoDBCollectionValidationActionMongoDBCollectionValidationLevelOpenLineageRunStatepowerbi_endorsementquery_username_strategyquick_sight_analysis_statusquick_sight_dataset_field_typequick_sight_dataset_import_modequick_sight_folder_typeSchemaRegistrySchemaCompatibilitySchemaRegistrySchemaTypeSourceCostUnitTypetable_typeWorkflowRunStatusWorkflowRunTypeWorkflowStatusWorkflowTypeAbstractionsAbstractionsBICloudInsightObjectStoreEventStoreDataQualityMetricNoSQLSchemaRegistryGlossaryGlossaryAtlasGlossaryAtlasGlossaryCategoryAtlasGlossaryTermData meshData meshDataDomainDataProductDataContractStakeholderStakeholderTitleRelational databasesRelational databasesDatabaseSchemaTableViewMaterialisedViewColumnQueryTablePartitionCalculationViewBigqueryTagDatabricksUnityCatalogTagSnowflakeDynamicTableSnowflakePipeSnowflakeStreamSnowflakeTagProcedureFunctionSQLQuery organizationQuery organizationNamespaceCollectionFolderCubesCubesCubeCubeDimensionCubeHierarchyCubeFieldAPIsAPIsAPIPathAPISpecAPIObjectAPIQueryAPIFieldAirflowAirflowAirflowDagAirflowTaskAmazonAmazonAmazon DynamoDBAmazon DynamoDBDynamoDBTableDynamoDBSecondaryIndexDynamoDBGlobalSecondaryIndexDynamoDBLocalSecondaryIndexAWS S3AWS S3S3BucketS3ObjectAmazon QuickSightAmazon QuickSightQuickSightAnalysisQuickSightAnalysisVisualQuickSightDashboardQuickSightDashboardVisualQuickSightDatasetQuickSightDatasetFieldQuickSightFolderAnaplanAnaplanAnaplanWorkspaceAnaplanAppAnaplanPageAnaplanModelAnaplanModuleAnaplanListAnaplanSystemDimensionAnaplanDimensionAnaplanLineItemAnaplanViewAnomaloAnomaloAnomaloCheckAppAppApplicationApplicationFieldMicrosoft AzureMicrosoft AzureAzure Data FactoryAzure Data FactoryAdfActivityAdfDataflowAdfDatasetAdfLinkedserviceAdfPipelineAzure Data Lake StorageAzure Data Lake StorageADLSAccountADLSContainerADLSObjectAzure Event HubAzure Event HubAzureEventHubAzureEventHubConsumerGroupAzure Service BusAzure Service BusAzureServiceBusNamespaceAzureServiceBusSchemaAzureServiceBusTopicCosmos DBCosmos DBCosmosMongoDBAccountCosmosMongoDBCollectionCosmosMongoDBDatabaseCogniteCogniteCognite3DModelCogniteAssetCogniteEventCogniteFileCogniteSequenceCogniteTimeseriesCustomCustomCustomEntityDataverseDataverseDataverseEntityDataverseAttributedbtdbtDbtColumnProcessDbtMetricDbtModelDbtModelColumnDbtProcessDbtSourceDbtTagDbtTestDomoDomoDomoCardDomoDashboardDomoDatasetDomoDatasetColumnDocumentDBDocumentDBDocumentDBCollectionDocumentDBDatabaseFivetranFivetranFivetranConnectorGoogleGoogleGoogle Cloud StorageGoogle Cloud StorageGCSBucketGCSObjectGoogle Data StudioGoogle Data StudioDataStudioAssetIBMIBMIBMCognosCognosCognosDashboardCognosDatasourceCognosExplorationCognosFileCognosFolderCognosModuleCognosPackageCognosReportKafkaKafkaKafkaConsumerGroupKafkaTopicLookerLookerLookerDashboardLookerExploreLookerFieldLookerFolderLookerLookLookerModelLookerProjectLookerQueryLookerTileLookerViewMatillionMatillionMatillionComponentMatillionGroupMatillionJobMatillionProjectMetabaseMetabaseMetabaseCollectionMetabaseDashboardMetabaseQuestionMicroStrategyMicroStrategyMicroStrategyAttributeMicroStrategyCubeMicroStrategyDocumentMicroStrategyDossierMicroStrategyFactMicroStrategyMetricMicroStrategyProjectMicroStrategyReportMicroStrategyVisualizationModeModeModeChartModeCollectionModeQueryModeReportModeWorkspaceModelsModelsModelAttributeModelAttributeAssociationModelDataModelModelEntityModelEntityAssociationModelVersionMongoDBMongoDBMongoDBCollectionMongDBDatabaseMonte CarloMonte CarloMCIncidentMCMonitorPower BIPower BIPower BIPowerBIColumnPowerBIDashboardPowerBIDataflowPowerBIDataflowEntityColumnPowerBIDatasetPowerBIDatasourcePowerBIMeasurePowerBIPagePowerBIReportPowerBITablePowerBITilePowerBIWorkspacePresetPresetPresetChartPresetDashboardPresetDatasetPresetWorkspaceQlikQlikQlikAppQlikChartQlikDatasetQlikSheetQlikSpaceQlikStreamRedashRedashRedashDashboardRedashQueryRedashVisualizationSalesforceSalesforceSalesforceDashboardSalesforceFieldSalesforceObjectSalesforceOrganizationSalesforceReportSaaSSigmaSigmaSigmaWorkbookSigmaPageSigmaDataElementSigmaDataElementFieldSigmaDatasetSigmaDatasetColumnSisenseSisenseSisenseDashboardSisenseDatamodelSisenseDatamodelTableSisenseFolderSisenseWidgetSodaSodaSodaCheckSparkSparkSparkJobSupersetSupersetSupersetChartSupersetDashboardSupersetDatasetTableauTableauTableauCalculatedFieldTableauDashboardTableauDatasourceTableauDatasourceFieldTableauFlowTableauMetricTableauProjectTableauSiteTableauWorkbookTableauWorksheetThoughtSpotThoughtSpotThoughtspotAnswerThoughtspotColumnThoughtspotDashletThoughtspotLiveboardThoughtspotTableThoughtspotViewThoughtspotWorksheet

CoreCoreReferenceableAssetConnectionCatalogTagTagAttachmentAccess controlAccess controlPersonaPurposeAuthPolicyAuthServiceBusinessPolicyBusinessPolicyExceptionBusinessPolicyIncidentBusinessPolicyLogIncidentLineageLineageColumnProcessBIProcessResourcesResourcesLinkFileReadmeReadmeTemplateBadgeWorkflowsWorkflowsWorkflowWorkflowRunTaskStructsStructsActionAuthPolicyConditionAuthPolicyValidityScheduleAwsCloudWatchMetricAwsTagAzureTagBadgeConditionBusinessPolicyRuleByocSsoConfigColumnValueFrequencyMapDbtMetricFilterDbtJobRunGoogleLabelGoogleTagHistogramKafkaTopicConsumptionMCRuleComparisonMCRuleSchedulePopularityInsightsSourceTagAttachmentSourceTagAttachmentValueSourceTagAttributeStarredDetailsEnumerationsEnumerationsAdfActivityStateADLSAccessTierADLSAccountStatusADLSEncryptionTypesADLSLeaseStateADLSLeaseStatusADLSObjectArchiveStatusADLSObjectTypeADLSPerformanceADLSProvisionStateADLSReplicationTypeADLSStorageKindAPIQueryParamTypeEnumatlas_operationAtlasGlossaryCategoryTypeAtlasGlossaryTermTypeAtlasGlossaryTypeAtlasGlossaryTermAssignmentStatusAtlasGlossaryTermRelationshipStatusAuthPolicyCategoryAuthPolicyResourceCategoryAuthPolicyTypecertificate_statusDataGlossaryDataProductCriticalityDataProductSensitivityDataProductStatusDataProductVisibilityDomoCardTypeDynamoDBSecondaryIndexProjectionTypeDynamoDBStatusfile_typeFivetranConnectorStatusFivetranProcessStatusgoogle_datastudio_asset_typeicon_typeincident_severitykafka_topic_cleanup_policykafka_topic_compression_typematillion_job_typeModelCardinalityTypeMongoDBCollectionValidationActionMongoDBCollectionValidationLevelOpenLineageRunStatepowerbi_endorsementquery_username_strategyquick_sight_analysis_statusquick_sight_dataset_field_typequick_sight_dataset_import_modequick_sight_folder_typeSchemaRegistrySchemaCompatibilitySchemaRegistrySchemaTypeSourceCostUnitTypetable_typeWorkflowRunStatusWorkflowRunTypeWorkflowStatusWorkflowTypeAbstractionsAbstractionsBICloudInsightObjectStoreEventStoreDataQualityMetricNoSQLSchemaRegistry

Referenceable

Asset

Connection

Catalog

Tag

TagAttachment

Access controlAccess controlPersonaPurposeAuthPolicyAuthService

Persona

Purpose

AuthPolicy

AuthService

BusinessPolicy

BusinessPolicyException

BusinessPolicyIncident

BusinessPolicyLog

Incident

LineageLineageColumnProcessBIProcess

ColumnProcess

BIProcess

ResourcesResourcesLinkFileReadmeReadmeTemplateBadge

Link

File

Readme

ReadmeTemplate

Badge

WorkflowsWorkflowsWorkflowWorkflowRunTask

Workflow

WorkflowRun

Task

StructsStructsActionAuthPolicyConditionAuthPolicyValidityScheduleAwsCloudWatchMetricAwsTagAzureTagBadgeConditionBusinessPolicyRuleByocSsoConfigColumnValueFrequencyMapDbtMetricFilterDbtJobRunGoogleLabelGoogleTagHistogramKafkaTopicConsumptionMCRuleComparisonMCRuleSchedulePopularityInsightsSourceTagAttachmentSourceTagAttachmentValueSourceTagAttributeStarredDetails

Action

AuthPolicyCondition

AuthPolicyValiditySchedule

AwsCloudWatchMetric

AwsTag

AzureTag

BadgeCondition

BusinessPolicyRule

ByocSsoConfig

ColumnValueFrequencyMap

DbtMetricFilter

DbtJobRun

GoogleLabel

GoogleTag

Histogram

KafkaTopicConsumption

MCRuleComparison

MCRuleSchedule

PopularityInsights

SourceTagAttachment

SourceTagAttachmentValue

SourceTagAttribute

StarredDetails

EnumerationsEnumerationsAdfActivityStateADLSAccessTierADLSAccountStatusADLSEncryptionTypesADLSLeaseStateADLSLeaseStatusADLSObjectArchiveStatusADLSObjectTypeADLSPerformanceADLSProvisionStateADLSReplicationTypeADLSStorageKindAPIQueryParamTypeEnumatlas_operationAtlasGlossaryCategoryTypeAtlasGlossaryTermTypeAtlasGlossaryTypeAtlasGlossaryTermAssignmentStatusAtlasGlossaryTermRelationshipStatusAuthPolicyCategoryAuthPolicyResourceCategoryAuthPolicyTypecertificate_statusDataGlossaryDataProductCriticalityDataProductSensitivityDataProductStatusDataProductVisibilityDomoCardTypeDynamoDBSecondaryIndexProjectionTypeDynamoDBStatusfile_typeFivetranConnectorStatusFivetranProcessStatusgoogle_datastudio_asset_typeicon_typeincident_severitykafka_topic_cleanup_policykafka_topic_compression_typematillion_job_typeModelCardinalityTypeMongoDBCollectionValidationActionMongoDBCollectionValidationLevelOpenLineageRunStatepowerbi_endorsementquery_username_strategyquick_sight_analysis_statusquick_sight_dataset_field_typequick_sight_dataset_import_modequick_sight_folder_typeSchemaRegistrySchemaCompatibilitySchemaRegistrySchemaTypeSourceCostUnitTypetable_typeWorkflowRunStatusWorkflowRunTypeWorkflowStatusWorkflowType

AdfActivityState

ADLSAccessTier

ADLSAccountStatus

ADLSEncryptionTypes

ADLSLeaseState

ADLSLeaseStatus

ADLSObjectArchiveStatus

ADLSObjectType

ADLSPerformance

ADLSProvisionState

ADLSReplicationType

ADLSStorageKind

APIQueryParamTypeEnum

atlas_operation

AtlasGlossaryCategoryType

AtlasGlossaryTermType

AtlasGlossaryType

AtlasGlossaryTermAssignmentStatus

AtlasGlossaryTermRelationshipStatus

AuthPolicyCategory

AuthPolicyResourceCategory

AuthPolicyType

certificate_status

DataGlossary

DataProductCriticality

DataProductSensitivity

DataProductStatus

DataProductVisibility

DomoCardType

DynamoDBSecondaryIndexProjectionType

DynamoDBStatus

file_type

FivetranConnectorStatus

FivetranProcessStatus

google_datastudio_asset_type

icon_type

incident_severity

kafka_topic_cleanup_policy

kafka_topic_compression_type

matillion_job_type

ModelCardinalityType

MongoDBCollectionValidationAction

MongoDBCollectionValidationLevel

OpenLineageRunState

powerbi_endorsement

query_username_strategy

quick_sight_analysis_status

quick_sight_dataset_field_type

quick_sight_dataset_import_mode

quick_sight_folder_type

SchemaRegistrySchemaCompatibility

SchemaRegistrySchemaType

SourceCostUnitType

table_type

WorkflowRunStatus

WorkflowRunType

WorkflowStatus

WorkflowType

AbstractionsAbstractionsBICloudInsightObjectStoreEventStoreDataQualityMetricNoSQLSchemaRegistry

BI

Cloud

Insight

ObjectStore

EventStore

DataQuality

Metric

NoSQL

SchemaRegistry

GlossaryGlossaryAtlasGlossaryAtlasGlossaryCategoryAtlasGlossaryTerm

AtlasGlossary

AtlasGlossaryCategory

AtlasGlossaryTerm

Data meshData meshDataDomainDataProductDataContractStakeholderStakeholderTitle

DataDomain

DataProduct

DataContract

Stakeholder

StakeholderTitle

Relational databasesRelational databasesDatabaseSchemaTableViewMaterialisedViewColumnQueryTablePartitionCalculationViewBigqueryTagDatabricksUnityCatalogTagSnowflakeDynamicTableSnowflakePipeSnowflakeStreamSnowflakeTagProcedureFunctionSQL

Database

Schema

Table

View

MaterialisedView

Column

Query

TablePartition

CalculationView

BigqueryTag

DatabricksUnityCatalogTag

SnowflakeDynamicTable

SnowflakePipe

SnowflakeStream

SnowflakeTag

Procedure

Function

SQL

Query organizationQuery organizationNamespaceCollectionFolder

Namespace

Collection

Folder

CubesCubesCubeCubeDimensionCubeHierarchyCubeField

Cube

CubeDimension

CubeHierarchy

CubeField

APIsAPIsAPIPathAPISpecAPIObjectAPIQueryAPIField

APIPath

APISpec

APIObject

APIQuery

APIField

AirflowAirflowAirflowDagAirflowTask

AirflowDag

AirflowTask

AmazonAmazonAmazon DynamoDBAmazon DynamoDBDynamoDBTableDynamoDBSecondaryIndexDynamoDBGlobalSecondaryIndexDynamoDBLocalSecondaryIndexAWS S3AWS S3S3BucketS3ObjectAmazon QuickSightAmazon QuickSightQuickSightAnalysisQuickSightAnalysisVisualQuickSightDashboardQuickSightDashboardVisualQuickSightDatasetQuickSightDatasetFieldQuickSightFolder

Amazon DynamoDBAmazon DynamoDBDynamoDBTableDynamoDBSecondaryIndexDynamoDBGlobalSecondaryIndexDynamoDBLocalSecondaryIndex

DynamoDBTable

DynamoDBSecondaryIndex

DynamoDBGlobalSecondaryIndex

DynamoDBLocalSecondaryIndex

AWS S3AWS S3S3BucketS3Object

S3Bucket

S3Object

Amazon QuickSightAmazon QuickSightQuickSightAnalysisQuickSightAnalysisVisualQuickSightDashboardQuickSightDashboardVisualQuickSightDatasetQuickSightDatasetFieldQuickSightFolder

QuickSightAnalysis

QuickSightAnalysisVisual

QuickSightDashboard

QuickSightDashboardVisual

QuickSightDataset

QuickSightDatasetField

QuickSightFolder

AnaplanAnaplanAnaplanWorkspaceAnaplanAppAnaplanPageAnaplanModelAnaplanModuleAnaplanListAnaplanSystemDimensionAnaplanDimensionAnaplanLineItemAnaplanView

AnaplanWorkspace

AnaplanApp

AnaplanPage

AnaplanModel

AnaplanModule

AnaplanList

AnaplanSystemDimension

AnaplanDimension

AnaplanLineItem

AnaplanView

AnomaloAnomaloAnomaloCheck

AnomaloCheck

AppAppApplicationApplicationField

Application

ApplicationField

Microsoft AzureMicrosoft AzureAzure Data FactoryAzure Data FactoryAdfActivityAdfDataflowAdfDatasetAdfLinkedserviceAdfPipelineAzure Data Lake StorageAzure Data Lake StorageADLSAccountADLSContainerADLSObjectAzure Event HubAzure Event HubAzureEventHubAzureEventHubConsumerGroupAzure Service BusAzure Service BusAzureServiceBusNamespaceAzureServiceBusSchemaAzureServiceBusTopicCosmos DBCosmos DBCosmosMongoDBAccountCosmosMongoDBCollectionCosmosMongoDBDatabase

Azure Data FactoryAzure Data FactoryAdfActivityAdfDataflowAdfDatasetAdfLinkedserviceAdfPipeline

AdfActivity

AdfDataflow

AdfDataset

AdfLinkedservice

AdfPipeline

Azure Data Lake StorageAzure Data Lake StorageADLSAccountADLSContainerADLSObject

ADLSAccount

ADLSContainer

ADLSObject

Azure Event HubAzure Event HubAzureEventHubAzureEventHubConsumerGroup

AzureEventHub

AzureEventHubConsumerGroup

Azure Service BusAzure Service BusAzureServiceBusNamespaceAzureServiceBusSchemaAzureServiceBusTopic

AzureServiceBusNamespace

AzureServiceBusSchema

AzureServiceBusTopic

Cosmos DBCosmos DBCosmosMongoDBAccountCosmosMongoDBCollectionCosmosMongoDBDatabase

CosmosMongoDBAccount

CosmosMongoDBCollection

CosmosMongoDBDatabase

CogniteCogniteCognite3DModelCogniteAssetCogniteEventCogniteFileCogniteSequenceCogniteTimeseries

Cognite3DModel

CogniteAsset

CogniteEvent

CogniteFile

CogniteSequence

CogniteTimeseries

CustomCustomCustomEntity

CustomEntity

DataverseDataverseDataverseEntityDataverseAttribute

DataverseEntity

DataverseAttribute

dbtdbtDbtColumnProcessDbtMetricDbtModelDbtModelColumnDbtProcessDbtSourceDbtTagDbtTest

DbtColumnProcess

DbtMetric

DbtModel

DbtModelColumn

DbtProcess

DbtSource

DbtTag

DbtTest

DomoDomoDomoCardDomoDashboardDomoDatasetDomoDatasetColumn

DomoCard

DomoDashboard

DomoDataset

DomoDatasetColumn

DocumentDBDocumentDBDocumentDBCollectionDocumentDBDatabase

DocumentDBCollection

DocumentDBDatabase

FivetranFivetranFivetranConnector

FivetranConnector

GoogleGoogleGoogle Cloud StorageGoogle Cloud StorageGCSBucketGCSObjectGoogle Data StudioGoogle Data StudioDataStudioAsset

Google Cloud StorageGoogle Cloud StorageGCSBucketGCSObject

GCSBucket

GCSObject

Google Data StudioGoogle Data StudioDataStudioAsset

DataStudioAsset

IBMIBMIBMCognosCognosCognosDashboardCognosDatasourceCognosExplorationCognosFileCognosFolderCognosModuleCognosPackageCognosReport

CognosCognosCognosDashboardCognosDatasourceCognosExplorationCognosFileCognosFolderCognosModuleCognosPackageCognosReport

CognosDashboard

CognosDatasource

CognosExploration

CognosFile

CognosFolder

CognosModule

CognosPackage

CognosReport

KafkaKafkaKafkaConsumerGroupKafkaTopic

KafkaConsumerGroup

KafkaTopic

LookerLookerLookerDashboardLookerExploreLookerFieldLookerFolderLookerLookLookerModelLookerProjectLookerQueryLookerTileLookerView

LookerDashboard

LookerExplore

LookerField

LookerFolder

LookerLook

LookerModel

LookerProject

LookerQuery

LookerTile

LookerView

MatillionMatillionMatillionComponentMatillionGroupMatillionJobMatillionProject

MatillionComponent

MatillionGroup

MatillionJob

MatillionProject

MetabaseMetabaseMetabaseCollectionMetabaseDashboardMetabaseQuestion

MetabaseCollection

MetabaseDashboard

MetabaseQuestion

MicroStrategyMicroStrategyMicroStrategyAttributeMicroStrategyCubeMicroStrategyDocumentMicroStrategyDossierMicroStrategyFactMicroStrategyMetricMicroStrategyProjectMicroStrategyReportMicroStrategyVisualization

MicroStrategyAttribute

MicroStrategyCube

MicroStrategyDocument

MicroStrategyDossier

MicroStrategyFact

MicroStrategyMetric

MicroStrategyProject

MicroStrategyReport

MicroStrategyVisualization

ModeModeModeChartModeCollectionModeQueryModeReportModeWorkspace

ModeChart

ModeCollection

ModeQuery

ModeReport

ModeWorkspace

ModelsModelsModelAttributeModelAttributeAssociationModelDataModelModelEntityModelEntityAssociationModelVersion

ModelAttribute

ModelAttributeAssociation

ModelDataModel

ModelEntity

ModelEntityAssociation

ModelVersion

MongoDBMongoDBMongoDBCollectionMongDBDatabase

MongoDBCollection

MongDBDatabase

Monte CarloMonte CarloMCIncidentMCMonitor

MCIncident

MCMonitor

Power BIPower BIPower BIPowerBIColumnPowerBIDashboardPowerBIDataflowPowerBIDataflowEntityColumnPowerBIDatasetPowerBIDatasourcePowerBIMeasurePowerBIPagePowerBIReportPowerBITablePowerBITilePowerBIWorkspace

PowerBIColumn

PowerBIDashboard

PowerBIDataflow

PowerBIDataflowEntityColumn

PowerBIDataset

PowerBIDatasource

PowerBIMeasure

PowerBIPage

PowerBIReport

PowerBITable

PowerBITile

PowerBIWorkspace

PresetPresetPresetChartPresetDashboardPresetDatasetPresetWorkspace

PresetChart

PresetDashboard

PresetDataset

PresetWorkspace

QlikQlikQlikAppQlikChartQlikDatasetQlikSheetQlikSpaceQlikStream

QlikApp

QlikChart

QlikDataset

QlikSheet

QlikSpace

QlikStream

RedashRedashRedashDashboardRedashQueryRedashVisualization

RedashDashboard

RedashQuery

RedashVisualization

SalesforceSalesforceSalesforceDashboardSalesforceFieldSalesforceObjectSalesforceOrganizationSalesforceReportSaaS

SalesforceDashboard

SalesforceField

SalesforceObject

SalesforceOrganization

SalesforceReport

SaaS

SigmaSigmaSigmaWorkbookSigmaPageSigmaDataElementSigmaDataElementFieldSigmaDatasetSigmaDatasetColumn

SigmaWorkbook

SigmaPage

SigmaDataElement

SigmaDataElementField

SigmaDataset

SigmaDatasetColumn

SisenseSisenseSisenseDashboardSisenseDatamodelSisenseDatamodelTableSisenseFolderSisenseWidget

SisenseDashboard

SisenseDatamodel

SisenseDatamodelTable

SisenseFolder

SisenseWidget

SodaSodaSodaCheck

SodaCheck

SparkSparkSparkJob

SparkJob

SupersetSupersetSupersetChartSupersetDashboardSupersetDataset

SupersetChart

SupersetDashboard

SupersetDataset

TableauTableauTableauCalculatedFieldTableauDashboardTableauDatasourceTableauDatasourceFieldTableauFlowTableauMetricTableauProjectTableauSiteTableauWorkbookTableauWorksheet

TableauCalculatedField

TableauDashboard

TableauDatasource

TableauDatasourceField

TableauFlow

TableauMetric

TableauProject

TableauSite

TableauWorkbook

TableauWorksheet

ThoughtSpotThoughtSpotThoughtspotAnswerThoughtspotColumnThoughtspotDashletThoughtspotLiveboardThoughtspotTableThoughtspotViewThoughtspotWorksheet

ThoughtspotAnswer

ThoughtspotColumn

ThoughtspotDashlet

ThoughtspotLiveboard

ThoughtspotTable

ThoughtspotView

ThoughtspotWorksheet

Endpoints

Create lineage between assetsDirectlyUsing OpenLineage

Directly

Using OpenLineage

Create lineage between columnsDirectlyUsing OpenLineage

Directly

Using OpenLineage

Remove lineage between assets



## Create lineage between assetsÂ¶
(source: https://developer.atlan.com/snippets/common-examples/lineage/manage/)



### DirectlyÂ¶
(source: https://developer.atlan.com/snippets/common-examples/lineage/manage/)

7.0.04.0.0

To create lineage between assets, you need to create aProcessentity.

Process

Input and output assets must already exist

Note that the assets you reference as the inputs and outputs of the process must already exist, before creating the process.

123456789101112131415161718

LineageProcessprocess=LineageProcess.creator(// (1)"Source 1, Source 2, Source 3 -> Target 1, Target 2",// (2)"default/snowflake/1657025257",// (3)"dag_123",// (4)List.of(// (5)Table.refByGuid("495b1516-aaaf-4390-8cfd-b11ade7a7799"),Table.refByGuid("d002dead-1655-4d75-abd6-ad889fa04bd4"),Table.refByQualifiedName("default/snowflake/1657025257/OPS/DEFAULT/RUN_STATS")),List.of(// (6)Table.refByGuid("86d9a061-7753-4884-b988-a02d3954bc24"),Table.refByQualifiedName("default/snowflake/1657025257/OPS/DEFAULT/FULL_STATS")),null)// (7).sql("select * from somewhere;")// (8).sourceURL("https://your.orchestrator/unique/id/123")// (9).build();AssetMutationResponseresponse=process.save(client);// (10)assertresponse.getCreatedAssets().size()==1// (11)assertresponse.getUpdatedAssets().size()==5// (12)

LineageProcessprocess=LineageProcess.creator(// (1)"Source 1, Source 2, Source 3 -> Target 1, Target 2",// (2)"default/snowflake/1657025257",// (3)"dag_123",// (4)List.of(// (5)Table.refByGuid("495b1516-aaaf-4390-8cfd-b11ade7a7799"),Table.refByGuid("d002dead-1655-4d75-abd6-ad889fa04bd4"),Table.refByQualifiedName("default/snowflake/1657025257/OPS/DEFAULT/RUN_STATS")),List.of(// (6)Table.refByGuid("86d9a061-7753-4884-b988-a02d3954bc24"),Table.refByQualifiedName("default/snowflake/1657025257/OPS/DEFAULT/FULL_STATS")),null)// (7).sql("select * from somewhere;")// (8).sourceURL("https://your.orchestrator/unique/id/123")// (9).build();AssetMutationResponseresponse=process.save(client);// (10)assertresponse.getCreatedAssets().size()==1// (11)assertresponse.getUpdatedAssets().size()==5// (12)

Use thecreator()method to initialize the object with allnecessary attributes for creating it.

creator()

Provide a name for how the process will be shown in the UI.

Provide thequalifiedNameof the connection that ran the process.Tips for the connectionThe process itself must be created within a connection for both access control and icon labelling. Use a connectionqualifiedNamethat indicates the system that ran the process:You could use the same connectionqualifiedNameas the source system, if it was the source system "pushing" data to the target(s).You could use the same connectionqualifiedNameas the target system, if it was the target system "pulling" data from the source(s).You could use a different connectionqualifiedNamefrom either source or target, if there is a system in-between doing the processing (for example an ETL engine or orchestrator).

Provide thequalifiedNameof the connection that ran the process.

qualifiedName

Tips for the connection

The process itself must be created within a connection for both access control and icon labelling. Use a connectionqualifiedNamethat indicates the system that ran the process:

qualifiedName

You could use the same connectionqualifiedNameas the source system, if it was the source system "pushing" data to the target(s).

qualifiedName

You could use the same connectionqualifiedNameas the target system, if it was the target system "pulling" data from the source(s).

qualifiedName

You could use a different connectionqualifiedNamefrom either source or target, if there is a system in-between doing the processing (for example an ETL engine or orchestrator).

qualifiedName

(Optional) Provide the unique ID of the process within that connection. This could be the unique DAG ID for an orchestrator, for example. Since it is optional, you can also sendnulland the SDK will generate a unique ID for you based on the unique combination of inputs and outputs for the process.Use your own ID if you canWhile the SDK can generate this ID for you, since it is based on the unique combination of inputs and outputs the ID can change if those inputs or outputs change. This could result in extra processes in lineage as this process itself changes over time.By using your own ID for the process, any changes that occur in that process over time (even if the inputs or outputs change) the same single process in Atlan will be updated.

(Optional) Provide the unique ID of the process within that connection. This could be the unique DAG ID for an orchestrator, for example. Since it is optional, you can also sendnulland the SDK will generate a unique ID for you based on the unique combination of inputs and outputs for the process.

null

Use your own ID if you can

While the SDK can generate this ID for you, since it is based on the unique combination of inputs and outputs the ID can change if those inputs or outputs change. This could result in extra processes in lineage as this process itself changes over time.

By using your own ID for the process, any changes that occur in that process over time (even if the inputs or outputs change) the same single process in Atlan will be updated.

Provide the list of inputs to the process. Note that each of these is only aReferenceto an asset, not a full asset object. For a reference you only need (in addition to the type of asset) either:its GUID (for the static<Type>.refByGuid()method)itsqualifiedName(for the static<Type>.refByQualifiedName()method)

Provide the list of inputs to the process. Note that each of these is only aReferenceto an asset, not a full asset object. For a reference you only need (in addition to the type of asset) either:

Reference

its GUID (for the static<Type>.refByGuid()method)

<Type>.refByGuid()

itsqualifiedName(for the static<Type>.refByQualifiedName()method)

qualifiedName

<Type>.refByQualifiedName()

Provide the list of outputs to the process. Note that each of these is again only aReferenceto an asset.

Provide the list of outputs to the process. Note that each of these is again only aReferenceto an asset.

Reference

(Optional) Provide the parentLineageProcessin which this process ran (for example, if this process is a subprocess of some higher-level process). If this is a top-level process, you can also sendnullfor this parameter (as in this example).

LineageProcess

null

(Optional) You can also add other properties to the lineage process, such as SQL code that runs within the process.

(Optional) You can also provide a link to the process, which will provide a button to click to go to that link from the Atlan UI when viewing the process in Atlan.

Call thesave()method to actually create the process. Because this operation will directly persist the asset in Atlan, you mustprovide it anAtlanClientthrough which to connect to the tenant.

save()

AtlanClient

The response will include that single lineage process asset that was created.

The response will also include the 5 data assets (3 inputs, 2 outputs) that were updated.

12345678910111213141516171819202122232425

frompyatlan.client.atlanimportAtlanClientfrompyatlan.model.assetsimportProcess,Tableclient=AtlanClient()process=Process.creator(# (1)name="Source 1, Source 2, Source 3 -> Target 1, Target 2",# (2)connection_qualified_name="default/snowflake/1657025257",# (3)process_id="dag_123",# (4)inputs=[# (5)Table.ref_by_guid(guid="495b1516-aaaf-4390-8cfd-b11ade7a7799"),Table.ref_by_guid(guid="d002dead-1655-4d75-abd6-ad889fa04bd4"),Table.ref_by_qualified_name(qualified_name="default/snowflake/1657025257/OPS/DEFAULT/RUN_STATS"),],outputs=[# (6)Table.ref_by_guid(guid="86d9a061-7753-4884-b988-a02d3954bc24"),Table.ref_by_qualified_name(qualified_name="default/snowflake/1657025257/OPS/DEFAULT/FULL_STATS"),],)# (7)process.sql="select * from somewhere;"# (8)process.source_url="https://your.orchestrator/unique/id/123"# (9)response=client.asset.save(process)# (10)assert(processes:=response.assets_created(Process))# (11)assertlen(processes)==1# (12)assert(tables:=response.assets_updated(Table))# (13)assertlen(tables)==2# (14)

frompyatlan.client.atlanimportAtlanClientfrompyatlan.model.assetsimportProcess,Tableclient=AtlanClient()process=Process.creator(# (1)name="Source 1, Source 2, Source 3 -> Target 1, Target 2",# (2)connection_qualified_name="default/snowflake/1657025257",# (3)process_id="dag_123",# (4)inputs=[# (5)Table.ref_by_guid(guid="495b1516-aaaf-4390-8cfd-b11ade7a7799"),Table.ref_by_guid(guid="d002dead-1655-4d75-abd6-ad889fa04bd4"),Table.ref_by_qualified_name(qualified_name="default/snowflake/1657025257/OPS/DEFAULT/RUN_STATS"),],outputs=[# (6)Table.ref_by_guid(guid="86d9a061-7753-4884-b988-a02d3954bc24"),Table.ref_by_qualified_name(qualified_name="default/snowflake/1657025257/OPS/DEFAULT/FULL_STATS"),],)# (7)process.sql="select * from somewhere;"# (8)process.source_url="https://your.orchestrator/unique/id/123"# (9)response=client.asset.save(process)# (10)assert(processes:=response.assets_created(Process))# (11)assertlen(processes)==1# (12)assert(tables:=response.assets_updated(Table))# (13)assertlen(tables)==2# (14)

Use thecreate()method to initialize the object with allnecessary attributes for creating it.

create()

Provide a name for how the process will be shown in the UI.

Provide thequalified_nameof the connection that ran the process.Tips for the connectionThe process itself must be created within a connection for both access control and icon labelling. Use a connectionqualified_namethat indicates the system that ran the process:You could use the same connectionqualified_nameas the source system, if it was the source system "pushing" data to the target(s).You could use the same connectionqualified_nameas the target system, if it was the target system "pulling" data from the source(s).You could use a different connectionqualified_namefrom either source or target, if there is a system in-between doing the processing (for example an ETL engine or orchestrator).

Provide thequalified_nameof the connection that ran the process.

qualified_name

Tips for the connection

The process itself must be created within a connection for both access control and icon labelling. Use a connectionqualified_namethat indicates the system that ran the process:

qualified_name

You could use the same connectionqualified_nameas the source system, if it was the source system "pushing" data to the target(s).

qualified_name

You could use the same connectionqualified_nameas the target system, if it was the target system "pulling" data from the source(s).

qualified_name

You could use a different connectionqualified_namefrom either source or target, if there is a system in-between doing the processing (for example an ETL engine or orchestrator).

qualified_name

(Optional) Provide the unique ID of the process within that connection. This could be the unique DAG ID for an orchestrator, for example. Since it is optional, you can also leave it out and the SDK will generate a unique ID for you based on the unique combination of inputs and outputs for the process.Use your own ID if you canWhile the SDK can generate this ID for you, since it is based on the unique combination of inputs and outputs the ID can change if those inputs or outputs change. This could result in extra processes in lineage as this process itself changes over time.By using your own ID for the process, any changes that occur in that process over time (even if the inputs or outputs change) the same single process in Atlan will be updated.

(Optional) Provide the unique ID of the process within that connection. This could be the unique DAG ID for an orchestrator, for example. Since it is optional, you can also leave it out and the SDK will generate a unique ID for you based on the unique combination of inputs and outputs for the process.

Use your own ID if you can

While the SDK can generate this ID for you, since it is based on the unique combination of inputs and outputs the ID can change if those inputs or outputs change. This could result in extra processes in lineage as this process itself changes over time.

By using your own ID for the process, any changes that occur in that process over time (even if the inputs or outputs change) the same single process in Atlan will be updated.

Provide the list of inputs to the process. Note that each of these is only aReferenceto an asset, not a full asset object. For a reference you only need (in addition to the type of asset) either:its GUID (for theref_by_guid()method)itsqualifiedName(for theref_by_qualified_name()method)

Provide the list of inputs to the process. Note that each of these is only aReferenceto an asset, not a full asset object. For a reference you only need (in addition to the type of asset) either:

Reference

its GUID (for theref_by_guid()method)

ref_by_guid()

itsqualifiedName(for theref_by_qualified_name()method)

qualifiedName

ref_by_qualified_name()

Provide the list of outputs to the process. Note that each of these is again only aReferenceto an asset.

Provide the list of outputs to the process. Note that each of these is again only aReferenceto an asset.

Reference

(Optional) Provide the parentProcessin which this process ran (for example, if this process is a subprocess of some higher-level process). If this is a top-level process, you can also sendNonefor this parameter (as in this example).

Process

None

(Optional) You can also add other properties to the lineage process, such as SQL code that runs within the process.

(Optional) You can also provide a link to the process, which will provide a button to click to go to that link from the Atlan UI when viewing the process in Atlan.

Call thesave()method to actually create the process.

save()

Check that aProcesswas created.

Process

Check that only 1Processwas created.

Process

Check that tables were updated.

Check that 5 tables (3 inputs, 2 outputs) were updated.

123456789101112131415161718

valprocess=LineageProcess.creator(// (1)"Source 1, Source 2, Source 3 -> Target 1, Target 2",// (2)"default/snowflake/1657025257",// (3)"dag_123",// (4)listOf<ICatalog>(// (5)Table.refByGuid("495b1516-aaaf-4390-8cfd-b11ade7a7799"),Table.refByGuid("d002dead-1655-4d75-abd6-ad889fa04bd4"),Table.refByQualifiedName("default/snowflake/1657025257/OPS/DEFAULT/RUN_STATS")),listOf<ICatalog>(// (6)Table.refByGuid("86d9a061-7753-4884-b988-a02d3954bc24"),Table.refByQualifiedName("default/snowflake/1657025257/OPS/DEFAULT/FULL_STATS")),null)// (7).sql("select * from somewhere;")// (8).sourceURL("https://your.orchestrator/unique/id/123")// (9).build()valresponse=process.save(client)// (10)assert(response.createdAssets.size==1)// (11)assert(response.updatedAssets.size==5)// (12)

valprocess=LineageProcess.creator(// (1)"Source 1, Source 2, Source 3 -> Target 1, Target 2",// (2)"default/snowflake/1657025257",// (3)"dag_123",// (4)listOf<ICatalog>(// (5)Table.refByGuid("495b1516-aaaf-4390-8cfd-b11ade7a7799"),Table.refByGuid("d002dead-1655-4d75-abd6-ad889fa04bd4"),Table.refByQualifiedName("default/snowflake/1657025257/OPS/DEFAULT/RUN_STATS")),listOf<ICatalog>(// (6)Table.refByGuid("86d9a061-7753-4884-b988-a02d3954bc24"),Table.refByQualifiedName("default/snowflake/1657025257/OPS/DEFAULT/FULL_STATS")),null)// (7).sql("select * from somewhere;")// (8).sourceURL("https://your.orchestrator/unique/id/123")// (9).build()valresponse=process.save(client)// (10)assert(response.createdAssets.size==1)// (11)assert(response.updatedAssets.size==5)// (12)

Use thecreator()method to initialize the object with allnecessary attributes for creating it.

creator()

Provide a name for how the process will be shown in the UI.

Provide thequalifiedNameof the connection that ran the process.Tips for the connectionThe process itself must be created within a connection for both access control and icon labelling. Use a connectionqualifiedNamethat indicates the system that ran the process:You could use the same connectionqualifiedNameas the source system, if it was the source system "pushing" data to the target(s).You could use the same connectionqualifiedNameas the target system, if it was the target system "pulling" data from the source(s).You could use a different connectionqualifiedNamefrom either source or target, if there is a system in-between doing the processing (for example an ETL engine or orchestrator).

Provide thequalifiedNameof the connection that ran the process.

qualifiedName

Tips for the connection

The process itself must be created within a connection for both access control and icon labelling. Use a connectionqualifiedNamethat indicates the system that ran the process:

qualifiedName

You could use the same connectionqualifiedNameas the source system, if it was the source system "pushing" data to the target(s).

qualifiedName

You could use the same connectionqualifiedNameas the target system, if it was the target system "pulling" data from the source(s).

qualifiedName

You could use a different connectionqualifiedNamefrom either source or target, if there is a system in-between doing the processing (for example an ETL engine or orchestrator).

qualifiedName

(Optional) Provide the unique ID of the process within that connection. This could be the unique DAG ID for an orchestrator, for example. Since it is optional, you can also sendnulland the SDK will generate a unique ID for you based on the unique combination of inputs and outputs for the process.Use your own ID if you canWhile the SDK can generate this ID for you, since it is based on the unique combination of inputs and outputs the ID can change if those inputs or outputs change. This could result in extra processes in lineage as this process itself changes over time.By using your own ID for the process, any changes that occur in that process over time (even if the inputs or outputs change) the same single process in Atlan will be updated.

(Optional) Provide the unique ID of the process within that connection. This could be the unique DAG ID for an orchestrator, for example. Since it is optional, you can also sendnulland the SDK will generate a unique ID for you based on the unique combination of inputs and outputs for the process.

null

Use your own ID if you can

While the SDK can generate this ID for you, since it is based on the unique combination of inputs and outputs the ID can change if those inputs or outputs change. This could result in extra processes in lineage as this process itself changes over time.

By using your own ID for the process, any changes that occur in that process over time (even if the inputs or outputs change) the same single process in Atlan will be updated.

Provide the list of inputs to the process. Note that each of these is only aReferenceto an asset, not a full asset object. For a reference you only need (in addition to the type of asset) either:its GUID (for the static<Type>.refByGuid()method)itsqualifiedName(for the static<Type>.refByQualifiedName()method)

Provide the list of inputs to the process. Note that each of these is only aReferenceto an asset, not a full asset object. For a reference you only need (in addition to the type of asset) either:

Reference

its GUID (for the static<Type>.refByGuid()method)

<Type>.refByGuid()

itsqualifiedName(for the static<Type>.refByQualifiedName()method)

qualifiedName

<Type>.refByQualifiedName()

Provide the list of outputs to the process. Note that each of these is again only aReferenceto an asset.

Provide the list of outputs to the process. Note that each of these is again only aReferenceto an asset.

Reference

(Optional) Provide the parentLineageProcessin which this process ran (for example, if this process is a subprocess of some higher-level process). If this is a top-level process, you can also sendnullfor this parameter (as in this example).

LineageProcess

null

(Optional) You can also add other properties to the lineage process, such as SQL code that runs within the process.

(Optional) You can also provide a link to the process, which will provide a button to click to go to that link from the Atlan UI when viewing the process in Atlan.

Call thesave()method to actually create the process. Because this operation will directly persist the asset in Atlan, you mustprovide it anAtlanClientthrough which to connect to the tenant.

save()

AtlanClient

The response will include that single lineage process asset that was created.

The response will also include the 5 data assets (3 inputs, 2 outputs) that were updated.

123456789101112131415161718192021222324252627282930313233343536373839

{"entities":[// (1){"typeName":"Process",// (2)"attributes":{"name":"Source 1, Source 2, Source 3 -> Target 1, Target 2",// (3)"qualifiedName":"default/snowflake/1657025257/dag_123",// (4)"inputs":[// (5){"typeName":"Table","guid":"495b1516-aaaf-4390-8cfd-b11ade7a7799"},{"typeName":"Table","guid":"d002dead-1655-4d75-abd6-ad889fa04bd4"},{"typeName":"Table","uniqueAttributes":{"qualifiedName":"default/snowflake/1657025257/OPS/DEFAULT/RUN_STATS"}}],"outputs":[// (6){"typeName":"Table","guid":"86d9a061-7753-4884-b988-a02d3954bc24"},{"typeName":"Table","uniqueAttributes":{"qualifiedName":"default/snowflake/1657025257/OPS/DEFAULT/FULL_STATS"}}]}}]}

{"entities":[// (1){"typeName":"Process",// (2)"attributes":{"name":"Source 1, Source 2, Source 3 -> Target 1, Target 2",// (3)"qualifiedName":"default/snowflake/1657025257/dag_123",// (4)"inputs":[// (5){"typeName":"Table","guid":"495b1516-aaaf-4390-8cfd-b11ade7a7799"},{"typeName":"Table","guid":"d002dead-1655-4d75-abd6-ad889fa04bd4"},{"typeName":"Table","uniqueAttributes":{"qualifiedName":"default/snowflake/1657025257/OPS/DEFAULT/RUN_STATS"}}],"outputs":[// (6){"typeName":"Table","guid":"86d9a061-7753-4884-b988-a02d3954bc24"},{"typeName":"Table","uniqueAttributes":{"qualifiedName":"default/snowflake/1657025257/OPS/DEFAULT/FULL_STATS"}}]}}]}

All assets must be wrapped in anentitiesarray.

entities

You must provide the exact type name for aProcessasset (case-sensitive).

Process

You must provide a name of the integration process.

You must provide a uniquequalifiedNamefor the integration process (case-sensitive).

qualifiedName

You must list all of the input assets to the process. These can be referenced by GUID or byqualifiedName.

qualifiedName

You must list all of the output assets from the process. These can also be referenced by either GUID orqualifiedName.

qualifiedName



### Using OpenLineageÂ¶
(source: https://developer.atlan.com/snippets/common-examples/lineage/manage/)



#### Creating connection for OpenLineageÂ¶
(source: https://developer.atlan.com/snippets/common-examples/lineage/manage/)

6.0.0

You must first configure OpenLineage before creating lineage between assets. You can either configure aSpark Assetsconnection in Atlan before sending any OpenLineage events. (You can skip theConfigure the integration in Apache Sparksection), or you can follow the steps below to create the Spark connection via SDKs.

Coming soon

123456789101112

frompyatlan.client.atlanimportAtlanClientfrompyatlan.model.enumsimportAtlanConnectorTypeclient=AtlanClient()admin_role_guid=client.role_cache.get_id_for_name("$admin")#(1)spark_connection=client.open_lineage.create_connection(#(2)name="open_lineage_connection",connector_type=AtlanConnectorType.SPARK,admin_roles=[admin_role_guid],admin_users=["jsmith"],admin_groups=["group2"],)

frompyatlan.client.atlanimportAtlanClientfrompyatlan.model.enumsimportAtlanConnectorTypeclient=AtlanClient()admin_role_guid=client.role_cache.get_id_for_name("$admin")#(1)spark_connection=client.open_lineage.create_connection(#(2)name="open_lineage_connection",connector_type=AtlanConnectorType.SPARK,admin_roles=[admin_role_guid],admin_users=["jsmith"],admin_groups=["group2"],)

Retrieve the GUID for the admin role, to use later for defining the roles that can administer the connection.

To create OpenLineage connection using theopen_lineage.create_connection()method. Below params are required:name: Provide a human-readable name for your connections.connector_type: Set the type of connection. Defaults toAtlanConnectorType.SPARK.(Optional)admin_roles: List the workspace roles that should be able to administer the connection (if any, defaults toNone). All users with that workspace role (current and future) will be administrators of the connection. Note that the values here need to be the GUID(s) of the workspace role(s). At least one of admin_roles, admin_groups, or admin_users must be provided.(Optional)admin_users: List the user names that can administer this connection (if any, defaults toNone). Note that the values here are the username(s) of the user(s). At least one of admin_roles, admin_groups, or admin_users must be provided.(Optional)admin_groups: List the group names that can administer this connection (if any, defaults toNone). All users within that group (current and future) will be administrators of the connection. Note that the values here are the name(s) of the group(s). At least one of admin_roles, admin_groups, or admin_users must be provided.WarningNote: At least one of the optional parametersadmin_roles,admin_users, oradmin_groupsmust be provided to successfully create the connection.

To create OpenLineage connection using theopen_lineage.create_connection()method. Below params are required:

open_lineage.create_connection()

name: Provide a human-readable name for your connections.

name

connector_type: Set the type of connection. Defaults toAtlanConnectorType.SPARK.

connector_type

AtlanConnectorType.SPARK

(Optional)admin_roles: List the workspace roles that should be able to administer the connection (if any, defaults toNone). All users with that workspace role (current and future) will be administrators of the connection. Note that the values here need to be the GUID(s) of the workspace role(s). At least one of admin_roles, admin_groups, or admin_users must be provided.

admin_roles

None

(Optional)admin_users: List the user names that can administer this connection (if any, defaults toNone). Note that the values here are the username(s) of the user(s). At least one of admin_roles, admin_groups, or admin_users must be provided.

admin_users

None

(Optional)admin_groups: List the group names that can administer this connection (if any, defaults toNone). All users within that group (current and future) will be administrators of the connection. Note that the values here are the name(s) of the group(s). At least one of admin_roles, admin_groups, or admin_users must be provided.WarningNote: At least one of the optional parametersadmin_roles,admin_users, oradmin_groupsmust be provided to successfully create the connection.

(Optional)admin_groups: List the group names that can administer this connection (if any, defaults toNone). All users within that group (current and future) will be administrators of the connection. Note that the values here are the name(s) of the group(s). At least one of admin_roles, admin_groups, or admin_users must be provided.

admin_groups

None

Warning

Note: At least one of the optional parametersadmin_roles,admin_users, oradmin_groupsmust be provided to successfully create the connection.

admin_roles

admin_users

admin_groups

Coming soon

12345678910111213

{"authType":"atlan_api_key",// (1)"name":"default-spark-1716979138-0",//(2)"connector":"spark",// (3)"connectorConfigName":"atlan-connectors-spark",// (4)"connectorType":"event",// (5)"extra":{"events.enable-partial-assets":true,"events.enabled":true,"events.topic":"openlineage_spark",// (6)"events.urlPath":"/events/openlineage/spark/api/v1/lineage"// (7)}}

{"authType":"atlan_api_key",// (1)"name":"default-spark-1716979138-0",//(2)"connector":"spark",// (3)"connectorConfigName":"atlan-connectors-spark",// (4)"connectorType":"event",// (5)"extra":{"events.enable-partial-assets":true,"events.enabled":true,"events.topic":"openlineage_spark",// (6)"events.urlPath":"/events/openlineage/spark/api/v1/lineage"// (7)}}

TheauthTypemust be exactlyatlan_api_key.

authType

atlan_api_key

Human-readable name for your credential which should follow the pattern:default-spark-<epoch>-0, where<epoch>is the time in milliseconds at which the credential is being created.

default-spark-<epoch>-0

<epoch>

Theconnectormust be exactlyspark.

connector

spark

TheconnectorConfigNamemust be exactlyatlan-connectors-spark.

connectorConfigName

atlan-connectors-spark

TheconnectorTypemust be exactlyevent.

connectorType

event

Theevents.topicmust be exactlyopenlineage_spark.

events.topic

openlineage_spark

Theevents.urlPathmust be exactly/events/openlineage/spark/api/v1/lineages.

events.urlPath

/events/openlineage/spark/api/v1/lineages

1234567891011121314151617181920212223

{"entities":[{"typeName":"Connection",// (1)"attributes":{"name":"open_lineage_connection",// (2)"connectorName":"spark",// (3)"qualifiedName":"default/spark/123456789",// (4)"category":"connector",// (5)"defaultCredentialGuid":"8b579147-6054-4a4c-8137-463cd349b393",// (6)"adminRoles":[// (7)"e7ae0295-c60a-469a-bd2c-fb903943aa02"],"adminGroups":[// (8)"group2"],"adminUsers":[// (9)"jsmith"]}}]}

{"entities":[{"typeName":"Connection",// (1)"attributes":{"name":"open_lineage_connection",// (2)"connectorName":"spark",// (3)"qualifiedName":"default/spark/123456789",// (4)"category":"connector",// (5)"defaultCredentialGuid":"8b579147-6054-4a4c-8137-463cd349b393",// (6)"adminRoles":[// (7)"e7ae0295-c60a-469a-bd2c-fb903943aa02"],"adminGroups":[// (8)"group2"],"adminUsers":[// (9)"jsmith"]}}]}

ThetypeNamemust be exactlyConnection.

typeName

Connection

Human-readable name for your connection, such asproductionordevelopment.

production

development

TheconnectorNamemust be exactlyspark.Determines the iconThis determines the icon that Atlan will use for all the assets in the connection. If you use a value that isnota known value, you will have a default gear icon instead.

TheconnectorNamemust be exactlyspark.

connectorName

spark

Determines the icon

This determines the icon that Atlan will use for all the assets in the connection. If you use a value that isnota known value, you will have a default gear icon instead.

ThequalifiedNameshould follow the pattern:default/spark/<epoch>, where<epoch>is the time in milliseconds at which the connection is being created.

ThequalifiedNameshould follow the pattern:default/spark/<epoch>, where<epoch>is the time in milliseconds at which the connection is being created.

qualifiedName

default/spark/<epoch>

<epoch>

Thecategorymust be exactlyconnector.

category

connector

ThedefaultCredentialGuidshould be obtained from theidin the response of the previous request.

defaultCredentialGuid

id

List any workspace roles that can administer this connection. All users with that workspace role (current and future) will be administrators of the connection. Note that the values here need to be the GUID(s) of the workspace role(s).At least one ofadminRoles,adminGroups, oradminUsersmust be provided.

adminRoles

adminGroups

adminUsers

List any groups that can administer this connection. All users within that group (current and future) will be administrators of the connection. Note that the values here are the name(s) of the group(s).At least one ofadminRoles,adminGroups, oradminUsersmust be provided.

adminRoles

adminGroups

adminUsers

List any users that can administer this connection. Note that the values here are the username(s) of the user(s).At least one ofadminRoles,adminGroups, oradminUsersmust be provided.

adminRoles

adminGroups

adminUsers



#### Creating lineage between assets using OpenLineageÂ¶
(source: https://developer.atlan.com/snippets/common-examples/lineage/manage/)

2.5.14.0.0

To create lineage between assets throughOpenLineage, you need to sendat leasttwo events: one indicating the start of a job run and the other indicating that job run is finished.

12345678910111213141516171819202122

Stringsnowflake="snowflake://abc123.snowflakecomputing.com";// (1)OpenLineageJobolj=OpenLineageJob.creator(// (2)"ol-spark","dag_123","https://your.orchestrator/unique/id/123").build();OpenLineageRunolr=OpenLineageRun.creator(olj).build();// (3)OpenLineageInputDatasetinputDataset=olj.createInput(snowflake,"OPS.DEFAULT.RUN_STATS").build();// (4)OpenLineageOutputDatasetoutputDataset=olj.createOutput(snowflake,"OPS.DEFAULT.FULL_STATS").build();// (5)OpenLineageEventstart=OpenLineageEvent.creator(// (6)olr,OpenLineage.RunEvent.EventType.START).input(inputDataset)// (7).input(olj.createInput(snowflake,"SOME.OTHER.TBL").build()).input(olj.createInput(snowflake,"AN.OTHER.TBL").build()).output(outputDataset)// (8).output(olj.createOutput(snowflake,"AN.OTHER.VIEW").build()).build();start.emit(client);// (9)

Stringsnowflake="snowflake://abc123.snowflakecomputing.com";// (1)OpenLineageJobolj=OpenLineageJob.creator(// (2)"ol-spark","dag_123","https://your.orchestrator/unique/id/123").build();OpenLineageRunolr=OpenLineageRun.creator(olj).build();// (3)OpenLineageInputDatasetinputDataset=olj.createInput(snowflake,"OPS.DEFAULT.RUN_STATS").build();// (4)OpenLineageOutputDatasetoutputDataset=olj.createOutput(snowflake,"OPS.DEFAULT.FULL_STATS").build();// (5)OpenLineageEventstart=OpenLineageEvent.creator(// (6)olr,OpenLineage.RunEvent.EventType.START).input(inputDataset)// (7).input(olj.createInput(snowflake,"SOME.OTHER.TBL").build()).input(olj.createInput(snowflake,"AN.OTHER.TBL").build()).output(outputDataset)// (8).output(olj.createOutput(snowflake,"AN.OTHER.VIEW").build()).build();start.emit(client);// (9)

Datasets used in data lineage need anamespacethat follows thesource-specific naming standards of OpenLineage.

namespace

Lineage is tracked through jobs. Each job must have:the name of a connection (that already exists in Atlan),a unique job name (used to idempotently update the same job with multiple runs), anda unique URI indicating the code or system responsible for producing this lineage.

the name of a connection (that already exists in Atlan),

a unique job name (used to idempotently update the same job with multiple runs), and

a unique URI indicating the code or system responsible for producing this lineage.

A job must be run at least once for any lineage to exist, and these separate runs of the same job are tracked throughOpenLineageRunobjects.

OpenLineageRun

You can define any number of inputs (sources) for lineage. Thenameof a dataset should use a.-qualified form. For example, a table should beDATABASE_NAME.SCHEMA_NAME.TABLE_NAME.

name

.

DATABASE_NAME.SCHEMA_NAME.TABLE_NAME

You can define any number of outputs (targets) for lineage. Thenameof a dataset should use a.-qualified form. For example, a table should beDATABASE_NAME.SCHEMA_NAME.TABLE_NAME.

name

.

DATABASE_NAME.SCHEMA_NAME.TABLE_NAME

Each run of a job must consist ofat leasttwo events â aSTARTevent indicating when the job ran began, and some terminal state indicating when the job run finished.

START

You can chain any number ofinputs to the event to indicate the source datasets for the lineage.

input

You can chain any number ofoutputs to the event to indicate the target datasets for the lineage.

output

Use theemit()method to actually send the event to Atlan to be processed. The processing itself occurs asynchronously, so a successfulemit()will only indicate that the event has been successfully sent to Atlan, not that it has (yet) been processed. Because this operation will directly persist the asset in Atlan, you mustprovide it anAtlanClientthrough which to connect to the tenant.

emit()

emit()

AtlanClient

12345

OpenLineageEventcomplete=OpenLineageEvent.creator(// (1)olr,OpenLineage.RunEvent.EventType.COMPLETE).build();complete.emit(client);// (2)

OpenLineageEventcomplete=OpenLineageEvent.creator(// (1)olr,OpenLineage.RunEvent.EventType.COMPLETE).build();complete.emit(client);// (2)

Since each run of a job must consist ofat leasttwo events, do not forget to send the terminal state indicating when the job has finished (and whether it was successful with aCOMPLETEor had some error with aFAIL.)

COMPLETE

FAIL

Once again, use theemit()method to actually send the event to Atlan to be processed (asynchronously). Because this operation will directly persist the asset in Atlan, you mustprovide it anAtlanClientthrough which to connect to the tenant.

emit()

AtlanClient

12345678910111213141516171819202122232425262728293031323334353637383940

frompyatlan.client.atlanimportAtlanClientfrompyatlan.model.enumsimportOpenLineageEventTypefrompyatlan.model.open_lineageimportOpenLineageEvent,OpenLineageJob,OpenLineageRunclient=AtlanClient()snowflake="snowflake://abc123.snowflakecomputing.com"# (1)job=OpenLineageJob.creator(# (2)connection_name="ol-spark",job_name="dag_123",producer="https://your.orchestrator/unique/id/123")run=OpenLineageRun.creator(job=job)# (3)input_dataset=job.create_input(namespace=snowflake,asset_name="OPS.DEFAULT.RUN_STATS")# (4)output_dataset=job.create_output(namespace=snowflake,asset_name="OPS.DEFAULT.FULL_STATS")# (5)start=OpenLineageEvent.creator(run=run,event_type=OpenLineageEventType.START)# (6)start.inputs=[input_dataset,job.create_input(namespace=snowflake,asset_name="SOME.OTHER.TBL"),job.create_input(namespace=snowflake,asset_name="AN.OTHER.TBL"),]# (7)start.outputs=[output_dataset,job.create_output(namespace=snowflake,asset_name="AN.OTHER.VIEW")]# (8)start.emit(client=client)# (9)

frompyatlan.client.atlanimportAtlanClientfrompyatlan.model.enumsimportOpenLineageEventTypefrompyatlan.model.open_lineageimportOpenLineageEvent,OpenLineageJob,OpenLineageRunclient=AtlanClient()snowflake="snowflake://abc123.snowflakecomputing.com"# (1)job=OpenLineageJob.creator(# (2)connection_name="ol-spark",job_name="dag_123",producer="https://your.orchestrator/unique/id/123")run=OpenLineageRun.creator(job=job)# (3)input_dataset=job.create_input(namespace=snowflake,asset_name="OPS.DEFAULT.RUN_STATS")# (4)output_dataset=job.create_output(namespace=snowflake,asset_name="OPS.DEFAULT.FULL_STATS")# (5)start=OpenLineageEvent.creator(run=run,event_type=OpenLineageEventType.START)# (6)start.inputs=[input_dataset,job.create_input(namespace=snowflake,asset_name="SOME.OTHER.TBL"),job.create_input(namespace=snowflake,asset_name="AN.OTHER.TBL"),]# (7)start.outputs=[output_dataset,job.create_output(namespace=snowflake,asset_name="AN.OTHER.VIEW")]# (8)start.emit(client=client)# (9)

Datasets used in data lineage need anamespacethat follows thesource-specific naming standards of OpenLineage.

namespace

Lineage is tracked through jobs. Each job must have:the name of a connection (that already exists in Atlan),a unique job name (used to idempotently update the same job with multiple runs), anda unique URI indicating the code or system responsible for producing this lineage.

the name of a connection (that already exists in Atlan),

a unique job name (used to idempotently update the same job with multiple runs), and

a unique URI indicating the code or system responsible for producing this lineage.

A job must be run at least once for any lineage to exist, and these separate runs of the same job are tracked throughOpenLineageRunobjects.

OpenLineageRun

You can define any number of inputs (sources) for lineage. Thenameof a dataset should use a.-qualified form. For example, a table should beDATABASE_NAME.SCHEMA_NAME.TABLE_NAME.

name

.

DATABASE_NAME.SCHEMA_NAME.TABLE_NAME

You can define any number of outputs (targets) for lineage. Thenameof a dataset should use a.-qualified form. For example, a table should beDATABASE_NAME.SCHEMA_NAME.TABLE_NAME.

name

.

DATABASE_NAME.SCHEMA_NAME.TABLE_NAME

Each run of a job must consist ofat leasttwo events â aSTARTevent indicating when the job ran began, and some terminal state indicating when the job run finished.

START

You can chain any number ofinputs to the event to indicate the source datasets for the lineage.

input

You can chain any number ofoutputs to the event to indicate the target datasets for the lineage.

output

Use theemit()method to actually send the event to Atlan to be processed. The processing itself occurs asynchronously, so a successfulemit()will only indicate that the event has been successfully sent to Atlan, not that it has (yet) been processed.

emit()

emit()

12345

complete=OpenLineageEvent.creator(run=run,event_type=OpenLineageEventType.COMPLETE)# (1)complete.emit(client=client)# (2)

complete=OpenLineageEvent.creator(run=run,event_type=OpenLineageEventType.COMPLETE)# (1)complete.emit(client=client)# (2)

Since each run of a job must consist ofat leasttwo events,
do not forget to send the terminal state indicating when the job
has finished (and whether it was successful with aCOMPLETEor had some error with aFAIL.)

COMPLETE

FAIL

Once again, use theemit()method to actually send the
event to Atlan to be processed (asynchronously).

emit()

12345678910111213141516171819202122

valsnowflake="snowflake://abc123.snowflakecomputing.com"// (1)valolj=OpenLineageJob.creator(// (2)"ol-spark","dag_123","https://your.orchestrator/unique/id/123").build()valolr=OpenLineageRun.creator(olj).build()// (3)valinputDataset=olj.createInput(snowflake,"OPS.DEFAULT.RUN_STATS").build()// (4)valoutputDataset=olj.createOutput(snowflake,"OPS.DEFAULT.FULL_STATS").build()// (5)valstart=OpenLineageEvent.creator(// (6)olr,OpenLineage.RunEvent.EventType.START).input(inputDataset)// (7).input(olj.createInput(snowflake,"SOME.OTHER.TBL").build()).input(olj.createInput(snowflake,"AN.OTHER.TBL").build()).output(outputDataset)// (8).output(olj.createOutput(snowflake,"AN.OTHER.VIEW").build()).build()start.emit(client)// (9)

valsnowflake="snowflake://abc123.snowflakecomputing.com"// (1)valolj=OpenLineageJob.creator(// (2)"ol-spark","dag_123","https://your.orchestrator/unique/id/123").build()valolr=OpenLineageRun.creator(olj).build()// (3)valinputDataset=olj.createInput(snowflake,"OPS.DEFAULT.RUN_STATS").build()// (4)valoutputDataset=olj.createOutput(snowflake,"OPS.DEFAULT.FULL_STATS").build()// (5)valstart=OpenLineageEvent.creator(// (6)olr,OpenLineage.RunEvent.EventType.START).input(inputDataset)// (7).input(olj.createInput(snowflake,"SOME.OTHER.TBL").build()).input(olj.createInput(snowflake,"AN.OTHER.TBL").build()).output(outputDataset)// (8).output(olj.createOutput(snowflake,"AN.OTHER.VIEW").build()).build()start.emit(client)// (9)

Datasets used in data lineage need anamespacethat follows thesource-specific naming standards of OpenLineage.

namespace

Lineage is tracked through jobs. Each job must have:the name of a connection (that already exists in Atlan),a unique job name (used to idempotently update the same job with multiple runs), anda unique URI indicating the code or system responsible for producing this lineage.

the name of a connection (that already exists in Atlan),

a unique job name (used to idempotently update the same job with multiple runs), and

a unique URI indicating the code or system responsible for producing this lineage.

A job must be run at least once for any lineage to exist, and these separate runs of the same job are tracked throughOpenLineageRunobjects.

OpenLineageRun

You can define any number of inputs (sources) for lineage. Thenameof a dataset should use a.-qualified form. For example, a table should beDATABASE_NAME.SCHEMA_NAME.TABLE_NAME.

name

.

DATABASE_NAME.SCHEMA_NAME.TABLE_NAME

You can define any number of outputs (targets) for lineage. Thenameof a dataset should use a.-qualified form. For example, a table should beDATABASE_NAME.SCHEMA_NAME.TABLE_NAME.

name

.

DATABASE_NAME.SCHEMA_NAME.TABLE_NAME

Each run of a job must consist ofat leasttwo events â aSTARTevent indicating when the job ran began, and some terminal state indicating when the job run finished.

START

You can chain any number ofinputs to the event to indicate the source datasets for the lineage.

input

You can chain any number ofoutputs to the event to indicate the target datasets for the lineage.

output

Use theemit()method to actually send the event to Atlan to be processed. The processing itself occurs asynchronously, so a successfulemit()will only indicate that the event has been successfully sent to Atlan, not that it has (yet) been processed. Because this operation will directly persist the asset in Atlan, you mustprovide it anAtlanClientthrough which to connect to the tenant.

emit()

emit()

AtlanClient

12345

valcomplete=OpenLineageEvent.creator(// (1)olr,OpenLineage.RunEvent.EventType.COMPLETE).build()complete.emit(client)// (2)

valcomplete=OpenLineageEvent.creator(// (1)olr,OpenLineage.RunEvent.EventType.COMPLETE).build()complete.emit(client)// (2)

Since each run of a job must consist ofat leasttwo events, do not forget to send the terminal state indicating when the job has finished (and whether it was successful with aCOMPLETEor had some error with aFAIL.)

COMPLETE

FAIL

Once again, use theemit()method to actually send the event to Atlan to be processed (asynchronously). Because this operation will directly persist the asset in Atlan, you mustprovide it anAtlanClientthrough which to connect to the tenant.

emit()

AtlanClient

1234567891011121314151617181920212223242526272829303132333435363738394041424344

{"eventTime":"2024-07-01T08:23:37.491542Z",// (1)"producer":"https://your.orchestrator/unique/id/123",// (2)"schemaURL":"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunEvent","eventType":"START",// (3)"job":{// (4)"namespace":"ol-spark","name":"dag_123","facets":{}},"run":{// (5)"runId":"eefd52c3-5871-4f0e-8ff5-237e9a6efb53","facets":{}},"inputs":[// (6){"namespace":"snowflake://abc123.snowflakecomputing.com","name":"OPS.DEFAULT.RUN_STATS","facets":{}},{"namespace":"snowflake://abc123.snowflakecomputing.com","name":"SOME.OTHER.TBL","facets":{}},{"namespace":"snowflake://abc123.snowflakecomputing.com","name":"AN.OTHER.TBL","facets":{}}],"outputs":[// (7){"namespace":"snowflake://abc123.snowflakecomputing.com","name":"OPS.DEFAULT.FULL_STATS","facets":{}},{"namespace":"snowflake://abc123.snowflakecomputing.com","name":"AN.OTHER.VIEW","facets":{}}]}

{"eventTime":"2024-07-01T08:23:37.491542Z",// (1)"producer":"https://your.orchestrator/unique/id/123",// (2)"schemaURL":"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunEvent","eventType":"START",// (3)"job":{// (4)"namespace":"ol-spark","name":"dag_123","facets":{}},"run":{// (5)"runId":"eefd52c3-5871-4f0e-8ff5-237e9a6efb53","facets":{}},"inputs":[// (6){"namespace":"snowflake://abc123.snowflakecomputing.com","name":"OPS.DEFAULT.RUN_STATS","facets":{}},{"namespace":"snowflake://abc123.snowflakecomputing.com","name":"SOME.OTHER.TBL","facets":{}},{"namespace":"snowflake://abc123.snowflakecomputing.com","name":"AN.OTHER.TBL","facets":{}}],"outputs":[// (7){"namespace":"snowflake://abc123.snowflakecomputing.com","name":"OPS.DEFAULT.FULL_STATS","facets":{}},{"namespace":"snowflake://abc123.snowflakecomputing.com","name":"AN.OTHER.VIEW","facets":{}}]}

Each event for a job run must have a time at which the event occurred.

Each event must have a URI indicating the code or system responsible for producing this lineage.

Each run of a job must consist ofat leasttwo events â aSTARTevent indicating when the job ran began, and some terminal state indicating when the job run finished.

START

Lineage is tracked through jobs. Each job must have:the name of a connection (that already exists in Atlan) as itsnamespace,a unique job name (used to idempotently update the same job with multiple runs)

the name of a connection (that already exists in Atlan) as itsnamespace,

namespace

a unique job name (used to idempotently update the same job with multiple runs)

A job must be run at least once for any lineage to exist, and each event for the same run of a job must be associated with the samerunId.

runId

You can define any number of inputs (sources) for lineage.Datasets used in data lineage need anamespacethat follows thesource-specific naming standards of OpenLineage.Thenameof a dataset should use a.-qualified form. For example, a table should beDATABASE_NAME.SCHEMA_NAME.TABLE_NAME.

Datasets used in data lineage need anamespacethat follows thesource-specific naming standards of OpenLineage.

namespace

Thenameof a dataset should use a.-qualified form. For example, a table should beDATABASE_NAME.SCHEMA_NAME.TABLE_NAME.

name

.

DATABASE_NAME.SCHEMA_NAME.TABLE_NAME

You can define any number of outputs (targets) for lineage.Datasets used in data lineage need anamespacethat follows thesource-specific naming standards of OpenLineage.Thenameof a dataset should use a.-qualified form. For example, a table should beDATABASE_NAME.SCHEMA_NAME.TABLE_NAME.

Datasets used in data lineage need anamespacethat follows thesource-specific naming standards of OpenLineage.

namespace

Thenameof a dataset should use a.-qualified form. For example, a table should beDATABASE_NAME.SCHEMA_NAME.TABLE_NAME.

name

.

DATABASE_NAME.SCHEMA_NAME.TABLE_NAME

123456789101112131415

{"eventTime":"2024-07-01T08:23:38.360567Z","producer":"https://your.orchestrator/unique/id/123","schemaURL":"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunEvent","eventType":"COMPLETE",// (1)"run":{"runId":"eefd52c3-5871-4f0e-8ff5-237e9a6efb53","facets":{}},"job":{"namespace":"ol-spark","name":"dag_123","facets":{}}}

{"eventTime":"2024-07-01T08:23:38.360567Z","producer":"https://your.orchestrator/unique/id/123","schemaURL":"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunEvent","eventType":"COMPLETE",// (1)"run":{"runId":"eefd52c3-5871-4f0e-8ff5-237e9a6efb53","facets":{}},"job":{"namespace":"ol-spark","name":"dag_123","facets":{}}}

Since each run of a job must consist ofat leasttwo events, do not forget to send the terminal state indicating when the job has finished (and whether it was successful with aCOMPLETEor had some error with aFAIL.)

COMPLETE

FAIL



## Create lineage between columnsÂ¶
(source: https://developer.atlan.com/snippets/common-examples/lineage/manage/)



### DirectlyÂ¶
(source: https://developer.atlan.com/snippets/common-examples/lineage/manage/)

2.0.04.0.0

To create lineage betweenrelational asset columns,
it is necessary to create aColumnProcessentity.

ColumnProcess

Lineage with relational columns

Before creating the ColumnProcess, verifylineage already existsbetween the associatedrelational assets, and ensure that the
columns referenced as inputs and outputs already exist.

123456789101112131415161718

ColumnProcesscolumnProcess=ColumnProcess.creator(// (1)"Source 1, Source 2, Source 3 -> Target 1, Target 2",// (2)"default/snowflake/1657025257",// (3)"dag_123",// (4)List.of(// (5)Column.refByGuid("495b1516-aaaf-4390-8cfd-b11ade7a7799"),Column.refByGuid("d002dead-1655-4d75-abd6-ad889fa04bd4"),Column.refByQualifiedName("default/snowflake/1657025257/OPS/DEFAULT/RUN_STATS/COLUMN")),List.of(// (6)Column.refByGuid("86d9a061-7753-4884-b988-a02d3954bc24"),Column.refByQualifiedName("default/snowflake/1657025257/OPS/DEFAULT/FULL_STATS/COLUMN")),Process.refByGuid("76d9a061-7753-9884-b988-a02d3954bc25"))// (7).sql("select * from somewhere;")// (8).sourceURL("https://your.orchestrator/unique/id/123")// (9).build();AssetMutationResponseresponse=columnProcess.save(client);// (10)assertresponse.getCreatedAssets().size()==1// (11)assertresponse.getUpdatedAssets().size()==5// (12)

ColumnProcesscolumnProcess=ColumnProcess.creator(// (1)"Source 1, Source 2, Source 3 -> Target 1, Target 2",// (2)"default/snowflake/1657025257",// (3)"dag_123",// (4)List.of(// (5)Column.refByGuid("495b1516-aaaf-4390-8cfd-b11ade7a7799"),Column.refByGuid("d002dead-1655-4d75-abd6-ad889fa04bd4"),Column.refByQualifiedName("default/snowflake/1657025257/OPS/DEFAULT/RUN_STATS/COLUMN")),List.of(// (6)Column.refByGuid("86d9a061-7753-4884-b988-a02d3954bc24"),Column.refByQualifiedName("default/snowflake/1657025257/OPS/DEFAULT/FULL_STATS/COLUMN")),Process.refByGuid("76d9a061-7753-9884-b988-a02d3954bc25"))// (7).sql("select * from somewhere;")// (8).sourceURL("https://your.orchestrator/unique/id/123")// (9).build();AssetMutationResponseresponse=columnProcess.save(client);// (10)assertresponse.getCreatedAssets().size()==1// (11)assertresponse.getUpdatedAssets().size()==5// (12)

Use thecreator()method to initialize the object with allnecessary attributes for creating it.

creator()

Provide a name for how the column process will be shown in the UI.

Provide thequalifiedNameof the connection that ran the column process.Tips for the connectionThe column process itself must be created within a connection for both access control and icon labelling. Use a connectionqualifiedNamethat indicates the system that ran the column process:You could use the same connectionqualifiedNameas the source system, if it was the source system "pushing" data to the target(s).You could use the same connectionqualifiedNameas the target system, if it was the target system "pulling" data from the source(s).You could use a different connectionqualifiedNamefrom either source or target, if there is a system in-between doing the processing (for example an ETL engine or orchestrator).

Provide thequalifiedNameof the connection that ran the column process.

qualifiedName

Tips for the connection

The column process itself must be created within a connection for both access control and icon labelling. Use a connectionqualifiedNamethat indicates the system that ran the column process:

qualifiedName

You could use the same connectionqualifiedNameas the source system, if it was the source system "pushing" data to the target(s).

qualifiedName

You could use the same connectionqualifiedNameas the target system, if it was the target system "pulling" data from the source(s).

qualifiedName

You could use a different connectionqualifiedNamefrom either source or target, if there is a system in-between doing the processing (for example an ETL engine or orchestrator).

qualifiedName

(Optional) Provide the unique ID of the column process within that connection. This could be the unique DAG ID for an orchestrator, for example. Since it is optional, you can also sendnulland the SDK will generate a unique ID for you based on the unique combination of inputs and outputs for the column process.Use your own ID if you canWhile the SDK can generate this ID for you, since it is based on the unique combination of inputs and outputs the ID can change if those inputs or outputs change. This could result in extra column processes in lineage as this process itself changes over time.By using your own ID for the column process, any changes that occur in that process over time (even if the inputs or outputs change) the same single process in Atlan will be updated.

(Optional) Provide the unique ID of the column process within that connection. This could be the unique DAG ID for an orchestrator, for example. Since it is optional, you can also sendnulland the SDK will generate a unique ID for you based on the unique combination of inputs and outputs for the column process.

null

Use your own ID if you can

While the SDK can generate this ID for you, since it is based on the unique combination of inputs and outputs the ID can change if those inputs or outputs change. This could result in extra column processes in lineage as this process itself changes over time.

By using your own ID for the column process, any changes that occur in that process over time (even if the inputs or outputs change) the same single process in Atlan will be updated.

Provide the list of inputs to the column process. Note that each of these is only aReferenceto an asset, not a full asset object. For a reference you only need (in addition to the type of asset) either:its GUID (for the static<Type>.refByGuid()method)itsqualifiedName(for the static<Type>.refByQualifiedName()method)

Provide the list of inputs to the column process. Note that each of these is only aReferenceto an asset, not a full asset object. For a reference you only need (in addition to the type of asset) either:

Reference

its GUID (for the static<Type>.refByGuid()method)

<Type>.refByGuid()

itsqualifiedName(for the static<Type>.refByQualifiedName()method)

qualifiedName

<Type>.refByQualifiedName()

Provide the list of outputs to the column process. Note that each of these is again only aReferenceto an asset.

Provide the list of outputs to the column process. Note that each of these is again only aReferenceto an asset.

Reference

Provide the parentLineageProcessin which this process ran since this process is a subprocess of some higher-level process.

LineageProcess

(Optional) You can also add other properties to the column process, such as SQL code that runs within the column process.

(Optional) You can also provide a link to the column process, which will provide a button to click to go to that link from the Atlan UI when viewing the column process in Atlan.

Call thesave()method to actually create the column process. Because this operation will directly persist the asset in Atlan, you mustprovide it anAtlanClientthrough which to connect to the tenant.

save()

AtlanClient

The response will include that single column process asset that was created.

The response will also include the 5 column assets (3 inputs, 2 outputs) that were updated.

1234567891011121314151617181920212223242526

frompyatlan.client.atlanimportAtlanClientfrompyatlan.model.assetsimportProcess,ColumnProcess,Columnclient=AtlanClient()column_process=ColumnProcess.creator(# (1)name="Source 1, Source 2, Source 3 -> Target 1, Target 2",# (2)connection_qualified_name="default/snowflake/1657025257",# (3)process_id="dag_123",# (4)inputs=[# (5)Column.ref_by_guid(guid="495b1516-aaaf-4390-8cfd-b11ade7a7799"),Column.ref_by_guid(guid="d002dead-1655-4d75-abd6-ad889fa04bd4"),Column.ref_by_qualified_name(qualified_name="default/snowflake/1657025257/OPS/DEFAULT/RUN_STATS/COLUMN"),],outputs=[# (6)Column.ref_by_guid(guid="86d9a061-7753-4884-b988-a02d3954bc24"),Column.ref_by_qualified_name(qualified_name="default/snowflake/1657025257/OPS/DEFAULT/FULL_STATS/COLUMN"),],parent=Process.ref_by_guid("76d9a061-7753-9884-b988-a02d3954bc25"),)# (7)column_process.sql="select * from somewhere;"# (8)column_process.source_url="https://your.orchestrator/unique/id/123"# (9)response=client.asset.save(column_process)# (10)assert(column_processes:=response.assets_created(ColumnProcess))# (11)assertlen(column_processes)==1# (12)assert(columns:=response.assets_updated(Column))# (13)assertlen(columns)==2# (14)

frompyatlan.client.atlanimportAtlanClientfrompyatlan.model.assetsimportProcess,ColumnProcess,Columnclient=AtlanClient()column_process=ColumnProcess.creator(# (1)name="Source 1, Source 2, Source 3 -> Target 1, Target 2",# (2)connection_qualified_name="default/snowflake/1657025257",# (3)process_id="dag_123",# (4)inputs=[# (5)Column.ref_by_guid(guid="495b1516-aaaf-4390-8cfd-b11ade7a7799"),Column.ref_by_guid(guid="d002dead-1655-4d75-abd6-ad889fa04bd4"),Column.ref_by_qualified_name(qualified_name="default/snowflake/1657025257/OPS/DEFAULT/RUN_STATS/COLUMN"),],outputs=[# (6)Column.ref_by_guid(guid="86d9a061-7753-4884-b988-a02d3954bc24"),Column.ref_by_qualified_name(qualified_name="default/snowflake/1657025257/OPS/DEFAULT/FULL_STATS/COLUMN"),],parent=Process.ref_by_guid("76d9a061-7753-9884-b988-a02d3954bc25"),)# (7)column_process.sql="select * from somewhere;"# (8)column_process.source_url="https://your.orchestrator/unique/id/123"# (9)response=client.asset.save(column_process)# (10)assert(column_processes:=response.assets_created(ColumnProcess))# (11)assertlen(column_processes)==1# (12)assert(columns:=response.assets_updated(Column))# (13)assertlen(columns)==2# (14)

Use thecreate()method to initialize the object with allnecessary attributes for creating it.

create()

Provide a name for how the column process will be shown in the UI.

Provide thequalified_nameof the connection that ran the column process.Tips for the connectionThe column process itself must be created within a connection for both access control and icon labelling. Use a connectionqualified_namethat indicates the system that ran the column process:You could use the same connectionqualified_nameas the source system, if it was the source system "pushing" data to the target(s).You could use the same connectionqualified_nameas the target system, if it was the target system "pulling" data from the source(s).You could use a different connectionqualified_namefrom either source or target, if there is a system in-between doing the processing (for example an ETL engine or orchestrator).

Provide thequalified_nameof the connection that ran the column process.

qualified_name

Tips for the connection

The column process itself must be created within a connection for both access control and icon labelling. Use a connectionqualified_namethat indicates the system that ran the column process:

qualified_name

You could use the same connectionqualified_nameas the source system, if it was the source system "pushing" data to the target(s).

qualified_name

You could use the same connectionqualified_nameas the target system, if it was the target system "pulling" data from the source(s).

qualified_name

You could use a different connectionqualified_namefrom either source or target, if there is a system in-between doing the processing (for example an ETL engine or orchestrator).

qualified_name

(Optional) Provide the unique ID of the column process within that connection. This could be the unique DAG ID for an orchestrator, for example. Since it is optional, you can also leave it out and the SDK will generate a unique ID for you based on the unique combination of inputs and outputs for the column process.Use your own ID if you canWhile the SDK can generate this ID for you, since it is based on the unique combination of inputs and outputs the ID can change if those inputs or outputs change. This could result in extra column processes in lineage as this column process itself changes over time.By using your own ID for the column process, any changes that occur in that column process over time (even if the inputs or outputs change) the same single column process in Atlan will be updated.

(Optional) Provide the unique ID of the column process within that connection. This could be the unique DAG ID for an orchestrator, for example. Since it is optional, you can also leave it out and the SDK will generate a unique ID for you based on the unique combination of inputs and outputs for the column process.

Use your own ID if you can

While the SDK can generate this ID for you, since it is based on the unique combination of inputs and outputs the ID can change if those inputs or outputs change. This could result in extra column processes in lineage as this column process itself changes over time.

By using your own ID for the column process, any changes that occur in that column process over time (even if the inputs or outputs change) the same single column process in Atlan will be updated.

Provide the list of inputs to the column process. Note that each of these is only aReferenceto an asset, not a full asset object. For a reference you only need (in addition to the type of asset) either:its GUID (for theref_by_guid()method)itsqualifiedName(for theref_by_qualified_name()method)

Provide the list of inputs to the column process. Note that each of these is only aReferenceto an asset, not a full asset object. For a reference you only need (in addition to the type of asset) either:

Reference

its GUID (for theref_by_guid()method)

ref_by_guid()

itsqualifiedName(for theref_by_qualified_name()method)

qualifiedName

ref_by_qualified_name()

Provide the list of outputs to the column process. Note that each of these is again only aReferenceto an asset.

Provide the list of outputs to the column process. Note that each of these is again only aReferenceto an asset.

Reference

Provide the parentProcessin which this process ran since this process is a subprocess of some  higher-level process.

Process

(Optional) You can also add other properties to the column process, such as SQL code that runs within the column process.

(Optional) You can also provide a link to the column process, which will provide a button to click to go to that link from the Atlan UI when viewing the column process in Atlan.

Call thesave()method to actually create the column process.

save()

Check that aColumnProcesswas created.

ColumnProcess

Check that only 1ColumnProcesswas created.

ColumnProcess

Check that tables were updated.

Check that 5 tables (3 inputs, 2 outputs) were updated.

123456789101112131415161718

valcolumnProcess=ColumnProcess.creator(// (1)"Source 1, Source 2, Source 3 -> Target 1, Target 2",// (2)"default/snowflake/1657025257",// (3)"dag_123",// (4)listOf<ICatalog>(// (5)Column.refByGuid("495b1516-aaaf-4390-8cfd-b11ade7a7799"),Column.refByGuid("d002dead-1655-4d75-abd6-ad889fa04bd4"),Column.refByQualifiedName("default/snowflake/1657025257/OPS/DEFAULT/RUN_STATS/COLUMN")),listOf<ICatalog>(// (6)Column.refByGuid("86d9a061-7753-4884-b988-a02d3954bc24"),Column.refByQualifiedName("default/snowflake/1657025257/OPS/DEFAULT/FULL_STATS/COLUMN")),Process.refByGuid("76d9a061-7753-9884-b988-a02d3954bc25"))// (7).sql("select * from somewhere;")// (8).sourceURL("https://your.orchestrator/unique/id/123")// (9).build()valresponse=columnProcess.save(client)// (10)assert(response.createdAssets.size==1)// (11)assert(response.updatedAssets.size==5)// (12)

valcolumnProcess=ColumnProcess.creator(// (1)"Source 1, Source 2, Source 3 -> Target 1, Target 2",// (2)"default/snowflake/1657025257",// (3)"dag_123",// (4)listOf<ICatalog>(// (5)Column.refByGuid("495b1516-aaaf-4390-8cfd-b11ade7a7799"),Column.refByGuid("d002dead-1655-4d75-abd6-ad889fa04bd4"),Column.refByQualifiedName("default/snowflake/1657025257/OPS/DEFAULT/RUN_STATS/COLUMN")),listOf<ICatalog>(// (6)Column.refByGuid("86d9a061-7753-4884-b988-a02d3954bc24"),Column.refByQualifiedName("default/snowflake/1657025257/OPS/DEFAULT/FULL_STATS/COLUMN")),Process.refByGuid("76d9a061-7753-9884-b988-a02d3954bc25"))// (7).sql("select * from somewhere;")// (8).sourceURL("https://your.orchestrator/unique/id/123")// (9).build()valresponse=columnProcess.save(client)// (10)assert(response.createdAssets.size==1)// (11)assert(response.updatedAssets.size==5)// (12)

Use thecreator()method to initialize the object with allnecessary attributes for creating it.

creator()

Provide a name for how the column process will be shown in the UI.

Provide thequalifiedNameof the connection that ran the column process.Tips for the connectionThe column process itself must be created within a connection for both access control and icon labelling. Use a connectionqualifiedNamethat indicates the system that ran the column process:You could use the same connectionqualifiedNameas the source system, if it was the source system "pushing" data to the target(s).You could use the same connectionqualifiedNameas the target system, if it was the target system "pulling" data from the source(s).You could use a different connectionqualifiedNamefrom either source or target, if there is a system in-between doing the processing (for example an ETL engine or orchestrator).

Provide thequalifiedNameof the connection that ran the column process.

qualifiedName

Tips for the connection

The column process itself must be created within a connection for both access control and icon labelling. Use a connectionqualifiedNamethat indicates the system that ran the column process:

qualifiedName

You could use the same connectionqualifiedNameas the source system, if it was the source system "pushing" data to the target(s).

qualifiedName

You could use the same connectionqualifiedNameas the target system, if it was the target system "pulling" data from the source(s).

qualifiedName

You could use a different connectionqualifiedNamefrom either source or target, if there is a system in-between doing the processing (for example an ETL engine or orchestrator).

qualifiedName

(Optional) Provide the unique ID of the column process within that connection. This could be the unique DAG ID for an orchestrator, for example. Since it is optional, you can also sendnulland the SDK will generate a unique ID for you based on the unique combination of inputs and outputs for the column process.Use your own ID if you canWhile the SDK can generate this ID for you, since it is based on the unique combination of inputs and outputs the ID can change if those inputs or outputs change. This could result in extra column processes in lineage as this process itself changes over time.By using your own ID for the column process, any changes that occur in that process over time (even if the inputs or outputs change) the same single process in Atlan will be updated.

(Optional) Provide the unique ID of the column process within that connection. This could be the unique DAG ID for an orchestrator, for example. Since it is optional, you can also sendnulland the SDK will generate a unique ID for you based on the unique combination of inputs and outputs for the column process.

null

Use your own ID if you can

While the SDK can generate this ID for you, since it is based on the unique combination of inputs and outputs the ID can change if those inputs or outputs change. This could result in extra column processes in lineage as this process itself changes over time.

By using your own ID for the column process, any changes that occur in that process over time (even if the inputs or outputs change) the same single process in Atlan will be updated.

Provide the list of inputs to the column process. Note that each of these is only aReferenceto an asset, not a full asset object. For a reference you only need (in addition to the type of asset) either:its GUID (for the static<Type>.refByGuid()method)itsqualifiedName(for the static<Type>.refByQualifiedName()method)

Provide the list of inputs to the column process. Note that each of these is only aReferenceto an asset, not a full asset object. For a reference you only need (in addition to the type of asset) either:

Reference

its GUID (for the static<Type>.refByGuid()method)

<Type>.refByGuid()

itsqualifiedName(for the static<Type>.refByQualifiedName()method)

qualifiedName

<Type>.refByQualifiedName()

Provide the list of outputs to the column process. Note that each of these is again only aReferenceto an asset.

Provide the list of outputs to the column process. Note that each of these is again only aReferenceto an asset.

Reference

Provide the parentLineageProcessin which this process ran since this process is a subprocess of some higher-level process.

LineageProcess

(Optional) You can also add other properties to the column process, such as SQL code that runs within the column process.

(Optional) You can also provide a link to the column process, which will provide a button to click to go to that link from the Atlan UI when viewing the column process in Atlan.

Call thesave()method to actually create the column process. Because this operation will directly persist the asset in Atlan, you mustprovide it anAtlanClientthrough which to connect to the tenant.

save()

AtlanClient

The response will include that single column process asset that was created.

The response will also include the 5 column assets (3 inputs, 2 outputs) that were updated.

12345678910111213141516171819202122232425262728293031323334353637383940414243444546

{"entities":[// (1){"typeName":"ColumnProcess",// (2)"attributes":{"name":"Source 1, Source 2, Source 3 -> Target 1, Target 2",// (3)"qualifiedName":"default/snowflake/1657025257/dag_123",// (4)"inputs":[// (5){"typeName":"Column","guid":"495b1516-aaaf-4390-8cfd-b11ade7a7799"},{"typeName":"Column","guid":"d002dead-1655-4d75-abd6-ad889fa04bd4"},{"typeName":"Column","uniqueAttributes":{"qualifiedName":"default/snowflake/1657025257/OPS/DEFAULT/RUN_STATS"}}],"outputs":[// (6){"typeName":"Column","guid":"86d9a061-7753-4884-b988-a02d3954bc24"},{"typeName":"Column","uniqueAttributes":{"qualifiedName":"default/snowflake/1657025257/OPS/DEFAULT/FULL_STATS"}}],"process":{// (7)"guid":"76d9a061-7753-9884-b988-a02d3954bc25","typeName":"Process","uniqueAttributes":{"qualifiedName":"default/snowflake/1657025257/parent_123"}}}}]}

{"entities":[// (1){"typeName":"ColumnProcess",// (2)"attributes":{"name":"Source 1, Source 2, Source 3 -> Target 1, Target 2",// (3)"qualifiedName":"default/snowflake/1657025257/dag_123",// (4)"inputs":[// (5){"typeName":"Column","guid":"495b1516-aaaf-4390-8cfd-b11ade7a7799"},{"typeName":"Column","guid":"d002dead-1655-4d75-abd6-ad889fa04bd4"},{"typeName":"Column","uniqueAttributes":{"qualifiedName":"default/snowflake/1657025257/OPS/DEFAULT/RUN_STATS"}}],"outputs":[// (6){"typeName":"Column","guid":"86d9a061-7753-4884-b988-a02d3954bc24"},{"typeName":"Column","uniqueAttributes":{"qualifiedName":"default/snowflake/1657025257/OPS/DEFAULT/FULL_STATS"}}],"process":{// (7)"guid":"76d9a061-7753-9884-b988-a02d3954bc25","typeName":"Process","uniqueAttributes":{"qualifiedName":"default/snowflake/1657025257/parent_123"}}}}]}

All assets must be wrapped in anentitiesarray.

entities

You must provide the exact type name for aColumnProcessasset (case-sensitive).

ColumnProcess

You must provide a name of the integration column process.

You must provide a uniquequalifiedNamefor the integration column process (case-sensitive).

qualifiedName

You must list all of the input assets to the column process. These can be referenced by GUID or byqualifiedName.

qualifiedName

You must list all of the output assets from the column process. These can also be referenced by either GUID orqualifiedName.

qualifiedName

You must provide the parentLineageProcessin which this process ran since this process is a subprocess of some higher-level process.

LineageProcess



### Using OpenLineageÂ¶
(source: https://developer.atlan.com/snippets/common-examples/lineage/manage/)

2.5.14.0.0

To create column-lineage between assets throughOpenLineage, you need only extend the details of theoutputsyou send in your OpenLineage events.

outputs

You must first configure OpenLineage

You must first configure aSpark Assetsconnection in Atlan before sending any OpenLineage events. (You can skip theConfigure the integration in Apache Sparksection.)

123456789101112131415161718192021222324252627282930313233343536

Stringsnowflake="snowflake://abc123.snowflakecomputing.com";// (1)OpenLineageJobolj=OpenLineageJob.creator(// (2)"ol-spark","dag_123","https://your.orchestrator/unique/id/123").build();OpenLineageRunolr=OpenLineageRun.creator(olj).build();// (3)OpenLineageInputDatasetinputDataset=olj.createInput(snowflake,"OPS.DEFAULT.RUN_STATS").build();// (4)OpenLineageOutputDatasetoutputDataset=olj.createOutput(snowflake,"OPS.DEFAULT.FULL_STATS")// (5).toField(// (6)"COLUMN",// (7)listOf(// (8)inputDataset.fromField("COLUMN").build(),inputDataset.fromField("ONE").build(),inputDataset.fromField("TWO").build(),),).toField("ANOTHER",listOf(inputDataset.fromField("THREE").build(),),).build();OpenLineageEventstart=OpenLineageEvent.creator(// (9)olr,OpenLineage.RunEvent.EventType.START).input(inputDataset)// (10).input(olj.createInput(snowflake,"SOME.OTHER.TBL").build()).input(olj.createInput(snowflake,"AN.OTHER.TBL").build()).output(outputDataset)// (11).output(olj.createOutput(snowflake,"AN.OTHER.VIEW").build()).build();start.emit(client);// (12)

Stringsnowflake="snowflake://abc123.snowflakecomputing.com";// (1)OpenLineageJobolj=OpenLineageJob.creator(// (2)"ol-spark","dag_123","https://your.orchestrator/unique/id/123").build();OpenLineageRunolr=OpenLineageRun.creator(olj).build();// (3)OpenLineageInputDatasetinputDataset=olj.createInput(snowflake,"OPS.DEFAULT.RUN_STATS").build();// (4)OpenLineageOutputDatasetoutputDataset=olj.createOutput(snowflake,"OPS.DEFAULT.FULL_STATS")// (5).toField(// (6)"COLUMN",// (7)listOf(// (8)inputDataset.fromField("COLUMN").build(),inputDataset.fromField("ONE").build(),inputDataset.fromField("TWO").build(),),).toField("ANOTHER",listOf(inputDataset.fromField("THREE").build(),),).build();OpenLineageEventstart=OpenLineageEvent.creator(// (9)olr,OpenLineage.RunEvent.EventType.START).input(inputDataset)// (10).input(olj.createInput(snowflake,"SOME.OTHER.TBL").build()).input(olj.createInput(snowflake,"AN.OTHER.TBL").build()).output(outputDataset)// (11).output(olj.createOutput(snowflake,"AN.OTHER.VIEW").build()).build();start.emit(client);// (12)

Datasets used in data lineage need anamespacethat follows thesource-specific naming standards of OpenLineage.

namespace

Lineage is tracked through jobs. Each job must have:the name of a connection (that already exists in Atlan),a unique job name (used to idempotently update the same job with multiple runs), anda unique URI indicating the code or system responsible for producing this lineage.

the name of a connection (that already exists in Atlan),

a unique job name (used to idempotently update the same job with multiple runs), and

a unique URI indicating the code or system responsible for producing this lineage.

A job must be run at least once for any lineage to exist, and these separate runs of the same job are tracked throughOpenLineageRunobjects.

OpenLineageRun

You can define any number of inputs (sources) for lineage. Thenameof a dataset should use a.-qualified form. For example, a table should beDATABASE_NAME.SCHEMA_NAME.TABLE_NAME.

name

.

DATABASE_NAME.SCHEMA_NAME.TABLE_NAME

You can define any number of outputs (targets) for lineage. Thenameof a dataset should use a.-qualified form. For example, a table should beDATABASE_NAME.SCHEMA_NAME.TABLE_NAME.

name

.

DATABASE_NAME.SCHEMA_NAME.TABLE_NAME

For column-level lineage, you specify the mappingonlyon the target (outputs) end of the lineage, by chaining atoFieldfor each output column.

toField

Each key for such atoField()chain is the name of a field (column) in theoutputdataset.

toField()

You can then provide a list that definesallinput (source) fields that map to this output field in column-level lineage.Create input fields from input datasetsYou can quickly create such a input (source) field from an input dataset using thefromField()method and the name of the column in that input dataset.

You can then provide a list that definesallinput (source) fields that map to this output field in column-level lineage.

Create input fields from input datasets

You can quickly create such a input (source) field from an input dataset using thefromField()method and the name of the column in that input dataset.

fromField()

Each run of a job must consist ofat leasttwo events â aSTARTevent indicating when the job ran began, and some terminal state indicating when the job run finished.

Each run of a job must consist ofat leasttwo events â aSTARTevent indicating when the job ran began, and some terminal state indicating when the job run finished.

START

You can chain any number ofinputs to the event to indicate the source datasets for the lineage.

input

You can chain any number ofoutputs to the event to indicate the target datasets for the lineage.

output

Use theemit()method to actually send the event to Atlan to be processed. The processing itself occurs asynchronously, so a successfulemit()will only indicate that the event has been successfully sent to Atlan, not that it has (yet) been processed. Because this operation will directly persist the asset in Atlan, you mustprovide it anAtlanClientthrough which to connect to the tenant.

emit()

emit()

AtlanClient

12345

OpenLineageEventcomplete=OpenLineageEvent.creator(// (1)olr,OpenLineage.RunEvent.EventType.COMPLETE).build();complete.emit(client);// (2)

OpenLineageEventcomplete=OpenLineageEvent.creator(// (1)olr,OpenLineage.RunEvent.EventType.COMPLETE).build();complete.emit(client);// (2)

Since each run of a job must consist ofat leasttwo events, do not forget to send the terminal state indicating when the job has finished (and whether it was successful with aCOMPLETEor had some error with aFAIL.)

COMPLETE

FAIL

Once again, use theemit()method to actually send the event to Atlan to be processed (asynchronously). Because this operation will directly persist the asset in Atlan, you mustprovide it anAtlanClientthrough which to connect to the tenant.

emit()

AtlanClient

1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556

frompyatlan.client.atlanimportAtlanClientfrompyatlan.model.enumsimportOpenLineageEventTypefrompyatlan.model.open_lineageimportOpenLineageEvent,OpenLineageJob,OpenLineageRunclient=AtlanClient()snowflake="snowflake://abc123.snowflakecomputing.com"# (1)job=OpenLineageJob.creator(# (2)connection_name="ol-spark",job_name="dag_123",producer="https://your.orchestrator/unique/id/123")run=OpenLineageRun.creator(job=job)# (3)input_dataset=job.create_input(namespace=snowflake,asset_name="OPS.DEFAULT.RUN_STATS")# (4)output_dataset=job.create_output(namespace=snowflake,asset_name="OPS.DEFAULT.FULL_STATS")# (5)output_dataset.to_fields=[# (6){# (7)"COLUMN":[input_dataset.from_field(field_name="COLUMN"),input_dataset.from_field(field_name="ONE"),input_dataset.from_field(field_name="TWO"),]# (8)},{"ANOTHER":[input_dataset.from_field(field_name="THREE"),]},]start=OpenLineageEvent.creator(run=run,event_type=OpenLineageEventType.START)# (9)start.inputs=[input_dataset,job.create_input(namespace=snowflake,asset_name="SOME.OTHER.TBL"),job.create_input(namespace=snowflake,asset_name="AN.OTHER.TBL"),]# (10)start.outputs=[output_dataset,job.create_output(namespace=snowflake,asset_name="AN.OTHER.VIEW")]# (11)start.emit()# (12)

frompyatlan.client.atlanimportAtlanClientfrompyatlan.model.enumsimportOpenLineageEventTypefrompyatlan.model.open_lineageimportOpenLineageEvent,OpenLineageJob,OpenLineageRunclient=AtlanClient()snowflake="snowflake://abc123.snowflakecomputing.com"# (1)job=OpenLineageJob.creator(# (2)connection_name="ol-spark",job_name="dag_123",producer="https://your.orchestrator/unique/id/123")run=OpenLineageRun.creator(job=job)# (3)input_dataset=job.create_input(namespace=snowflake,asset_name="OPS.DEFAULT.RUN_STATS")# (4)output_dataset=job.create_output(namespace=snowflake,asset_name="OPS.DEFAULT.FULL_STATS")# (5)output_dataset.to_fields=[# (6){# (7)"COLUMN":[input_dataset.from_field(field_name="COLUMN"),input_dataset.from_field(field_name="ONE"),input_dataset.from_field(field_name="TWO"),]# (8)},{"ANOTHER":[input_dataset.from_field(field_name="THREE"),]},]start=OpenLineageEvent.creator(run=run,event_type=OpenLineageEventType.START)# (9)start.inputs=[input_dataset,job.create_input(namespace=snowflake,asset_name="SOME.OTHER.TBL"),job.create_input(namespace=snowflake,asset_name="AN.OTHER.TBL"),]# (10)start.outputs=[output_dataset,job.create_output(namespace=snowflake,asset_name="AN.OTHER.VIEW")]# (11)start.emit()# (12)

Datasets used in data lineage need anamespacethat follows thesource-specific naming standards of OpenLineage.

namespace

Lineage is tracked through jobs. Each job must have:the name of a connection (that already exists in Atlan),a unique job name (used to idempotently update the same job with multiple runs), anda unique URI indicating the code or system responsible for producing this lineage.

the name of a connection (that already exists in Atlan),

a unique job name (used to idempotently update the same job with multiple runs), and

a unique URI indicating the code or system responsible for producing this lineage.

A job must be run at least once for any lineage to exist, and these separate runs of the same job are tracked throughOpenLineageRunobjects.

OpenLineageRun

You can define any number of inputs (sources) for lineage. Thenameof a dataset should use a.-qualified form. For example, a table should beDATABASE_NAME.SCHEMA_NAME.TABLE_NAME.

name

.

DATABASE_NAME.SCHEMA_NAME.TABLE_NAME

You can define any number of outputs (targets) for lineage. Thenameof a dataset should use a.-qualified form. For example, a table should beDATABASE_NAME.SCHEMA_NAME.TABLE_NAME.

name

.

DATABASE_NAME.SCHEMA_NAME.TABLE_NAME

For column-level lineage, you specify the mappingonlyon the target (outputs) end of the lineage to theto_fieldsattribute.

to_fields

Each key is the name of a field (column) in theoutputdataset.

You can then provide a list that definesallinput (source)
fields that map to this output field in column-level lineage.Create input fields from input datasetsYou can quickly create such a input (source) field from an input dataset using thefrom_Field()method and the name of the column in that input dataset.

You can then provide a list that definesallinput (source)
fields that map to this output field in column-level lineage.

Create input fields from input datasets

You can quickly create such a input (source) field from an input dataset using thefrom_Field()method and the name of the column in that input dataset.

from_Field()

Each run of a job must consist ofat leasttwo events â aSTARTevent indicating when the job ran began, and some terminal state indicating when the job run finished.

Each run of a job must consist ofat leasttwo events â aSTARTevent indicating when the job ran began, and some terminal state indicating when the job run finished.

START

You can chain any number ofinputs to the event to indicate the source datasets for the lineage.

input

You can chain any number ofoutputs to the event to indicate the target datasets for the lineage.

output

Use theemit()method to actually send the event to Atlan to be processed. The processing itself occurs asynchronously, so a successfulemit()will only indicate that the event has been successfully sent to Atlan, not that it has (yet) been processed.

emit()

emit()

12345

complete=OpenLineageEvent.creator(run=run,event_type=OpenLineageEventType.COMPLETE)# (1)complete.emit()# (2)

complete=OpenLineageEvent.creator(run=run,event_type=OpenLineageEventType.COMPLETE)# (1)complete.emit()# (2)

Since each run of a job must consist ofat leasttwo events,
do not forget to send the terminal state indicating when the job
has finished (and whether it was successful with aCOMPLETEor had some error with aFAIL.)

COMPLETE

FAIL

Once again, use theemit()method to actually send the
event to Atlan to be processed (asynchronously).

emit()

123456789101112131415161718192021222324252627282930313233343536

valsnowflake="snowflake://abc123.snowflakecomputing.com"// (1)valolj=OpenLineageJob.creator(// (2)"ol-spark","dag_123","https://your.orchestrator/unique/id/123").build()valolr=OpenLineageRun.creator(olj).build()// (3)valinputDataset=olj.createInput(snowflake,"OPS.DEFAULT.RUN_STATS").build()// (4)valoutputDataset=olj.createOutput(snowflake,"OPS.DEFAULT.FULL_STATS")// (5).toField(// (6)"COLUMN",// (7)listOf(// (8)inputDataset.fromField("COLUMN").build(),inputDataset.fromField("ONE").build(),inputDataset.fromField("TWO").build(),),).toField("ANOTHER",listOf(inputDataset.fromField("THREE").build(),),).build()valstart=OpenLineageEvent.creator(// (9)olr,OpenLineage.RunEvent.EventType.START).input(inputDataset)// (10).input(olj.createInput(snowflake,"SOME.OTHER.TBL").build()).input(olj.createInput(snowflake,"AN.OTHER.TBL").build()).output(outputDataset)// (11).output(olj.createOutput(snowflake,"AN.OTHER.VIEW").build()).build()start.emit(client)// (12)

valsnowflake="snowflake://abc123.snowflakecomputing.com"// (1)valolj=OpenLineageJob.creator(// (2)"ol-spark","dag_123","https://your.orchestrator/unique/id/123").build()valolr=OpenLineageRun.creator(olj).build()// (3)valinputDataset=olj.createInput(snowflake,"OPS.DEFAULT.RUN_STATS").build()// (4)valoutputDataset=olj.createOutput(snowflake,"OPS.DEFAULT.FULL_STATS")// (5).toField(// (6)"COLUMN",// (7)listOf(// (8)inputDataset.fromField("COLUMN").build(),inputDataset.fromField("ONE").build(),inputDataset.fromField("TWO").build(),),).toField("ANOTHER",listOf(inputDataset.fromField("THREE").build(),),).build()valstart=OpenLineageEvent.creator(// (9)olr,OpenLineage.RunEvent.EventType.START).input(inputDataset)// (10).input(olj.createInput(snowflake,"SOME.OTHER.TBL").build()).input(olj.createInput(snowflake,"AN.OTHER.TBL").build()).output(outputDataset)// (11).output(olj.createOutput(snowflake,"AN.OTHER.VIEW").build()).build()start.emit(client)// (12)

Datasets used in data lineage need anamespacethat follows thesource-specific naming standards of OpenLineage.

namespace

Lineage is tracked through jobs. Each job must have:the name of a connection (that already exists in Atlan),a unique job name (used to idempotently update the same job with multiple runs), anda unique URI indicating the code or system responsible for producing this lineage.

the name of a connection (that already exists in Atlan),

a unique job name (used to idempotently update the same job with multiple runs), and

a unique URI indicating the code or system responsible for producing this lineage.

A job must be run at least once for any lineage to exist, and these separate runs of the same job are tracked throughOpenLineageRunobjects.

OpenLineageRun

You can define any number of inputs (sources) for lineage. Thenameof a dataset should use a.-qualified form. For example, a table should beDATABASE_NAME.SCHEMA_NAME.TABLE_NAME.

name

.

DATABASE_NAME.SCHEMA_NAME.TABLE_NAME

You can define any number of outputs (targets) for lineage. Thenameof a dataset should use a.-qualified form. For example, a table should beDATABASE_NAME.SCHEMA_NAME.TABLE_NAME.

name

.

DATABASE_NAME.SCHEMA_NAME.TABLE_NAME

For column-level lineage, you specify the mappingonlyon the target (outputs) end of the lineage, by chaining atoFieldfor each output column.

toField

Each key for such atoField()chain is the name of a field (column) in theoutputdataset.

toField()

You can then provide a list that definesallinput (source) fields that map to this output field in column-level lineage.Create input fields from input datasetsYou can quickly create such a input (source) field from an input dataset using thefromField()method and the name of the column in that input dataset.

You can then provide a list that definesallinput (source) fields that map to this output field in column-level lineage.

Create input fields from input datasets

You can quickly create such a input (source) field from an input dataset using thefromField()method and the name of the column in that input dataset.

fromField()

Each run of a job must consist ofat leasttwo events â aSTARTevent indicating when the job ran began, and some terminal state indicating when the job run finished.

Each run of a job must consist ofat leasttwo events â aSTARTevent indicating when the job ran began, and some terminal state indicating when the job run finished.

START

You can chain any number ofinputs to the event to indicate the source datasets for the lineage.

input

You can chain any number ofoutputs to the event to indicate the target datasets for the lineage.

output

Use theemit()method to actually send the event to Atlan to be processed. The processing itself occurs asynchronously, so a successfulemit()will only indicate that the event has been successfully sent to Atlan, not that it has (yet) been processed. Because this operation will directly persist the asset in Atlan, you mustprovide it anAtlanClientthrough which to connect to the tenant.

emit()

emit()

AtlanClient

12345

valcomplete=OpenLineageEvent.creator(// (1)olr,OpenLineage.RunEvent.EventType.COMPLETE).build()complete.emit(client)// (2)

valcomplete=OpenLineageEvent.creator(// (1)olr,OpenLineage.RunEvent.EventType.COMPLETE).build()complete.emit(client)// (2)

Since each run of a job must consist ofat leasttwo events, do not forget to send the terminal state indicating when the job has finished (and whether it was successful with aCOMPLETEor had some error with aFAIL.)

COMPLETE

FAIL

Once again, use theemit()method to actually send the event to Atlan to be processed (asynchronously). Because this operation will directly persist the asset in Atlan, you mustprovide it anAtlanClientthrough which to connect to the tenant.

emit()

AtlanClient

12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879

{"eventTime":"2024-07-01T08:23:37.491542Z",// (1)"producer":"https://your.orchestrator/unique/id/123",// (2)"schemaURL":"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunEvent","eventType":"START",// (3)"job":{// (4)"namespace":"ol-spark","name":"dag_123","facets":{}},"run":{// (5)"runId":"eefd52c3-5871-4f0e-8ff5-237e9a6efb53","facets":{}},"inputs":[// (6){"namespace":"snowflake://abc123.snowflakecomputing.com","name":"OPS.DEFAULT.RUN_STATS","facets":{}},{"namespace":"snowflake://abc123.snowflakecomputing.com","name":"SOME.OTHER.TBL","facets":{}},{"namespace":"snowflake://abc123.snowflakecomputing.com","name":"AN.OTHER.TBL","facets":{}}],"outputs":[// (7){"namespace":"snowflake://abc123.snowflakecomputing.com","name":"OPS.DEFAULT.FULL_STATS","facets":{"columnLineage":{// (8)"_producer":"https://your.orchestrator/unique/id/123","_schemaURL":"https://openlineage.io/spec/facets/1-1-0/ColumnLineageDatasetFacet.json#/$defs/ColumnLineageDatasetFacet","fields":{"COLUMN":{// (9)"inputFields":[// (10){"namespace":"snowflake://abc123.snowflakecomputing.com","name":"OPS.DEFAULT.RUN_STATS","field":"COLUMN"},{"namespace":"snowflake://abc123.snowflakecomputing.com","name":"OPS.DEFAULT.RUN_STATS","field":"ONE"},{"namespace":"snowflake://abc123.snowflakecomputing.com","name":"OPS.DEFAULT.RUN_STATS","field":"TWO"}]},"ANOTHER":{"inputFields":[{"namespace":"snowflake://abc123.snowflakecomputing.com","name":"OPS.DEFAULT.RUN_STATS","field":"THREE"}]}}}}},{"namespace":"snowflake://abc123.snowflakecomputing.com","name":"AN.OTHER.VIEW","facets":{}}]}

{"eventTime":"2024-07-01T08:23:37.491542Z",// (1)"producer":"https://your.orchestrator/unique/id/123",// (2)"schemaURL":"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunEvent","eventType":"START",// (3)"job":{// (4)"namespace":"ol-spark","name":"dag_123","facets":{}},"run":{// (5)"runId":"eefd52c3-5871-4f0e-8ff5-237e9a6efb53","facets":{}},"inputs":[// (6){"namespace":"snowflake://abc123.snowflakecomputing.com","name":"OPS.DEFAULT.RUN_STATS","facets":{}},{"namespace":"snowflake://abc123.snowflakecomputing.com","name":"SOME.OTHER.TBL","facets":{}},{"namespace":"snowflake://abc123.snowflakecomputing.com","name":"AN.OTHER.TBL","facets":{}}],"outputs":[// (7){"namespace":"snowflake://abc123.snowflakecomputing.com","name":"OPS.DEFAULT.FULL_STATS","facets":{"columnLineage":{// (8)"_producer":"https://your.orchestrator/unique/id/123","_schemaURL":"https://openlineage.io/spec/facets/1-1-0/ColumnLineageDatasetFacet.json#/$defs/ColumnLineageDatasetFacet","fields":{"COLUMN":{// (9)"inputFields":[// (10){"namespace":"snowflake://abc123.snowflakecomputing.com","name":"OPS.DEFAULT.RUN_STATS","field":"COLUMN"},{"namespace":"snowflake://abc123.snowflakecomputing.com","name":"OPS.DEFAULT.RUN_STATS","field":"ONE"},{"namespace":"snowflake://abc123.snowflakecomputing.com","name":"OPS.DEFAULT.RUN_STATS","field":"TWO"}]},"ANOTHER":{"inputFields":[{"namespace":"snowflake://abc123.snowflakecomputing.com","name":"OPS.DEFAULT.RUN_STATS","field":"THREE"}]}}}}},{"namespace":"snowflake://abc123.snowflakecomputing.com","name":"AN.OTHER.VIEW","facets":{}}]}

Each event for a job run must have a time at which the event occurred.

Each event must have a URI indicating the code or system responsible for producing this lineage.

Each run of a job must consist ofat leasttwo events â aSTARTevent indicating when the job ran began, and some terminal state indicating when the job run finished.

START

Lineage is tracked through jobs. Each job must have:the name of a connection (that already exists in Atlan) as itsnamespace,a unique job name (used to idempotently update the same job with multiple runs)

the name of a connection (that already exists in Atlan) as itsnamespace,

namespace

a unique job name (used to idempotently update the same job with multiple runs)

A job must be run at least once for any lineage to exist, and each event for the same run of a job must be associated with the samerunId.

runId

You can define any number of inputs (sources) for lineage.Datasets used in data lineage need anamespacethat follows thesource-specific naming standards of OpenLineage.Thenameof a dataset should use a.-qualified form. For example, a table should beDATABASE_NAME.SCHEMA_NAME.TABLE_NAME.

Datasets used in data lineage need anamespacethat follows thesource-specific naming standards of OpenLineage.

namespace

Thenameof a dataset should use a.-qualified form. For example, a table should beDATABASE_NAME.SCHEMA_NAME.TABLE_NAME.

name

.

DATABASE_NAME.SCHEMA_NAME.TABLE_NAME

You can define any number of outputs (targets) for lineage.Datasets used in data lineage need anamespacethat follows thesource-specific naming standards of OpenLineage.Thenameof a dataset should use a.-qualified form. For example, a table should beDATABASE_NAME.SCHEMA_NAME.TABLE_NAME.

Datasets used in data lineage need anamespacethat follows thesource-specific naming standards of OpenLineage.

namespace

Thenameof a dataset should use a.-qualified form. For example, a table should beDATABASE_NAME.SCHEMA_NAME.TABLE_NAME.

name

.

DATABASE_NAME.SCHEMA_NAME.TABLE_NAME

For column-level lineage, you specify the mappingonlyon the target (outputs) end of the lineage, by including acolumnLineagefacet with an embeddedfieldsobject.

columnLineage

fields

Each key for thefieldsobject is the name of a field (column) in theoutputdataset.

fields

You can then provide a list that definesallinput (source) fields that map to this output field in column-level lineage.

123456789101112131415

{"eventTime":"2024-07-01T08:23:38.360567Z","producer":"https://your.orchestrator/unique/id/123","schemaURL":"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunEvent","eventType":"COMPLETE",// (1)"run":{"runId":"eefd52c3-5871-4f0e-8ff5-237e9a6efb53","facets":{}},"job":{"namespace":"ol-spark","name":"dag_123","facets":{}}}

{"eventTime":"2024-07-01T08:23:38.360567Z","producer":"https://your.orchestrator/unique/id/123","schemaURL":"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunEvent","eventType":"COMPLETE",// (1)"run":{"runId":"eefd52c3-5871-4f0e-8ff5-237e9a6efb53","facets":{}},"job":{"namespace":"ol-spark","name":"dag_123","facets":{}}}

Since each run of a job must consist ofat leasttwo events, do not forget to send the terminal state indicating when the job has finished (and whether it was successful with aCOMPLETEor had some error with aFAIL.)

COMPLETE

FAIL



## Remove lineage between assetsÂ¶
(source: https://developer.atlan.com/snippets/common-examples/lineage/manage/)

7.0.04.0.0

To remove lineage between assets, you need to delete theProcessentity that links them:

Process

Only deletes the process indicated, no more

Also be aware that this will only delete the process with the GUID specified. It willnotremove any column processes that may also exist. To remove those column processes as well, you must identify the GUID of each column-level process and call the samepurgemethod against each of those GUIDs.

purge

1234567

AssetMutationResponseresponse=Asset.purge(client,"b4113341-251b-4adc-81fb-2420501c30e6");// (1)Assetdeleted=response.getDeletedAssets().get(0);// (2)LineageProcessprocess;if(deletedinstanceofLineageProcess){process=(LineageProcess)deleted;// (3)}

AssetMutationResponseresponse=Asset.purge(client,"b4113341-251b-4adc-81fb-2420501c30e6");// (1)Assetdeleted=response.getDeletedAssets().get(0);// (2)LineageProcessprocess;if(deletedinstanceofLineageProcess){process=(LineageProcess)deleted;// (3)}

Provide the GUID for the process to the staticAsset.purge()method. Because this operation will directly remove the asset from Atlan, you mustprovide it anAtlanClientthrough which to connect to the tenant.

Asset.purge()

AtlanClient

The response will include that single process that was purged.

If you want to confirm the details, you'll need to type-check and then cast the genericAssetreturned into aProcess.

Asset

Process

12345678

frompyatlan.client.atlanimportAtlanClientfrompyatlan.model.assetsimportProcessclient=AtlanClient()response=client.asset.purge_by_guid(# (1)guid="b4113341-251b-4adc-81fb-2420501c30e6")# (2)assert(processes:=response.assets_deleted(Process))# (3)assertlen(processes)==1# (4)

frompyatlan.client.atlanimportAtlanClientfrompyatlan.model.assetsimportProcessclient=AtlanClient()response=client.asset.purge_by_guid(# (1)guid="b4113341-251b-4adc-81fb-2420501c30e6")# (2)assert(processes:=response.assets_deleted(Process))# (3)assertlen(processes)==1# (4)

Invoke theasset.purge_by_guidto delete theProcess.

asset.purge_by_guid

Process

Provide the GUID of the process to be purged.

Check that aProcesswas purged.

Process

Check that only 1Processwas purged.

Process

1234

valresponse:AssetMutationResponse=Asset.purge(client,"b4113341-251b-4adc-81fb-2420501c30e6")// (1)valdeleted=response.deletedAssets[0]// (2)valprocess=if(deletedisLineageProcess)deletedelsenull// (3)

valresponse:AssetMutationResponse=Asset.purge(client,"b4113341-251b-4adc-81fb-2420501c30e6")// (1)valdeleted=response.deletedAssets[0]// (2)valprocess=if(deletedisLineageProcess)deletedelsenull// (3)

Provide the GUID for the process to the staticAsset.purge()method. Because this operation will directly remove the asset from Atlan, you mustprovide it anAtlanClientthrough which to connect to the tenant.

Asset.purge()

AtlanClient

The response will include that single process that was purged.

If you want to confirm the details, you'll need to type-check and then cast the genericAssetreturned into aProcess.

Asset

Process

1

// (1)

// (1)

All of the details are in the request URL, there is no payload for a deletion. The GUID for the process itself (not any of its inputs or outputs) is what is listed in the URL.

This will irreversibly delete the process, and therefore the lineage it represented. The input and output assets themselves will also be updated, to no longer be linked to the (now non-existent) process. However, the input and output assets themselves will continue to exist in Atlan.



#### Cookie consent
(source: https://developer.atlan.com/snippets/common-examples/lineage/manage/)

We use cookies to:Anonymously measure page views, andAllow you to give us one-click feedback on any page.We donotcollect or store:Any personally identifiable information.Any information for any (re)marketing purposes.With your consent, you're helping us to make our documentation better ð

Anonymously measure page views, and

Allow you to give us one-click feedback on any page.

Any personally identifiable information.

Any information for any (re)marketing purposes.

Google Analytics
