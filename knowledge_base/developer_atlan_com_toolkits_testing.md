# Testing toolkitÂ¶
(source: https://developer.atlan.com/toolkits/testing/)

Overview

Getting started

Common tasks

Asset-specific

Governance structures

Reference

ToolkitsToolkitsPackagesPackagesRunning exampleDefine via templateRender your packageDevelop your logicTest your logicRelease (GA)Widget referenceTypedefsTypedefsRunning exampleDefine via templateRender your modelTest your modelBind the SDKsWrite integration testTest baseline UXRelease (GA)TestingTesting

ToolkitsToolkitsPackagesPackagesRunning exampleDefine via templateRender your packageDevelop your logicTest your logicRelease (GA)Widget referenceTypedefsTypedefsRunning exampleDefine via templateRender your modelTest your modelBind the SDKsWrite integration testTest baseline UXRelease (GA)TestingTesting

PackagesPackagesRunning exampleDefine via templateRender your packageDevelop your logicTest your logicRelease (GA)Widget reference

Running example

Define via template

Render your package

Develop your logic

Test your logic

Release (GA)

Widget reference

TypedefsTypedefsRunning exampleDefine via templateRender your modelTest your modelBind the SDKsWrite integration testTest baseline UXRelease (GA)

Running example

Define via template

Render your model

Test your model

Bind the SDKs

Write integration test

Test baseline UX

Release (GA)

TestingTesting

Overview

Getting startedGetting startedOther important conceptsDocumentation conventionsIntegration optionsIntegration optionsCLIdbtJavaPythonKotlinScalaClojureGoEventsRaw REST APISite map

Other important concepts

Documentation conventions

Integration optionsIntegration optionsCLIdbtJavaPythonKotlinScalaClojureGoEventsRaw REST API

CLI

dbt

Java

Python

Kotlin

Scala

Clojure

Go

Events

Raw REST API

Site map

Common tasksCommon tasksCommon asset actionsCommon asset actionsCertify assetsManage announcementsChange descriptionChange ownersTag (classify) assetsChange custom metadataLink terms to assetsLink domains to assetsManage asset READMEsAdd asset resourcesManage asset relationships with attributesAsset CRUD operationsAsset CRUD operationsCreate an assetRetrieve an assetUpdate an assetDelete an assetFind and apply suggestionsRestore an assetReview changes to an assetReview accesses of an assetGet all assets that...Get all assets that...Search for assetsSearch examplesLineageLineageManage lineageTraverse lineageBulk updatesBulk updatesCombine multiple operationsUpdate multiple assetsEnd-to-end bulk updateEvent handlingEvent handlingWebhook <> LambdaWebhook <> LambdaSet up LambdaCode your logicDeploy your codeSet up webhookManage your webhook

Common asset actionsCommon asset actionsCertify assetsManage announcementsChange descriptionChange ownersTag (classify) assetsChange custom metadataLink terms to assetsLink domains to assetsManage asset READMEsAdd asset resourcesManage asset relationships with attributes

Certify assets

Manage announcements

Change description

Change owners

Tag (classify) assets

Change custom metadata

Link terms to assets

Link domains to assets

Manage asset READMEs

Add asset resources

Manage asset relationships with attributes

Asset CRUD operationsAsset CRUD operationsCreate an assetRetrieve an assetUpdate an assetDelete an assetFind and apply suggestionsRestore an assetReview changes to an assetReview accesses of an asset

Create an asset

Retrieve an asset

Update an asset

Delete an asset

Find and apply suggestions

Restore an asset

Review changes to an asset

Review accesses of an asset

Get all assets that...Get all assets that...Search for assetsSearch examples

Search for assets

Search examples

LineageLineageManage lineageTraverse lineage

Manage lineage

Traverse lineage

Bulk updatesBulk updatesCombine multiple operationsUpdate multiple assetsEnd-to-end bulk update

Combine multiple operations

Update multiple assets

End-to-end bulk update

Event handlingEvent handlingWebhook <> LambdaWebhook <> LambdaSet up LambdaCode your logicDeploy your codeSet up webhookManage your webhook

Webhook <> LambdaWebhook <> LambdaSet up LambdaCode your logicDeploy your codeSet up webhookManage your webhook

Set up Lambda

Code your logic

Deploy your code

Set up webhook

Manage your webhook

Asset-specificAsset-specificGlossary operationsGlossary operationsCreate objectsRetrieval by nameCreate a hierarchyCategorize termsTraverse category hierarchyCreating assetsCreating assetsManage relational assetsManage cube assetsManage object store assetsManage object store assetsManage AWS S3 assetsManage Azure Data Lake Storage assetsManage Google Cloud Storage assetsManage BI assetsManage BI assetsManage Google Data Studio assetsManage Preset assetsManage Superset assetsManage API assetsManage file assetsManage Airflow assetsManage Kafka assetsManage Azure Event Hub assetsManage App assetsManage AI assetsManage Insights assetsManage QuickSight assetsManage DocumentDB assetsManage Data Quality assetsManage Data Quality assetsManage Data Quality rulesConnector types and iconsData meshData meshManage data domainsManage data productsData contractsData contractsManage data contracts (via CLI)Manage data contracts (via SDK)Profiling and popularityProfiling and popularityManage column profilingManage popularity

Glossary operationsGlossary operationsCreate objectsRetrieval by nameCreate a hierarchyCategorize termsTraverse category hierarchy

Create objects

Retrieval by name

Create a hierarchy

Categorize terms

Traverse category hierarchy

Creating assetsCreating assetsManage relational assetsManage cube assetsManage object store assetsManage object store assetsManage AWS S3 assetsManage Azure Data Lake Storage assetsManage Google Cloud Storage assetsManage BI assetsManage BI assetsManage Google Data Studio assetsManage Preset assetsManage Superset assetsManage API assetsManage file assetsManage Airflow assetsManage Kafka assetsManage Azure Event Hub assetsManage App assetsManage AI assetsManage Insights assetsManage QuickSight assetsManage DocumentDB assetsManage Data Quality assetsManage Data Quality assetsManage Data Quality rulesConnector types and icons

Manage relational assets

Manage cube assets

Manage object store assetsManage object store assetsManage AWS S3 assetsManage Azure Data Lake Storage assetsManage Google Cloud Storage assets

Manage AWS S3 assets

Manage Azure Data Lake Storage assets

Manage Google Cloud Storage assets

Manage BI assetsManage BI assetsManage Google Data Studio assetsManage Preset assetsManage Superset assets

Manage Google Data Studio assets

Manage Preset assets

Manage Superset assets

Manage API assets

Manage file assets

Manage Airflow assets

Manage Kafka assets

Manage Azure Event Hub assets

Manage App assets

Manage AI assets

Manage Insights assets

Manage QuickSight assets

Manage DocumentDB assets

Manage Data Quality assetsManage Data Quality assetsManage Data Quality rules

Manage Data Quality rules

Connector types and icons

Data meshData meshManage data domainsManage data products

Manage data domains

Manage data products

Data contractsData contractsManage data contracts (via CLI)Manage data contracts (via SDK)

Manage data contracts (via CLI)

Manage data contracts (via SDK)

Profiling and popularityProfiling and popularityManage column profilingManage popularity

Manage column profiling

Manage popularity

Governance structuresGovernance structuresCustom metadataCustom metadataCreate custom metadataRetrieve custom metadataUpdate custom metadataDelete custom metadataManage badgesManage options (enumerations)Tag managementTag managementManage Atlan tagsMonitor propagationAccess controlAccess controlManage personasManage purposesManage policiesAccess eventsAPI token managementRun queries on an assetUsers and groupsUsers and groupsCreate users and groupsRetrieve users and groupsUpdate users and groupsDelete users and groupsManage SSO group mappingPackages and workflowsPackages and workflowsManage workflowsManage workflow schedulesSupported packagesSupported packagesAthena assetsAsset importAsset export (basic)API token connection adminBigQuery assetsConnection deleteConfluent Kafka assetsdbt assetsDynamoDB assetsDatabricks assetsDatabricks minerFivetran enrichmentGlue assetsLooker assetsLineage builderLineage generator (no transformation)MongoDB assetsOracle assetsPostgres assetsPowerBI assetsRedshift assetsRelational assets builderSnowflake assetsSnowflake minerSigma assetsSQL Server assetsTableau assetsFile managementFile management

Custom metadataCustom metadataCreate custom metadataRetrieve custom metadataUpdate custom metadataDelete custom metadataManage badgesManage options (enumerations)

Create custom metadata

Retrieve custom metadata

Update custom metadata

Delete custom metadata

Manage badges

Manage options (enumerations)

Tag managementTag managementManage Atlan tagsMonitor propagation

Manage Atlan tags

Monitor propagation

Access controlAccess controlManage personasManage purposesManage policiesAccess eventsAPI token managementRun queries on an asset

Manage personas

Manage purposes

Manage policies

Access events

API token management

Run queries on an asset

Users and groupsUsers and groupsCreate users and groupsRetrieve users and groupsUpdate users and groupsDelete users and groupsManage SSO group mapping

Create users and groups

Retrieve users and groups

Update users and groups

Delete users and groups

Manage SSO group mapping

Packages and workflowsPackages and workflowsManage workflowsManage workflow schedulesSupported packagesSupported packagesAthena assetsAsset importAsset export (basic)API token connection adminBigQuery assetsConnection deleteConfluent Kafka assetsdbt assetsDynamoDB assetsDatabricks assetsDatabricks minerFivetran enrichmentGlue assetsLooker assetsLineage builderLineage generator (no transformation)MongoDB assetsOracle assetsPostgres assetsPowerBI assetsRedshift assetsRelational assets builderSnowflake assetsSnowflake minerSigma assetsSQL Server assetsTableau assets

Manage workflows

Manage workflow schedules

Supported packagesSupported packagesAthena assetsAsset importAsset export (basic)API token connection adminBigQuery assetsConnection deleteConfluent Kafka assetsdbt assetsDynamoDB assetsDatabricks assetsDatabricks minerFivetran enrichmentGlue assetsLooker assetsLineage builderLineage generator (no transformation)MongoDB assetsOracle assetsPostgres assetsPowerBI assetsRedshift assetsRelational assets builderSnowflake assetsSnowflake minerSigma assetsSQL Server assetsTableau assets

Athena assets

Asset import

Asset export (basic)

API token connection admin

BigQuery assets

Connection delete

Confluent Kafka assets

dbt assets

DynamoDB assets

Databricks assets

Databricks miner

Fivetran enrichment

Glue assets

Looker assets

Lineage builder

Lineage generator (no transformation)

MongoDB assets

Oracle assets

Postgres assets

PowerBI assets

Redshift assets

Relational assets builder

Snowflake assets

Snowflake miner

Sigma assets

SQL Server assets

Tableau assets

File managementFile management

ReferenceReferenceSearchingSearchingQueryingQueryingTerm-level queriesFull text queriesRank feature queriesCompound queriesSearchable fieldsSearchable fieldsCommon search fieldsGlossary-specific search fieldsLimiting detailsSorting search resultsPaging search resultsAggregating search resultsEventsEventsEvent triggersEvent triggersAsset is createdAsset is updatedAsset is deletedCustom metadata is addedCustom metadata is removedAsset is taggedAsset is untaggedLineage is createdEvent typesEvent typesENTITY_CREATEENTITY_UPDATEENTITY_DELETEBUSINESS_ATTRIBUTE_UPDATECLASSIFICATION_ADDCLASSIFICATION_DELETESpecificationsSpecificationsData contract specOpenLineage specTypesTypesCoreCoreReferenceableAssetConnectionCatalogTagTagAttachmentAccess controlAccess controlPersonaPurposeAuthPolicyAuthServiceBusinessPolicyBusinessPolicyExceptionBusinessPolicyIncidentBusinessPolicyLogIncidentLineageLineageColumnProcessBIProcessResourcesResourcesLinkFileReadmeReadmeTemplateBadgeWorkflowsWorkflowsWorkflowWorkflowRunTaskStructsStructsActionAuthPolicyConditionAuthPolicyValidityScheduleAwsCloudWatchMetricAwsTagAzureTagBadgeConditionBusinessPolicyRuleByocSsoConfigColumnValueFrequencyMapDbtMetricFilterDbtJobRunGoogleLabelGoogleTagHistogramKafkaTopicConsumptionMCRuleComparisonMCRuleSchedulePopularityInsightsSourceTagAttachmentSourceTagAttachmentValueSourceTagAttributeStarredDetailsEnumerationsEnumerationsAdfActivityStateADLSAccessTierADLSAccountStatusADLSEncryptionTypesADLSLeaseStateADLSLeaseStatusADLSObjectArchiveStatusADLSObjectTypeADLSPerformanceADLSProvisionStateADLSReplicationTypeADLSStorageKindAPIQueryParamTypeEnumatlas_operationAtlasGlossaryCategoryTypeAtlasGlossaryTermTypeAtlasGlossaryTypeAtlasGlossaryTermAssignmentStatusAtlasGlossaryTermRelationshipStatusAuthPolicyCategoryAuthPolicyResourceCategoryAuthPolicyTypecertificate_statusDataGlossaryDataProductCriticalityDataProductSensitivityDataProductStatusDataProductVisibilityDomoCardTypeDynamoDBSecondaryIndexProjectionTypeDynamoDBStatusfile_typeFivetranConnectorStatusFivetranProcessStatusgoogle_datastudio_asset_typeicon_typeincident_severitykafka_topic_cleanup_policykafka_topic_compression_typematillion_job_typeModelCardinalityTypeMongoDBCollectionValidationActionMongoDBCollectionValidationLevelOpenLineageRunStatepowerbi_endorsementquery_username_strategyquick_sight_analysis_statusquick_sight_dataset_field_typequick_sight_dataset_import_modequick_sight_folder_typeSchemaRegistrySchemaCompatibilitySchemaRegistrySchemaTypeSourceCostUnitTypetable_typeWorkflowRunStatusWorkflowRunTypeWorkflowStatusWorkflowTypeAbstractionsAbstractionsBICloudInsightObjectStoreEventStoreDataQualityMetricNoSQLSchemaRegistryGlossaryGlossaryAtlasGlossaryAtlasGlossaryCategoryAtlasGlossaryTermData meshData meshDataDomainDataProductDataContractStakeholderStakeholderTitleRelational databasesRelational databasesDatabaseSchemaTableViewMaterialisedViewColumnQueryTablePartitionCalculationViewBigqueryTagDatabricksUnityCatalogTagSnowflakeDynamicTableSnowflakePipeSnowflakeStreamSnowflakeTagProcedureFunctionSQLQuery organizationQuery organizationNamespaceCollectionFolderCubesCubesCubeCubeDimensionCubeHierarchyCubeFieldAPIsAPIsAPIPathAPISpecAPIObjectAPIQueryAPIFieldAirflowAirflowAirflowDagAirflowTaskAmazonAmazonAmazon DynamoDBAmazon DynamoDBDynamoDBTableDynamoDBSecondaryIndexDynamoDBGlobalSecondaryIndexDynamoDBLocalSecondaryIndexAWS S3AWS S3S3BucketS3ObjectAmazon QuickSightAmazon QuickSightQuickSightAnalysisQuickSightAnalysisVisualQuickSightDashboardQuickSightDashboardVisualQuickSightDatasetQuickSightDatasetFieldQuickSightFolderAnaplanAnaplanAnaplanWorkspaceAnaplanAppAnaplanPageAnaplanModelAnaplanModuleAnaplanListAnaplanSystemDimensionAnaplanDimensionAnaplanLineItemAnaplanViewAnomaloAnomaloAnomaloCheckAppAppApplicationApplicationFieldMicrosoft AzureMicrosoft AzureAzure Data FactoryAzure Data FactoryAdfActivityAdfDataflowAdfDatasetAdfLinkedserviceAdfPipelineAzure Data Lake StorageAzure Data Lake StorageADLSAccountADLSContainerADLSObjectAzure Event HubAzure Event HubAzureEventHubAzureEventHubConsumerGroupAzure Service BusAzure Service BusAzureServiceBusNamespaceAzureServiceBusSchemaAzureServiceBusTopicCosmos DBCosmos DBCosmosMongoDBAccountCosmosMongoDBCollectionCosmosMongoDBDatabaseCogniteCogniteCognite3DModelCogniteAssetCogniteEventCogniteFileCogniteSequenceCogniteTimeseriesCustomCustomCustomEntityDataverseDataverseDataverseEntityDataverseAttributedbtdbtDbtColumnProcessDbtMetricDbtModelDbtModelColumnDbtProcessDbtSourceDbtTagDbtTestDomoDomoDomoCardDomoDashboardDomoDatasetDomoDatasetColumnDocumentDBDocumentDBDocumentDBCollectionDocumentDBDatabaseFivetranFivetranFivetranConnectorGoogleGoogleGoogle Cloud StorageGoogle Cloud StorageGCSBucketGCSObjectGoogle Data StudioGoogle Data StudioDataStudioAssetIBMIBMIBMCognosCognosCognosDashboardCognosDatasourceCognosExplorationCognosFileCognosFolderCognosModuleCognosPackageCognosReportKafkaKafkaKafkaConsumerGroupKafkaTopicLookerLookerLookerDashboardLookerExploreLookerFieldLookerFolderLookerLookLookerModelLookerProjectLookerQueryLookerTileLookerViewMatillionMatillionMatillionComponentMatillionGroupMatillionJobMatillionProjectMetabaseMetabaseMetabaseCollectionMetabaseDashboardMetabaseQuestionMicroStrategyMicroStrategyMicroStrategyAttributeMicroStrategyCubeMicroStrategyDocumentMicroStrategyDossierMicroStrategyFactMicroStrategyMetricMicroStrategyProjectMicroStrategyReportMicroStrategyVisualizationModeModeModeChartModeCollectionModeQueryModeReportModeWorkspaceModelsModelsModelAttributeModelAttributeAssociationModelDataModelModelEntityModelEntityAssociationModelVersionMongoDBMongoDBMongoDBCollectionMongDBDatabaseMonte CarloMonte CarloMCIncidentMCMonitorPower BIPower BIPower BIPowerBIColumnPowerBIDashboardPowerBIDataflowPowerBIDataflowEntityColumnPowerBIDatasetPowerBIDatasourcePowerBIMeasurePowerBIPagePowerBIReportPowerBITablePowerBITilePowerBIWorkspacePresetPresetPresetChartPresetDashboardPresetDatasetPresetWorkspaceQlikQlikQlikAppQlikChartQlikDatasetQlikSheetQlikSpaceQlikStreamRedashRedashRedashDashboardRedashQueryRedashVisualizationSalesforceSalesforceSalesforceDashboardSalesforceFieldSalesforceObjectSalesforceOrganizationSalesforceReportSaaSSigmaSigmaSigmaWorkbookSigmaPageSigmaDataElementSigmaDataElementFieldSigmaDatasetSigmaDatasetColumnSisenseSisenseSisenseDashboardSisenseDatamodelSisenseDatamodelTableSisenseFolderSisenseWidgetSodaSodaSodaCheckSparkSparkSparkJobSupersetSupersetSupersetChartSupersetDashboardSupersetDatasetTableauTableauTableauCalculatedFieldTableauDashboardTableauDatasourceTableauDatasourceFieldTableauFlowTableauMetricTableauProjectTableauSiteTableauWorkbookTableauWorksheetThoughtSpotThoughtSpotThoughtspotAnswerThoughtspotColumnThoughtspotDashletThoughtspotLiveboardThoughtspotTableThoughtspotViewThoughtspotWorksheetEndpoints

SearchingSearchingQueryingQueryingTerm-level queriesFull text queriesRank feature queriesCompound queriesSearchable fieldsSearchable fieldsCommon search fieldsGlossary-specific search fieldsLimiting detailsSorting search resultsPaging search resultsAggregating search results

QueryingQueryingTerm-level queriesFull text queriesRank feature queriesCompound queries

Term-level queries

Full text queries

Rank feature queries

Compound queries

Searchable fieldsSearchable fieldsCommon search fieldsGlossary-specific search fields

Common search fields

Glossary-specific search fields

Limiting details

Sorting search results

Paging search results

Aggregating search results

EventsEventsEvent triggersEvent triggersAsset is createdAsset is updatedAsset is deletedCustom metadata is addedCustom metadata is removedAsset is taggedAsset is untaggedLineage is createdEvent typesEvent typesENTITY_CREATEENTITY_UPDATEENTITY_DELETEBUSINESS_ATTRIBUTE_UPDATECLASSIFICATION_ADDCLASSIFICATION_DELETE

Event triggersEvent triggersAsset is createdAsset is updatedAsset is deletedCustom metadata is addedCustom metadata is removedAsset is taggedAsset is untaggedLineage is created

Asset is created

Asset is updated

Asset is deleted

Custom metadata is added

Custom metadata is removed

Asset is tagged

Asset is untagged

Lineage is created

Event typesEvent typesENTITY_CREATEENTITY_UPDATEENTITY_DELETEBUSINESS_ATTRIBUTE_UPDATECLASSIFICATION_ADDCLASSIFICATION_DELETE

ENTITY_CREATE

ENTITY_UPDATE

ENTITY_DELETE

BUSINESS_ATTRIBUTE_UPDATE

CLASSIFICATION_ADD

CLASSIFICATION_DELETE

SpecificationsSpecificationsData contract specOpenLineage spec

Data contract spec

OpenLineage spec

TypesTypesCoreCoreReferenceableAssetConnectionCatalogTagTagAttachmentAccess controlAccess controlPersonaPurposeAuthPolicyAuthServiceBusinessPolicyBusinessPolicyExceptionBusinessPolicyIncidentBusinessPolicyLogIncidentLineageLineageColumnProcessBIProcessResourcesResourcesLinkFileReadmeReadmeTemplateBadgeWorkflowsWorkflowsWorkflowWorkflowRunTaskStructsStructsActionAuthPolicyConditionAuthPolicyValidityScheduleAwsCloudWatchMetricAwsTagAzureTagBadgeConditionBusinessPolicyRuleByocSsoConfigColumnValueFrequencyMapDbtMetricFilterDbtJobRunGoogleLabelGoogleTagHistogramKafkaTopicConsumptionMCRuleComparisonMCRuleSchedulePopularityInsightsSourceTagAttachmentSourceTagAttachmentValueSourceTagAttributeStarredDetailsEnumerationsEnumerationsAdfActivityStateADLSAccessTierADLSAccountStatusADLSEncryptionTypesADLSLeaseStateADLSLeaseStatusADLSObjectArchiveStatusADLSObjectTypeADLSPerformanceADLSProvisionStateADLSReplicationTypeADLSStorageKindAPIQueryParamTypeEnumatlas_operationAtlasGlossaryCategoryTypeAtlasGlossaryTermTypeAtlasGlossaryTypeAtlasGlossaryTermAssignmentStatusAtlasGlossaryTermRelationshipStatusAuthPolicyCategoryAuthPolicyResourceCategoryAuthPolicyTypecertificate_statusDataGlossaryDataProductCriticalityDataProductSensitivityDataProductStatusDataProductVisibilityDomoCardTypeDynamoDBSecondaryIndexProjectionTypeDynamoDBStatusfile_typeFivetranConnectorStatusFivetranProcessStatusgoogle_datastudio_asset_typeicon_typeincident_severitykafka_topic_cleanup_policykafka_topic_compression_typematillion_job_typeModelCardinalityTypeMongoDBCollectionValidationActionMongoDBCollectionValidationLevelOpenLineageRunStatepowerbi_endorsementquery_username_strategyquick_sight_analysis_statusquick_sight_dataset_field_typequick_sight_dataset_import_modequick_sight_folder_typeSchemaRegistrySchemaCompatibilitySchemaRegistrySchemaTypeSourceCostUnitTypetable_typeWorkflowRunStatusWorkflowRunTypeWorkflowStatusWorkflowTypeAbstractionsAbstractionsBICloudInsightObjectStoreEventStoreDataQualityMetricNoSQLSchemaRegistryGlossaryGlossaryAtlasGlossaryAtlasGlossaryCategoryAtlasGlossaryTermData meshData meshDataDomainDataProductDataContractStakeholderStakeholderTitleRelational databasesRelational databasesDatabaseSchemaTableViewMaterialisedViewColumnQueryTablePartitionCalculationViewBigqueryTagDatabricksUnityCatalogTagSnowflakeDynamicTableSnowflakePipeSnowflakeStreamSnowflakeTagProcedureFunctionSQLQuery organizationQuery organizationNamespaceCollectionFolderCubesCubesCubeCubeDimensionCubeHierarchyCubeFieldAPIsAPIsAPIPathAPISpecAPIObjectAPIQueryAPIFieldAirflowAirflowAirflowDagAirflowTaskAmazonAmazonAmazon DynamoDBAmazon DynamoDBDynamoDBTableDynamoDBSecondaryIndexDynamoDBGlobalSecondaryIndexDynamoDBLocalSecondaryIndexAWS S3AWS S3S3BucketS3ObjectAmazon QuickSightAmazon QuickSightQuickSightAnalysisQuickSightAnalysisVisualQuickSightDashboardQuickSightDashboardVisualQuickSightDatasetQuickSightDatasetFieldQuickSightFolderAnaplanAnaplanAnaplanWorkspaceAnaplanAppAnaplanPageAnaplanModelAnaplanModuleAnaplanListAnaplanSystemDimensionAnaplanDimensionAnaplanLineItemAnaplanViewAnomaloAnomaloAnomaloCheckAppAppApplicationApplicationFieldMicrosoft AzureMicrosoft AzureAzure Data FactoryAzure Data FactoryAdfActivityAdfDataflowAdfDatasetAdfLinkedserviceAdfPipelineAzure Data Lake StorageAzure Data Lake StorageADLSAccountADLSContainerADLSObjectAzure Event HubAzure Event HubAzureEventHubAzureEventHubConsumerGroupAzure Service BusAzure Service BusAzureServiceBusNamespaceAzureServiceBusSchemaAzureServiceBusTopicCosmos DBCosmos DBCosmosMongoDBAccountCosmosMongoDBCollectionCosmosMongoDBDatabaseCogniteCogniteCognite3DModelCogniteAssetCogniteEventCogniteFileCogniteSequenceCogniteTimeseriesCustomCustomCustomEntityDataverseDataverseDataverseEntityDataverseAttributedbtdbtDbtColumnProcessDbtMetricDbtModelDbtModelColumnDbtProcessDbtSourceDbtTagDbtTestDomoDomoDomoCardDomoDashboardDomoDatasetDomoDatasetColumnDocumentDBDocumentDBDocumentDBCollectionDocumentDBDatabaseFivetranFivetranFivetranConnectorGoogleGoogleGoogle Cloud StorageGoogle Cloud StorageGCSBucketGCSObjectGoogle Data StudioGoogle Data StudioDataStudioAssetIBMIBMIBMCognosCognosCognosDashboardCognosDatasourceCognosExplorationCognosFileCognosFolderCognosModuleCognosPackageCognosReportKafkaKafkaKafkaConsumerGroupKafkaTopicLookerLookerLookerDashboardLookerExploreLookerFieldLookerFolderLookerLookLookerModelLookerProjectLookerQueryLookerTileLookerViewMatillionMatillionMatillionComponentMatillionGroupMatillionJobMatillionProjectMetabaseMetabaseMetabaseCollectionMetabaseDashboardMetabaseQuestionMicroStrategyMicroStrategyMicroStrategyAttributeMicroStrategyCubeMicroStrategyDocumentMicroStrategyDossierMicroStrategyFactMicroStrategyMetricMicroStrategyProjectMicroStrategyReportMicroStrategyVisualizationModeModeModeChartModeCollectionModeQueryModeReportModeWorkspaceModelsModelsModelAttributeModelAttributeAssociationModelDataModelModelEntityModelEntityAssociationModelVersionMongoDBMongoDBMongoDBCollectionMongDBDatabaseMonte CarloMonte CarloMCIncidentMCMonitorPower BIPower BIPower BIPowerBIColumnPowerBIDashboardPowerBIDataflowPowerBIDataflowEntityColumnPowerBIDatasetPowerBIDatasourcePowerBIMeasurePowerBIPagePowerBIReportPowerBITablePowerBITilePowerBIWorkspacePresetPresetPresetChartPresetDashboardPresetDatasetPresetWorkspaceQlikQlikQlikAppQlikChartQlikDatasetQlikSheetQlikSpaceQlikStreamRedashRedashRedashDashboardRedashQueryRedashVisualizationSalesforceSalesforceSalesforceDashboardSalesforceFieldSalesforceObjectSalesforceOrganizationSalesforceReportSaaSSigmaSigmaSigmaWorkbookSigmaPageSigmaDataElementSigmaDataElementFieldSigmaDatasetSigmaDatasetColumnSisenseSisenseSisenseDashboardSisenseDatamodelSisenseDatamodelTableSisenseFolderSisenseWidgetSodaSodaSodaCheckSparkSparkSparkJobSupersetSupersetSupersetChartSupersetDashboardSupersetDatasetTableauTableauTableauCalculatedFieldTableauDashboardTableauDatasourceTableauDatasourceFieldTableauFlowTableauMetricTableauProjectTableauSiteTableauWorkbookTableauWorksheetThoughtSpotThoughtSpotThoughtspotAnswerThoughtspotColumnThoughtspotDashletThoughtspotLiveboardThoughtspotTableThoughtspotViewThoughtspotWorksheet

CoreCoreReferenceableAssetConnectionCatalogTagTagAttachmentAccess controlAccess controlPersonaPurposeAuthPolicyAuthServiceBusinessPolicyBusinessPolicyExceptionBusinessPolicyIncidentBusinessPolicyLogIncidentLineageLineageColumnProcessBIProcessResourcesResourcesLinkFileReadmeReadmeTemplateBadgeWorkflowsWorkflowsWorkflowWorkflowRunTaskStructsStructsActionAuthPolicyConditionAuthPolicyValidityScheduleAwsCloudWatchMetricAwsTagAzureTagBadgeConditionBusinessPolicyRuleByocSsoConfigColumnValueFrequencyMapDbtMetricFilterDbtJobRunGoogleLabelGoogleTagHistogramKafkaTopicConsumptionMCRuleComparisonMCRuleSchedulePopularityInsightsSourceTagAttachmentSourceTagAttachmentValueSourceTagAttributeStarredDetailsEnumerationsEnumerationsAdfActivityStateADLSAccessTierADLSAccountStatusADLSEncryptionTypesADLSLeaseStateADLSLeaseStatusADLSObjectArchiveStatusADLSObjectTypeADLSPerformanceADLSProvisionStateADLSReplicationTypeADLSStorageKindAPIQueryParamTypeEnumatlas_operationAtlasGlossaryCategoryTypeAtlasGlossaryTermTypeAtlasGlossaryTypeAtlasGlossaryTermAssignmentStatusAtlasGlossaryTermRelationshipStatusAuthPolicyCategoryAuthPolicyResourceCategoryAuthPolicyTypecertificate_statusDataGlossaryDataProductCriticalityDataProductSensitivityDataProductStatusDataProductVisibilityDomoCardTypeDynamoDBSecondaryIndexProjectionTypeDynamoDBStatusfile_typeFivetranConnectorStatusFivetranProcessStatusgoogle_datastudio_asset_typeicon_typeincident_severitykafka_topic_cleanup_policykafka_topic_compression_typematillion_job_typeModelCardinalityTypeMongoDBCollectionValidationActionMongoDBCollectionValidationLevelOpenLineageRunStatepowerbi_endorsementquery_username_strategyquick_sight_analysis_statusquick_sight_dataset_field_typequick_sight_dataset_import_modequick_sight_folder_typeSchemaRegistrySchemaCompatibilitySchemaRegistrySchemaTypeSourceCostUnitTypetable_typeWorkflowRunStatusWorkflowRunTypeWorkflowStatusWorkflowTypeAbstractionsAbstractionsBICloudInsightObjectStoreEventStoreDataQualityMetricNoSQLSchemaRegistry

Referenceable

Asset

Connection

Catalog

Tag

TagAttachment

Access controlAccess controlPersonaPurposeAuthPolicyAuthService

Persona

Purpose

AuthPolicy

AuthService

BusinessPolicy

BusinessPolicyException

BusinessPolicyIncident

BusinessPolicyLog

Incident

LineageLineageColumnProcessBIProcess

ColumnProcess

BIProcess

ResourcesResourcesLinkFileReadmeReadmeTemplateBadge

Link

File

Readme

ReadmeTemplate

Badge

WorkflowsWorkflowsWorkflowWorkflowRunTask

Workflow

WorkflowRun

Task

StructsStructsActionAuthPolicyConditionAuthPolicyValidityScheduleAwsCloudWatchMetricAwsTagAzureTagBadgeConditionBusinessPolicyRuleByocSsoConfigColumnValueFrequencyMapDbtMetricFilterDbtJobRunGoogleLabelGoogleTagHistogramKafkaTopicConsumptionMCRuleComparisonMCRuleSchedulePopularityInsightsSourceTagAttachmentSourceTagAttachmentValueSourceTagAttributeStarredDetails

Action

AuthPolicyCondition

AuthPolicyValiditySchedule

AwsCloudWatchMetric

AwsTag

AzureTag

BadgeCondition

BusinessPolicyRule

ByocSsoConfig

ColumnValueFrequencyMap

DbtMetricFilter

DbtJobRun

GoogleLabel

GoogleTag

Histogram

KafkaTopicConsumption

MCRuleComparison

MCRuleSchedule

PopularityInsights

SourceTagAttachment

SourceTagAttachmentValue

SourceTagAttribute

StarredDetails

EnumerationsEnumerationsAdfActivityStateADLSAccessTierADLSAccountStatusADLSEncryptionTypesADLSLeaseStateADLSLeaseStatusADLSObjectArchiveStatusADLSObjectTypeADLSPerformanceADLSProvisionStateADLSReplicationTypeADLSStorageKindAPIQueryParamTypeEnumatlas_operationAtlasGlossaryCategoryTypeAtlasGlossaryTermTypeAtlasGlossaryTypeAtlasGlossaryTermAssignmentStatusAtlasGlossaryTermRelationshipStatusAuthPolicyCategoryAuthPolicyResourceCategoryAuthPolicyTypecertificate_statusDataGlossaryDataProductCriticalityDataProductSensitivityDataProductStatusDataProductVisibilityDomoCardTypeDynamoDBSecondaryIndexProjectionTypeDynamoDBStatusfile_typeFivetranConnectorStatusFivetranProcessStatusgoogle_datastudio_asset_typeicon_typeincident_severitykafka_topic_cleanup_policykafka_topic_compression_typematillion_job_typeModelCardinalityTypeMongoDBCollectionValidationActionMongoDBCollectionValidationLevelOpenLineageRunStatepowerbi_endorsementquery_username_strategyquick_sight_analysis_statusquick_sight_dataset_field_typequick_sight_dataset_import_modequick_sight_folder_typeSchemaRegistrySchemaCompatibilitySchemaRegistrySchemaTypeSourceCostUnitTypetable_typeWorkflowRunStatusWorkflowRunTypeWorkflowStatusWorkflowType

AdfActivityState

ADLSAccessTier

ADLSAccountStatus

ADLSEncryptionTypes

ADLSLeaseState

ADLSLeaseStatus

ADLSObjectArchiveStatus

ADLSObjectType

ADLSPerformance

ADLSProvisionState

ADLSReplicationType

ADLSStorageKind

APIQueryParamTypeEnum

atlas_operation

AtlasGlossaryCategoryType

AtlasGlossaryTermType

AtlasGlossaryType

AtlasGlossaryTermAssignmentStatus

AtlasGlossaryTermRelationshipStatus

AuthPolicyCategory

AuthPolicyResourceCategory

AuthPolicyType

certificate_status

DataGlossary

DataProductCriticality

DataProductSensitivity

DataProductStatus

DataProductVisibility

DomoCardType

DynamoDBSecondaryIndexProjectionType

DynamoDBStatus

file_type

FivetranConnectorStatus

FivetranProcessStatus

google_datastudio_asset_type

icon_type

incident_severity

kafka_topic_cleanup_policy

kafka_topic_compression_type

matillion_job_type

ModelCardinalityType

MongoDBCollectionValidationAction

MongoDBCollectionValidationLevel

OpenLineageRunState

powerbi_endorsement

query_username_strategy

quick_sight_analysis_status

quick_sight_dataset_field_type

quick_sight_dataset_import_mode

quick_sight_folder_type

SchemaRegistrySchemaCompatibility

SchemaRegistrySchemaType

SourceCostUnitType

table_type

WorkflowRunStatus

WorkflowRunType

WorkflowStatus

WorkflowType

AbstractionsAbstractionsBICloudInsightObjectStoreEventStoreDataQualityMetricNoSQLSchemaRegistry

BI

Cloud

Insight

ObjectStore

EventStore

DataQuality

Metric

NoSQL

SchemaRegistry

GlossaryGlossaryAtlasGlossaryAtlasGlossaryCategoryAtlasGlossaryTerm

AtlasGlossary

AtlasGlossaryCategory

AtlasGlossaryTerm

Data meshData meshDataDomainDataProductDataContractStakeholderStakeholderTitle

DataDomain

DataProduct

DataContract

Stakeholder

StakeholderTitle

Relational databasesRelational databasesDatabaseSchemaTableViewMaterialisedViewColumnQueryTablePartitionCalculationViewBigqueryTagDatabricksUnityCatalogTagSnowflakeDynamicTableSnowflakePipeSnowflakeStreamSnowflakeTagProcedureFunctionSQL

Database

Schema

Table

View

MaterialisedView

Column

Query

TablePartition

CalculationView

BigqueryTag

DatabricksUnityCatalogTag

SnowflakeDynamicTable

SnowflakePipe

SnowflakeStream

SnowflakeTag

Procedure

Function

SQL

Query organizationQuery organizationNamespaceCollectionFolder

Namespace

Collection

Folder

CubesCubesCubeCubeDimensionCubeHierarchyCubeField

Cube

CubeDimension

CubeHierarchy

CubeField

APIsAPIsAPIPathAPISpecAPIObjectAPIQueryAPIField

APIPath

APISpec

APIObject

APIQuery

APIField

AirflowAirflowAirflowDagAirflowTask

AirflowDag

AirflowTask

AmazonAmazonAmazon DynamoDBAmazon DynamoDBDynamoDBTableDynamoDBSecondaryIndexDynamoDBGlobalSecondaryIndexDynamoDBLocalSecondaryIndexAWS S3AWS S3S3BucketS3ObjectAmazon QuickSightAmazon QuickSightQuickSightAnalysisQuickSightAnalysisVisualQuickSightDashboardQuickSightDashboardVisualQuickSightDatasetQuickSightDatasetFieldQuickSightFolder

Amazon DynamoDBAmazon DynamoDBDynamoDBTableDynamoDBSecondaryIndexDynamoDBGlobalSecondaryIndexDynamoDBLocalSecondaryIndex

DynamoDBTable

DynamoDBSecondaryIndex

DynamoDBGlobalSecondaryIndex

DynamoDBLocalSecondaryIndex

AWS S3AWS S3S3BucketS3Object

S3Bucket

S3Object

Amazon QuickSightAmazon QuickSightQuickSightAnalysisQuickSightAnalysisVisualQuickSightDashboardQuickSightDashboardVisualQuickSightDatasetQuickSightDatasetFieldQuickSightFolder

QuickSightAnalysis

QuickSightAnalysisVisual

QuickSightDashboard

QuickSightDashboardVisual

QuickSightDataset

QuickSightDatasetField

QuickSightFolder

AnaplanAnaplanAnaplanWorkspaceAnaplanAppAnaplanPageAnaplanModelAnaplanModuleAnaplanListAnaplanSystemDimensionAnaplanDimensionAnaplanLineItemAnaplanView

AnaplanWorkspace

AnaplanApp

AnaplanPage

AnaplanModel

AnaplanModule

AnaplanList

AnaplanSystemDimension

AnaplanDimension

AnaplanLineItem

AnaplanView

AnomaloAnomaloAnomaloCheck

AnomaloCheck

AppAppApplicationApplicationField

Application

ApplicationField

Microsoft AzureMicrosoft AzureAzure Data FactoryAzure Data FactoryAdfActivityAdfDataflowAdfDatasetAdfLinkedserviceAdfPipelineAzure Data Lake StorageAzure Data Lake StorageADLSAccountADLSContainerADLSObjectAzure Event HubAzure Event HubAzureEventHubAzureEventHubConsumerGroupAzure Service BusAzure Service BusAzureServiceBusNamespaceAzureServiceBusSchemaAzureServiceBusTopicCosmos DBCosmos DBCosmosMongoDBAccountCosmosMongoDBCollectionCosmosMongoDBDatabase

Azure Data FactoryAzure Data FactoryAdfActivityAdfDataflowAdfDatasetAdfLinkedserviceAdfPipeline

AdfActivity

AdfDataflow

AdfDataset

AdfLinkedservice

AdfPipeline

Azure Data Lake StorageAzure Data Lake StorageADLSAccountADLSContainerADLSObject

ADLSAccount

ADLSContainer

ADLSObject

Azure Event HubAzure Event HubAzureEventHubAzureEventHubConsumerGroup

AzureEventHub

AzureEventHubConsumerGroup

Azure Service BusAzure Service BusAzureServiceBusNamespaceAzureServiceBusSchemaAzureServiceBusTopic

AzureServiceBusNamespace

AzureServiceBusSchema

AzureServiceBusTopic

Cosmos DBCosmos DBCosmosMongoDBAccountCosmosMongoDBCollectionCosmosMongoDBDatabase

CosmosMongoDBAccount

CosmosMongoDBCollection

CosmosMongoDBDatabase

CogniteCogniteCognite3DModelCogniteAssetCogniteEventCogniteFileCogniteSequenceCogniteTimeseries

Cognite3DModel

CogniteAsset

CogniteEvent

CogniteFile

CogniteSequence

CogniteTimeseries

CustomCustomCustomEntity

CustomEntity

DataverseDataverseDataverseEntityDataverseAttribute

DataverseEntity

DataverseAttribute

dbtdbtDbtColumnProcessDbtMetricDbtModelDbtModelColumnDbtProcessDbtSourceDbtTagDbtTest

DbtColumnProcess

DbtMetric

DbtModel

DbtModelColumn

DbtProcess

DbtSource

DbtTag

DbtTest

DomoDomoDomoCardDomoDashboardDomoDatasetDomoDatasetColumn

DomoCard

DomoDashboard

DomoDataset

DomoDatasetColumn

DocumentDBDocumentDBDocumentDBCollectionDocumentDBDatabase

DocumentDBCollection

DocumentDBDatabase

FivetranFivetranFivetranConnector

FivetranConnector

GoogleGoogleGoogle Cloud StorageGoogle Cloud StorageGCSBucketGCSObjectGoogle Data StudioGoogle Data StudioDataStudioAsset

Google Cloud StorageGoogle Cloud StorageGCSBucketGCSObject

GCSBucket

GCSObject

Google Data StudioGoogle Data StudioDataStudioAsset

DataStudioAsset

IBMIBMIBMCognosCognosCognosDashboardCognosDatasourceCognosExplorationCognosFileCognosFolderCognosModuleCognosPackageCognosReport

CognosCognosCognosDashboardCognosDatasourceCognosExplorationCognosFileCognosFolderCognosModuleCognosPackageCognosReport

CognosDashboard

CognosDatasource

CognosExploration

CognosFile

CognosFolder

CognosModule

CognosPackage

CognosReport

KafkaKafkaKafkaConsumerGroupKafkaTopic

KafkaConsumerGroup

KafkaTopic

LookerLookerLookerDashboardLookerExploreLookerFieldLookerFolderLookerLookLookerModelLookerProjectLookerQueryLookerTileLookerView

LookerDashboard

LookerExplore

LookerField

LookerFolder

LookerLook

LookerModel

LookerProject

LookerQuery

LookerTile

LookerView

MatillionMatillionMatillionComponentMatillionGroupMatillionJobMatillionProject

MatillionComponent

MatillionGroup

MatillionJob

MatillionProject

MetabaseMetabaseMetabaseCollectionMetabaseDashboardMetabaseQuestion

MetabaseCollection

MetabaseDashboard

MetabaseQuestion

MicroStrategyMicroStrategyMicroStrategyAttributeMicroStrategyCubeMicroStrategyDocumentMicroStrategyDossierMicroStrategyFactMicroStrategyMetricMicroStrategyProjectMicroStrategyReportMicroStrategyVisualization

MicroStrategyAttribute

MicroStrategyCube

MicroStrategyDocument

MicroStrategyDossier

MicroStrategyFact

MicroStrategyMetric

MicroStrategyProject

MicroStrategyReport

MicroStrategyVisualization

ModeModeModeChartModeCollectionModeQueryModeReportModeWorkspace

ModeChart

ModeCollection

ModeQuery

ModeReport

ModeWorkspace

ModelsModelsModelAttributeModelAttributeAssociationModelDataModelModelEntityModelEntityAssociationModelVersion

ModelAttribute

ModelAttributeAssociation

ModelDataModel

ModelEntity

ModelEntityAssociation

ModelVersion

MongoDBMongoDBMongoDBCollectionMongDBDatabase

MongoDBCollection

MongDBDatabase

Monte CarloMonte CarloMCIncidentMCMonitor

MCIncident

MCMonitor

Power BIPower BIPower BIPowerBIColumnPowerBIDashboardPowerBIDataflowPowerBIDataflowEntityColumnPowerBIDatasetPowerBIDatasourcePowerBIMeasurePowerBIPagePowerBIReportPowerBITablePowerBITilePowerBIWorkspace

PowerBIColumn

PowerBIDashboard

PowerBIDataflow

PowerBIDataflowEntityColumn

PowerBIDataset

PowerBIDatasource

PowerBIMeasure

PowerBIPage

PowerBIReport

PowerBITable

PowerBITile

PowerBIWorkspace

PresetPresetPresetChartPresetDashboardPresetDatasetPresetWorkspace

PresetChart

PresetDashboard

PresetDataset

PresetWorkspace

QlikQlikQlikAppQlikChartQlikDatasetQlikSheetQlikSpaceQlikStream

QlikApp

QlikChart

QlikDataset

QlikSheet

QlikSpace

QlikStream

RedashRedashRedashDashboardRedashQueryRedashVisualization

RedashDashboard

RedashQuery

RedashVisualization

SalesforceSalesforceSalesforceDashboardSalesforceFieldSalesforceObjectSalesforceOrganizationSalesforceReportSaaS

SalesforceDashboard

SalesforceField

SalesforceObject

SalesforceOrganization

SalesforceReport

SaaS

SigmaSigmaSigmaWorkbookSigmaPageSigmaDataElementSigmaDataElementFieldSigmaDatasetSigmaDatasetColumn

SigmaWorkbook

SigmaPage

SigmaDataElement

SigmaDataElementField

SigmaDataset

SigmaDatasetColumn

SisenseSisenseSisenseDashboardSisenseDatamodelSisenseDatamodelTableSisenseFolderSisenseWidget

SisenseDashboard

SisenseDatamodel

SisenseDatamodelTable

SisenseFolder

SisenseWidget

SodaSodaSodaCheck

SodaCheck

SparkSparkSparkJob

SparkJob

SupersetSupersetSupersetChartSupersetDashboardSupersetDataset

SupersetChart

SupersetDashboard

SupersetDataset

TableauTableauTableauCalculatedFieldTableauDashboardTableauDatasourceTableauDatasourceFieldTableauFlowTableauMetricTableauProjectTableauSiteTableauWorkbookTableauWorksheet

TableauCalculatedField

TableauDashboard

TableauDatasource

TableauDatasourceField

TableauFlow

TableauMetric

TableauProject

TableauSite

TableauWorkbook

TableauWorksheet

ThoughtSpotThoughtSpotThoughtspotAnswerThoughtspotColumnThoughtspotDashletThoughtspotLiveboardThoughtspotTableThoughtspotViewThoughtspotWorksheet

ThoughtspotAnswer

ThoughtspotColumn

ThoughtspotDashlet

ThoughtspotLiveboard

ThoughtspotTable

ThoughtspotView

ThoughtspotWorksheet

Endpoints

Writing tests for non-toolkit based scriptsStep 1: Rename directory to snake_caseStep 2: Refactor main.pyStep 3: Add integration testsRecommended testing strategy for scripts

Step 1: Rename directory to snake_case

Step 2: Refactor main.py

Step 3: Add integration tests

Recommended testing strategy for scripts

Writing tests for non-toolkit based scripts using Cursor AI code editorStep 1: Setup Cursor rulesStep 2: Running the agent with the defined Rules

Step 1: Setup Cursor rules

Step 2: Running the agent with the defined Rules

Mocking / Patching third party HTTP interactionsWhen do you need this?The problem with real API calls in testsThe solution: VCR (Video Cassette Recorder)How VCR worksWrite VCR-based integration tests

When do you need this?

The problem with real API calls in tests

The solution: VCR (Video Cassette Recorder)

How VCR works

Write VCR-based integration tests

Containerizing marketplace scriptsOverviewPrerequisitesRequired files for containerizationDevelopment workflowProduction release workflowBest practices for containerized scripts

Overview

Prerequisites

Required files for containerization

Development workflow

Production release workflow

Best practices for containerized scripts

With the testing toolkit we can guide you to write robust, reusable integration tests for connectors and utilities in Atlan.



## Writing tests for non-toolkit based scriptsÂ¶
(source: https://developer.atlan.com/toolkits/testing/)

You can write integration tests for existing scripts in themarketplace-csa-scriptsrepository, even if they are not based on package toolkits. These tests help verify script behavior end-to-end in a real Atlan tenant.

marketplace-csa-scripts

We'll begin by performing minimal refactoring of the existing script, as it's necessary to enable writing integration tests.



### Step 1: Rename directory tosnake_caseÂ¶
(source: https://developer.atlan.com/toolkits/testing/)

snake_case

If the script is inkebab-casedirectory, convert it tosnake_case.

kebab-case

snake_case

Do this just after renaming

Update references inmkdocs.yml, delete the old directory, and verify imports/links still work.

mkdocs.yml

For example:

Before:scripts/
â””â”€â”€ designation-based-group-provisioning/
    â”œâ”€â”€ main.py
    â”œâ”€â”€ index.md
    â””â”€â”€ tests/
        â””â”€â”€ test_main.py

scripts/
â””â”€â”€ designation-based-group-provisioning/
    â”œâ”€â”€ main.py
    â”œâ”€â”€ index.md
    â””â”€â”€ tests/
        â””â”€â”€ test_main.py

scripts/
â””â”€â”€ designation-based-group-provisioning/
    â”œâ”€â”€ main.py
    â”œâ”€â”€ index.md
    â””â”€â”€ tests/
        â””â”€â”€ test_main.py

After:scripts/
â””â”€â”€ designation_based_group_provisioning/
    â”œâ”€â”€ main.py
    â”œâ”€â”€ index.md
    â””â”€â”€ tests/
        â””â”€â”€ test_main.py

scripts/
â””â”€â”€ designation_based_group_provisioning/
    â”œâ”€â”€ main.py
    â”œâ”€â”€ index.md
    â””â”€â”€ tests/
        â””â”€â”€ test_main.py

scripts/
â””â”€â”€ designation_based_group_provisioning/
    â”œâ”€â”€ main.py
    â”œâ”€â”€ index.md
    â””â”€â”€ tests/
        â””â”€â”€ test_main.py



### Step 2: Refactormain.pyÂ¶
(source: https://developer.atlan.com/toolkits/testing/)

main.py

DO

Refactor the script without altering logic or flow.

Wrap all logic inside functions.

Create a single entry point:main(args: argparse.Namespace)

main(args: argparse.Namespace)

Call helper functions frommain()â€” each should receive only requiredargsorinputs.

main()

args

inputs

DO NOT

Rename or restructure existing functions.

Change the sequence or logic flow.

Modify argument parsing.

Add/remove logging unless required for debugging.

Example refactoredmain.py:

main.py

1234567891011121314151617181920212223242526272829303132333435

importargparsefromtypingimportAnyfrompyatlan.client.atlanimportAtlanClientfrompyatlan.pkg.utilsimportget_client,set_package_headersdefload_input_file(file_path:str)->Any:"""Load and validate the input file."""# Your file loading logic herepassdefprocess_data_with_atlan(client:AtlanClient,data:Any)->None:"""Process the loaded data using Atlan client."""# Your data processing logic herepassdefmain(args:argparse.Namespace)->None:"""Main entry point for the script."""# Initialize Atlan clientclient=get_client(impersonate_user_id=args.user_id)client=set_package_headers(client)# Load and process datadata=load_input_file(args.input_file)process_data_with_atlan(client,data)if__name__=="__main__":parser=argparse.ArgumentParser(description="Script description")parser.add_argument("--user-id",required=True,help="User ID for impersonation")parser.add_argument("--input-file",required=True,help="Path to input file")args=parser.parse_args()main(args)

importargparsefromtypingimportAnyfrompyatlan.client.atlanimportAtlanClientfrompyatlan.pkg.utilsimportget_client,set_package_headersdefload_input_file(file_path:str)->Any:"""Load and validate the input file."""# Your file loading logic herepassdefprocess_data_with_atlan(client:AtlanClient,data:Any)->None:"""Process the loaded data using Atlan client."""# Your data processing logic herepassdefmain(args:argparse.Namespace)->None:"""Main entry point for the script."""# Initialize Atlan clientclient=get_client(impersonate_user_id=args.user_id)client=set_package_headers(client)# Load and process datadata=load_input_file(args.input_file)process_data_with_atlan(client,data)if__name__=="__main__":parser=argparse.ArgumentParser(description="Script description")parser.add_argument("--user-id",required=True,help="User ID for impersonation")parser.add_argument("--input-file",required=True,help="Path to input file")args=parser.parse_args()main(args)



### Step 3: Add integration testsÂ¶
(source: https://developer.atlan.com/toolkits/testing/)



#### Prerequisites: Install test dependenciesÂ¶
(source: https://developer.atlan.com/toolkits/testing/)

Before writing tests, you need to install the required testing dependencies. Choose one of the following methods:

Option 1: Install from package (recommended if available)pipinstall-e".[test]"

pipinstall-e".[test]"

pipinstall-e".[test]"

Option 2: Install explicitly with requirements file

Create arequirements-test.txtfile:

requirements-test.txt

123456

pytest>=7.4.0coverage>=7.6.1# pytest plugins (optional but recommended)pytest-order>=1.3.0pytest-sugar>=1.0.0pytest-timer[termcolor]>=1.0.0

pytest>=7.4.0coverage>=7.6.1# pytest plugins (optional but recommended)pytest-order>=1.3.0pytest-sugar>=1.0.0pytest-timer[termcolor]>=1.0.0

Install the dependencies:

pipinstall-rrequirements-test.txt

pipinstall-rrequirements-test.txt

Ready to proceed

Once dependencies are installed, you can proceed to write your integration tests.



#### Test layout fortest_main.pyÂ¶
(source: https://developer.atlan.com/toolkits/testing/)

test_main.py

Create atests/folder if not already present:

tests/

scripts/
â””â”€â”€ my_script/
    â”œâ”€â”€ main.py
    â””â”€â”€ tests/
        â””â”€â”€ test_main.py

scripts/
â””â”€â”€ my_script/
    â”œâ”€â”€ main.py
    â””â”€â”€ tests/
        â””â”€â”€ test_main.py

test_main_functions

test_main

main()

test_after_main

Example Reference:For a complete real-world example, see the integration test fordesignation_based_group_provisioning/main.py.

designation_based_group_provisioning/main.py



### Recommended testing strategy for scriptsÂ¶
(source: https://developer.atlan.com/toolkits/testing/)

When writingintegration testsfor scripts inmarketplace-csa-scripts, follow these practices to ensure reliable and production-relevant test coverage:

marketplace-csa-scripts



#### Best practicesÂ¶
(source: https://developer.atlan.com/toolkits/testing/)

âœ… DO:

Test against real Atlan tenants- Integration tests should interact with actual Atlan instances to validate real behavior

Use environment variablesfor all secrets and configuration values

Load configuration safelyvia.envfiles, CI/CD secrets, or shell configs â€” never hardcode sensitive data

.env

ğŸ”„ MOCK ONLY WHEN NECESSARY:

Use mocking or patching sparingly, and only for:

External/third-party API calls(non-Atlan services)

Database interactionsnot managed by Atlan

Non-deterministic behavior(e.g., random data, time-based logic)

âŒ AVOID:

Mockingpyatlanclients or any Atlan interactions unless absolutely necessary

pyatlan



#### Common pitfalls to avoidÂ¶
(source: https://developer.atlan.com/toolkits/testing/)

âŒDon't hardcode sensitive values

Never hardcode API keys, user-specific secrets, or test asset names

Instead:Use environment variables andpyatlan.test_utils.TestId.make_unique()for unique naming

pyatlan.test_utils.TestId.make_unique()

Best practice:Generate test objects in fixtures for reusability and proper cleanup

âŒDon't use fake data

Avoid placeholder data that doesn't reflect real Atlan entity structures

Instead:Use data that closely mirrors production for meaningful tests

âŒDon't mock Atlan client methods

Integration tests must executereal operationsagainst live Atlan tenants

Why:Mocking undermines the purpose of integration testing and may miss regressions

Remember:You're testing the integration, not the individual components

123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125

importpytestfromtypesimportSimpleNamespacefrompyatlan.pkg.utilsimportget_client,set_package_headersimportpandasaspdfromscripts.designation_based_group_provisioning.mainimport(review_groups,get_default_groups,get_ungrouped_users,map_users_by_designation,main,)frompyatlan.model.groupimportAtlanGroup,CreateGroupResponsefrompyatlan.client.atlanimportAtlanClientfrompyatlan.test_utilsimportTestIdfromtypingimportGeneratorimportosfrompathlibimportPathTEST_PATH=Path(__file__).parentTEST_GROUP_NAME=TestId.make_unique("csa-dbgp-test")@pytest.fixture(scope="module")defconfig()->SimpleNamespace:returnSimpleNamespace(user_id=os.environ.get("ATLAN_USER_ID"),mapping_file=f"{TEST_PATH}/test_mapping.csv",missing_groups_handler="SKIP",remove_from_default_group="",domain_name="mock-tenant.atlan.com",)@pytest.fixture(scope="module")defclient(config):ifconfig.user_id:client=get_client(impersonate_user_id=config.user_id)else:client=AtlanClient()client=set_package_headers(client)returnclient@pytest.fixture(scope="module")defgroup(client:AtlanClient)->Generator[CreateGroupResponse,None,None]:to_create=AtlanGroup.create(TEST_GROUP_NAME)g=client.group.create(group=to_create)# Read the CSV filedf=pd.read_csv(f"{TEST_PATH}/mapping.csv")# Replace values in the 'GROUP_NAME' column with the test group namedf["GROUP_NAME"]=df["GROUP_NAME"].replace("Data Engineers and Scientists",TEST_GROUP_NAME)# Save the updated test CSVdf.to_csv(f"{TEST_PATH}/test_mapping.csv",index=False)assertos.path.exists(f"{TEST_PATH}/test_mapping.csv")yieldgclient.group.purge(g.group)os.remove(f"{TEST_PATH}/test_mapping.csv")deftest_main_functions(config:SimpleNamespace,client:AtlanClient,group:AtlanGroup,caplog:pytest.LogCaptureFixture,):# Test configuration validationassertconfig.mapping_file.endswith(".csv")# Test group review functionalityverified_groups=review_groups(config.mapping_file,config.missing_groups_handler,client)assertcaplog.records[0].levelname=="INFO"assert"-> Source information procured."incaplog.records[0].messageassertisinstance(verified_groups,set)default_groups=get_default_groups(client)assertcaplog.records[6].levelname=="INFO"assert"DEFAULT groups found:"incaplog.records[6].messageassertisinstance(default_groups,list)andlen(default_groups)>0groupless_users=get_ungrouped_users(default_groups=default_groups,client=client)assertisinstance(groupless_users,list)andlen(groupless_users)>0unmappable_users=map_users_by_designation(user_list=groupless_users,mapping_file=config.mapping_file,verified_groups=verified_groups,client=client,)assertisinstance(unmappable_users,list)andlen(unmappable_users)>0deftest_main(config:SimpleNamespace,client:AtlanClient,group:AtlanGroup,caplog:pytest.LogCaptureFixture,):# Test end-to-end main function executionmain(config)# Verify expected log messagesassertcaplog.records[0].levelname=="INFO"assert"SDK Client initialized for tenant"incaplog.records[0].messageassert"Input file path -"incaplog.records[1].messageassert"-> Source information procured."incaplog.records[2].messageassert"Total distinct groups in the input:"incaplog.records[3].message@pytest.mark.order(after="test_main")deftest_after_main(client:AtlanClient,group:CreateGroupResponse):result=client.group.get_by_name(TEST_GROUP_NAME)assertresultandlen(result)==1test_group=result[0]asserttest_group.pathasserttest_group.nameasserttest_group.id==group.groupasserttest_group.attributesassertnottest_group.attributes.description# Make sure users are successfully assigned# to the test group after running the workflowasserttest_group.user_countandtest_group.user_count>=1

importpytestfromtypesimportSimpleNamespacefrompyatlan.pkg.utilsimportget_client,set_package_headersimportpandasaspdfromscripts.designation_based_group_provisioning.mainimport(review_groups,get_default_groups,get_ungrouped_users,map_users_by_designation,main,)frompyatlan.model.groupimportAtlanGroup,CreateGroupResponsefrompyatlan.client.atlanimportAtlanClientfrompyatlan.test_utilsimportTestIdfromtypingimportGeneratorimportosfrompathlibimportPathTEST_PATH=Path(__file__).parentTEST_GROUP_NAME=TestId.make_unique("csa-dbgp-test")@pytest.fixture(scope="module")defconfig()->SimpleNamespace:returnSimpleNamespace(user_id=os.environ.get("ATLAN_USER_ID"),mapping_file=f"{TEST_PATH}/test_mapping.csv",missing_groups_handler="SKIP",remove_from_default_group="",domain_name="mock-tenant.atlan.com",)@pytest.fixture(scope="module")defclient(config):ifconfig.user_id:client=get_client(impersonate_user_id=config.user_id)else:client=AtlanClient()client=set_package_headers(client)returnclient@pytest.fixture(scope="module")defgroup(client:AtlanClient)->Generator[CreateGroupResponse,None,None]:to_create=AtlanGroup.create(TEST_GROUP_NAME)g=client.group.create(group=to_create)# Read the CSV filedf=pd.read_csv(f"{TEST_PATH}/mapping.csv")# Replace values in the 'GROUP_NAME' column with the test group namedf["GROUP_NAME"]=df["GROUP_NAME"].replace("Data Engineers and Scientists",TEST_GROUP_NAME)# Save the updated test CSVdf.to_csv(f"{TEST_PATH}/test_mapping.csv",index=False)assertos.path.exists(f"{TEST_PATH}/test_mapping.csv")yieldgclient.group.purge(g.group)os.remove(f"{TEST_PATH}/test_mapping.csv")deftest_main_functions(config:SimpleNamespace,client:AtlanClient,group:AtlanGroup,caplog:pytest.LogCaptureFixture,):# Test configuration validationassertconfig.mapping_file.endswith(".csv")# Test group review functionalityverified_groups=review_groups(config.mapping_file,config.missing_groups_handler,client)assertcaplog.records[0].levelname=="INFO"assert"-> Source information procured."incaplog.records[0].messageassertisinstance(verified_groups,set)default_groups=get_default_groups(client)assertcaplog.records[6].levelname=="INFO"assert"DEFAULT groups found:"incaplog.records[6].messageassertisinstance(default_groups,list)andlen(default_groups)>0groupless_users=get_ungrouped_users(default_groups=default_groups,client=client)assertisinstance(groupless_users,list)andlen(groupless_users)>0unmappable_users=map_users_by_designation(user_list=groupless_users,mapping_file=config.mapping_file,verified_groups=verified_groups,client=client,)assertisinstance(unmappable_users,list)andlen(unmappable_users)>0deftest_main(config:SimpleNamespace,client:AtlanClient,group:AtlanGroup,caplog:pytest.LogCaptureFixture,):# Test end-to-end main function executionmain(config)# Verify expected log messagesassertcaplog.records[0].levelname=="INFO"assert"SDK Client initialized for tenant"incaplog.records[0].messageassert"Input file path -"incaplog.records[1].messageassert"-> Source information procured."incaplog.records[2].messageassert"Total distinct groups in the input:"incaplog.records[3].message@pytest.mark.order(after="test_main")deftest_after_main(client:AtlanClient,group:CreateGroupResponse):result=client.group.get_by_name(TEST_GROUP_NAME)assertresultandlen(result)==1test_group=result[0]asserttest_group.pathasserttest_group.nameasserttest_group.id==group.groupasserttest_group.attributesassertnottest_group.attributes.description# Make sure users are successfully assigned# to the test group after running the workflowasserttest_group.user_countandtest_group.user_count>=1



## Writing tests for non-toolkit based scripts using Cursor AI code editorÂ¶
(source: https://developer.atlan.com/toolkits/testing/)

You can leverage AI code editors likeCursorto help with refactoring existing scripts and generating integration tests for themarketplace-csa-scriptsrepository. However, it's important to be aware of the potential issues and risks that may arise.

marketplace-csa-scripts



### Step 1: Setup Cursor rulesÂ¶
(source: https://developer.atlan.com/toolkits/testing/)

To ensure the AI agent provides the desired results based on your prompts, you need to set up custom rules for your code editor.

Create a rules file:Create the file.cursor/rules/csa-scripts-tests.mdcin your project directory.You can start by copying theexample ruleand modifying them to match your needs.

Create a rules file:

Create the file.cursor/rules/csa-scripts-tests.mdcin your project directory.

.cursor/rules/csa-scripts-tests.mdc

You can start by copying theexample ruleand modifying them to match your needs.

example rule

Refine rules over time:As you use AI for refactoring and generating tests, you can refine the rules. By adding more context (e.g: multiple packages and varied test patterns), the AI will become more effective over time, improving its results.

Refine rules over time:

As you use AI for refactoring and generating tests, you can refine the rules. By adding more context (e.g: multiple packages and varied test patterns), the AI will become more effective over time, improving its results.



### Step 2: Running the agent with the defined RulesÂ¶
(source: https://developer.atlan.com/toolkits/testing/)

To run the AI agent with the defined rules, follow these steps:

Open the cursor chat:Presscmd + Lto open a new chat in the Cursor IDE.Click onAdd Context, then selectcsa-scripts-tests.mdcto load the rules you defined.

Open the cursor chat:

Presscmd + Lto open a new chat in the Cursor IDE.

cmd + L

Click onAdd Context, then selectcsa-scripts-tests.mdcto load the rules you defined.

Add Context

csa-scripts-tests.mdc

Provide a clear prompt:After loading the rules, provide a clear prompt like the following to refactor your script and add integration tests:Refactor `scripts/asset-change-notification/main.py` using the latest Cursor rules and add integration tests in `scripts/asset_change_notification/tests/test_main.py` to ensure functionality and coverage.

Provide a clear prompt:

After loading the rules, provide a clear prompt like the following to refactor your script and add integration tests:Refactor `scripts/asset-change-notification/main.py` using the latest Cursor rules and add integration tests in `scripts/asset_change_notification/tests/test_main.py` to ensure functionality and coverage.

Refactor `scripts/asset-change-notification/main.py` using the latest Cursor rules and add integration tests in `scripts/asset_change_notification/tests/test_main.py` to ensure functionality and coverage.

Refactor `scripts/asset-change-notification/main.py` using the latest Cursor rules and add integration tests in `scripts/asset_change_notification/tests/test_main.py` to ensure functionality and coverage.

Review results:Once the AI completes the task, review the generated results carefully. You may need to accept or reject parts of the refactoring based on your preferences and quality standards.

Review results:

Once the AI completes the task, review the generated results carefully. You may need to accept or reject parts of the refactoring based on your preferences and quality standards.



#### Common IssuesÂ¶
(source: https://developer.atlan.com/toolkits/testing/)

Low accuracy across models: 
  AI results can be highly inconsistent, even after experimenting with different combinations of rules and prompts. In many cases, only a small fraction of attempts yield satisfactory results.

Low accuracy across models: 
  AI results can be highly inconsistent, even after experimenting with different combinations of rules and prompts. In many cases, only a small fraction of attempts yield satisfactory results.

Inconsistent output: 
  Regardless of using detailed or minimal rules, and trying various AI models (Claude 3.7, Sonnet 3.5, Gemini, OpenAI), the output often lacks consistency, leading to unsatisfactory refactorings.

Inconsistent output: 
  Regardless of using detailed or minimal rules, and trying various AI models (Claude 3.7, Sonnet 3.5, Gemini, OpenAI), the output often lacks consistency, leading to unsatisfactory refactorings.

Claude 3.7, Sonnet 3.5, Gemini, OpenAI



#### Risks in refactoringÂ¶
(source: https://developer.atlan.com/toolkits/testing/)

Code deletion: 
  AI can unintentionally remove important parts of the original code during refactoring.

Code deletion: 
  AI can unintentionally remove important parts of the original code during refactoring.

Unnecessary code addition: 
  AI might add code that changes the behavior of the script, potentially introducing bugs.

Unnecessary code addition: 
  AI might add code that changes the behavior of the script, potentially introducing bugs.

Flaky or insufficient tests: 
  Generated tests are often overly simplistic or unreliable. AI may also mock components that should not be mocked, leading to incomplete test coverage.

Flaky or insufficient tests: 
  Generated tests are often overly simplistic or unreliable. AI may also mock components that should not be mocked, leading to incomplete test coverage.



## Mocking / Patching third party HTTP interactionsÂ¶
(source: https://developer.atlan.com/toolkits/testing/)



### When do you need this?Â¶
(source: https://developer.atlan.com/toolkits/testing/)

This approach is essential when building connectors or utility packages that interact with external systems, such as:

Fetching data from third-party APIs

Integrating with external databases

Calling web services that require authentication



### The problem with real API calls in testsÂ¶
(source: https://developer.atlan.com/toolkits/testing/)

âŒChallenges with direct API testing:- Requires credentials and environment configurations
- Difficult to integrate into automated test suites
- Slow execution times, especially in CI/CD pipelines
- Hard to maintain as more integrations are added
- External service availability can break tests



### The solution: VCR (Video Cassette Recorder)Â¶
(source: https://developer.atlan.com/toolkits/testing/)

âœ…Benefits of using VCR:- Record real API interactions once during development
- Replay saved responses in tests without network calls
- Fast, reliable, and reproducible tests
- Works offline and in CI environments

Thevcrpylibrary captures and saves HTTP interactions in files called"cassettes"during development.

vcrpy



### How VCR worksÂ¶
(source: https://developer.atlan.com/toolkits/testing/)

The workflow:

Recordâ†’ Run tests once with real API calls to record interactions

Saveâ†’ Store responses in local "cassette" files (YAMLorJSON)

YAML

JSON

Replayâ†’ Future test runs use saved responses instead of real HTTP requests

Customizeâ†’ Optionally modify saved responses to simulate different scenarios

The benefits:

ğŸš€Faster tests- No network latency

ğŸ”’Reliable- No dependency on external service availability

ğŸ”„Reproducible- Same responses every time

ğŸ› ï¸Configurable- Easy to simulate edge cases and error conditions

Hybrid approach

VCR sits between integration and unit tests â€” it uses real API behavior but avoids needing a live environment every time. This makes tests easier to maintain, faster to run, and more configurable as your project grows.



### Write VCR-based integration testsÂ¶
(source: https://developer.atlan.com/toolkits/testing/)

6.0.6

For this example, we are usinghttpbin.org, which provides a simple and fast way to testvcrpyby recording HTTP request and response interactions.

Before writing tests, make sure you've installed the test dependencies
in your local environment. You can do that by running the following command:

pipinstall-e".[test]"

pipinstall-e".[test]"

Alternatively, you can explicitly install the required packages
by creating arequirements-test.txtfile and installing them using:

requirements-test.txt

12345678910

pytest>=7.4.0
coverage>=7.6.1
# pytest plugins (optional but recommended) 
pytest-order>=1.3.0
pytest-sugar>=1.0.0
pytest-timer[termcolor]>=1.0.0
pytest-vcr~=1.0.2
# pinned vcrpy to v6.x since vcrpy>=7.0 requires urllib3>=2.0
# which breaks compatibility with Python 3.8
vcrpy~=6.0.2

pytest>=7.4.0
coverage>=7.6.1
# pytest plugins (optional but recommended) 
pytest-order>=1.3.0
pytest-sugar>=1.0.0
pytest-timer[termcolor]>=1.0.0
pytest-vcr~=1.0.2
# pinned vcrpy to v6.x since vcrpy>=7.0 requires urllib3>=2.0
# which breaks compatibility with Python 3.8
vcrpy~=6.0.2

123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081

importpytestimportrequestsimportosfrompyatlan.test_utils.base_vcrimportBaseVCR# (1)classTestHTTPBin(BaseVCR):"""Integration tests to demonstrate VCR.py capabilitiesby recording and replaying HTTP interactions usingHTTPBin (https://httpbin.org) for GET, POST, PUT, and DELETE requests."""BASE_URL="https://httpbin.org"@pytest.fixture(scope="module")# (2)defvcr_config(self):"""Override the VCR configuration to use JSON serialization across the module."""config=self._BASE_CONFIG.copy()config.update({"serializer":"pretty-json"})returnconfig@pytest.fixture(scope="module")defvcr_cassette_dir(self,request):# (3)"""Override the directory path for storing VCR cassettes.If a custom cassette directory is set in the class, it is used;otherwise, the default directory structure is created under "tests/cassettes"."""returnself._CASSETTES_DIRoros.path.join("tests/vcr_cassettes",request.module.__name__)@pytest.mark.vcr()deftest_httpbin_get(self):# (4)"""Test a simple GET request to httpbin."""url=f"{self.BASE_URL}/get"response=requests.get(url,params={"test":"value"})assertresponse.status_code==200assertresponse.json()["args"]["test"]=="value"@pytest.mark.vcr()deftest_httpbin_post(self):"""Test a simple POST request to httpbin."""url=f"{self.BASE_URL}/post"payload={"name":"atlan","type":"integration-test"}response=requests.post(url,json=payload)assertresponse.status_code==200assertresponse.json()["json"]==payload@pytest.mark.vcr()deftest_httpbin_put(self):"""Test a simple PUT request to httpbin."""url=f"{self.BASE_URL}/put"payload={"update":"value"}response=requests.put(url,json=payload)assertresponse.status_code==200assertresponse.json()["json"]==payload@pytest.mark.vcr()deftest_httpbin_delete(self):"""Test a simple DELETE request to httpbin."""url=f"{self.BASE_URL}/delete"response=requests.delete(url)assertresponse.status_code==200# HTTPBin returns an empty JSON object for DELETEassertresponse.json()["args"]=={}

importpytestimportrequestsimportosfrompyatlan.test_utils.base_vcrimportBaseVCR# (1)classTestHTTPBin(BaseVCR):"""Integration tests to demonstrate VCR.py capabilitiesby recording and replaying HTTP interactions usingHTTPBin (https://httpbin.org) for GET, POST, PUT, and DELETE requests."""BASE_URL="https://httpbin.org"@pytest.fixture(scope="module")# (2)defvcr_config(self):"""Override the VCR configuration to use JSON serialization across the module."""config=self._BASE_CONFIG.copy()config.update({"serializer":"pretty-json"})returnconfig@pytest.fixture(scope="module")defvcr_cassette_dir(self,request):# (3)"""Override the directory path for storing VCR cassettes.If a custom cassette directory is set in the class, it is used;otherwise, the default directory structure is created under "tests/cassettes"."""returnself._CASSETTES_DIRoros.path.join("tests/vcr_cassettes",request.module.__name__)@pytest.mark.vcr()deftest_httpbin_get(self):# (4)"""Test a simple GET request to httpbin."""url=f"{self.BASE_URL}/get"response=requests.get(url,params={"test":"value"})assertresponse.status_code==200assertresponse.json()["args"]["test"]=="value"@pytest.mark.vcr()deftest_httpbin_post(self):"""Test a simple POST request to httpbin."""url=f"{self.BASE_URL}/post"payload={"name":"atlan","type":"integration-test"}response=requests.post(url,json=payload)assertresponse.status_code==200assertresponse.json()["json"]==payload@pytest.mark.vcr()deftest_httpbin_put(self):"""Test a simple PUT request to httpbin."""url=f"{self.BASE_URL}/put"payload={"update":"value"}response=requests.put(url,json=payload)assertresponse.status_code==200assertresponse.json()["json"]==payload@pytest.mark.vcr()deftest_httpbin_delete(self):"""Test a simple DELETE request to httpbin."""url=f"{self.BASE_URL}/delete"response=requests.delete(url)assertresponse.status_code==200# HTTPBin returns an empty JSON object for DELETEassertresponse.json()["args"]=={}

Start by importing theBaseVCRclass frompyatlan.test_utils.base_vcr,
which already includes base/default configurations for VCR-based tests, such asvcr_config,vcr_cassette_dir, and custom serializers likepretty-yaml(default for cassettes) andpretty-json(another cassette format).

Start by importing theBaseVCRclass frompyatlan.test_utils.base_vcr,
which already includes base/default configurations for VCR-based tests, such asvcr_config,vcr_cassette_dir, and custom serializers likepretty-yaml(default for cassettes) andpretty-json(another cassette format).

BaseVCR

pyatlan.test_utils.base_vcr

vcr_config

vcr_cassette_dir

pretty-yaml

pretty-json

(Optional) To override any defaultvcr_config(), you can redefine the@pytest.fixture->vcr_config()inside your test class.
For example, you can update the serializer to use the custompretty-jsonserializer.

(Optional) To override any defaultvcr_config(), you can redefine the@pytest.fixture->vcr_config()inside your test class.
For example, you can update the serializer to use the custompretty-jsonserializer.

vcr_config()

@pytest.fixture

vcr_config()

pretty-json

(Optional) To override the defaultcassette directory path,
you can redefine the@pytest.fixture->vcr_cassette_dir()inside your test class.

(Optional) To override the defaultcassette directory path,
you can redefine the@pytest.fixture->vcr_cassette_dir()inside your test class.

@pytest.fixture

vcr_cassette_dir()

When writing tests (e.gtest_my_scenario), make sure to add the@pytest.mark.vcr()decorator to mark them as VCR test cases. For each test case, a separate cassette (HTTP recording) will created inside thetests/vcr_cassettes/directory.

When writing tests (e.gtest_my_scenario), make sure to add the@pytest.mark.vcr()decorator to mark them as VCR test cases. For each test case, a separate cassette (HTTP recording) will created inside thetests/vcr_cassettes/directory.

test_my_scenario

@pytest.mark.vcr()

tests/vcr_cassettes/

Once you run all the tests using:

pytesttests/integration/test_http_bin.py

pytesttests/integration/test_http_bin.py

Since this is the first time running them, vcrpy will record all the HTTP interactions automatically and save them into thetests/vcr_cassettes/directory

tests/vcr_cassettes/

For example, here's a saved cassette for theTestHTTPBin.test_httpbin_posttest:

TestHTTPBin.test_httpbin_post

123456789101112131415161718192021222324252627282930313233343536373839

interactions:-request:body:|-{"name": "atlan","type": "integration-test"}headers:{}method:POSTuri:https://httpbin.org/postresponse:body:string:|-{"args": {},"data": "{\"name\": \"atlan\", \"type\": \"integration-test\"}","files": {},"form": {},"headers": {"Accept": "*/*","Accept-Encoding": "gzip, deflate","Content-Length": "45","Content-Type": "application/json","Host": "httpbin.org","User-Agent": "python-requests/2.32.3","X-Amzn-Trace-Id": "Root=1-680f7290-276efa7f015f83d24d9fdfc4"},"json": {"name": "atlan","type": "integration-test"},"origin": "x.x.x.x","url": "https://httpbin.org/post"}headers:{}status:code:200message:OKversion:1

interactions:-request:body:|-{"name": "atlan","type": "integration-test"}headers:{}method:POSTuri:https://httpbin.org/postresponse:body:string:|-{"args": {},"data": "{\"name\": \"atlan\", \"type\": \"integration-test\"}","files": {},"form": {},"headers": {"Accept": "*/*","Accept-Encoding": "gzip, deflate","Content-Length": "45","Content-Type": "application/json","Host": "httpbin.org","User-Agent": "python-requests/2.32.3","X-Amzn-Trace-Id": "Root=1-680f7290-276efa7f015f83d24d9fdfc4"},"json": {"name": "atlan","type": "integration-test"},"origin": "x.x.x.x","url": "https://httpbin.org/post"}headers:{}status:code:200message:OKversion:1

vcrpy

There might be cases where VCR.py's recorded responses are not sufficient for your testing needs, even after applying custom configurations. In such scenarios, you can switch to using Python's built-inmock/patch objectlibrary for greater flexibility and control over external dependencies.



## Containerizing marketplace scriptsÂ¶
(source: https://developer.atlan.com/toolkits/testing/)



### OverviewÂ¶
(source: https://developer.atlan.com/toolkits/testing/)

When your script is ready for production deployment, you'll need to create package-specific Docker images for reliable and consistent execution across different environments.

Why containerize?- âœ…Consistent executionacross all environments
- âœ…Proper versioningand rollback capability
- âœ…Isolated dependenciesprevent conflicts
- âœ…Automated deploymentvia CI/CD pipelines



### PrerequisitesÂ¶
(source: https://developer.atlan.com/toolkits/testing/)

Complete these steps first

Before containerizing your script, ensure you have:

âœ…Completed script refactoringfrom theWriting tests for non-toolkit based scriptssection

âœ…Working integration teststhat validate your script functionality

âœ…Script directory renamedtosnake_caseformat (if applicable)

snake_case



### Required files for containerizationÂ¶
(source: https://developer.atlan.com/toolkits/testing/)

File checklist

For each package script (e.gscripts/designation_based_group_provisioning/), you need to create5 essential files:

scripts/designation_based_group_provisioning/

ğŸ“version.txt- Semantic versioning

version.txt

ğŸ³Dockerfile- Container image definition

Dockerfile

ğŸ“¦requirements.txt- Package dependencies

requirements.txt

ğŸ§ªrequirements-test.txt- Testing dependencies

requirements-test.txt

ğŸ”’ Vulnerability scan (usingsnykCLI)

snyk

Let's create each file step by step:



#### 1.version.txt- semantic versioningÂ¶
(source: https://developer.atlan.com/toolkits/testing/)

version.txt

Create a version file to track your package releases:

1

1.0.0dev

1.0.0dev

Semantic versioning guidelines

You should use.devsuffix for development

.dev

Followsemantic versioningprinciples:

MAJORversion: incompatible API changes

MINORversion: backwards-compatible functionality additions

PATCHversion: backwards-compatible bug fixes



#### 2.Dockerfile- package-specific imageÂ¶
(source: https://developer.atlan.com/toolkits/testing/)

Dockerfile

Create a production-ready Docker image for your script:

123456789101112131415161718192021222324252627282930313233

# Use the latest pyatlan-wolfi-base imageFROMghcr.io/atlanhq/pyatlan-wolfi-base:8.0.1-3.13# Build argumentsARGPKG_DIRARGAPP_DIR=/app/designation_based_group_provisioning# Container metadataLABELorg.opencontainers.image.vendor="Atlan Pte. Ltd."\org.opencontainers.image.source="https://github.com/atlanhq/marketplace-csa-scripts"\org.opencontainers.image.description="Atlan image for designation_based_group_provisioning custom package."\org.opencontainers.image.licenses="Apache-2.0"# Switch to root for package installationUSERroot# Copy and install package requirementsCOPY${PKG_DIR}/requirements.txtrequirements.txt# Install additional requirements system-wide with cachingRUN--mount=type=cache,target=/root/.cache/uv\uvpipinstall--system-rrequirements.txt&&\rmrequirements.txt# Copy application code and utilitiesCOPY${PKG_DIR}${APP_DIR}/COPYutils/app/scripts/utils/# Switch back to nonroot user for securityUSERnonroot# Set working directoryWORKDIR/app

# Use the latest pyatlan-wolfi-base imageFROMghcr.io/atlanhq/pyatlan-wolfi-base:8.0.1-3.13# Build argumentsARGPKG_DIRARGAPP_DIR=/app/designation_based_group_provisioning# Container metadataLABELorg.opencontainers.image.vendor="Atlan Pte. Ltd."\org.opencontainers.image.source="https://github.com/atlanhq/marketplace-csa-scripts"\org.opencontainers.image.description="Atlan image for designation_based_group_provisioning custom package."\org.opencontainers.image.licenses="Apache-2.0"# Switch to root for package installationUSERroot# Copy and install package requirementsCOPY${PKG_DIR}/requirements.txtrequirements.txt# Install additional requirements system-wide with cachingRUN--mount=type=cache,target=/root/.cache/uv\uvpipinstall--system-rrequirements.txt&&\rmrequirements.txt# Copy application code and utilitiesCOPY${PKG_DIR}${APP_DIR}/COPYutils/app/scripts/utils/# Switch back to nonroot user for securityUSERnonroot# Set working directoryWORKDIR/app

Aboutpyatlan-wolfi-base

pyatlan-wolfi-base

Usepyatlan-wolfi-baseimages for package scripts. The image is built on top ofChainguard Wolfi imagewithpyatlan. We use it because it is a vulnerability-free open source image and this image will auto-publish toghcron everypyatlanrelease (see image tag contains suffix e.g:8.0.1-3.13->pyatlan_version-python-version). If you want to use a custompyatlan-wolfi-basefor development (with differentpyatlan version,pyatlan branchorpython version) you can also do this by manually triggering theGH workflow. Following are the inputs for that workflow:

pyatlan-wolfi-base

pyatlan

ghcr

pyatlan

8.0.1-3.13

pyatlan_version-python-version

pyatlan-wolfi-base

pyatlan version

pyatlan branch

python version

Navigate toBuild Pyatlan Wolfi Base Imageworkflow

Click"Run workflow"and provide the following inputs:

main

dev

amd64

release

amd64+arm64

dev

3.13

3.11

latest

7.2.0

git://github.com/atlanhq/atlan-python.git@branch

APP-1234



#### 3.requirements.txt- package dependenciesÂ¶
(source: https://developer.atlan.com/toolkits/testing/)

requirements.txt

Generate your package dependencies usingpipreqsand include requiredOTELlogging dependencies:

pipreqs

OTEL

1234567891011

# Package-specific dependencies
# Generated via: pipreqs /path/to/pkg --force
pyatlan>=8.0.0
pandas>=2.0.0
# Add your specific dependencies here...

# Required for OpenTelemetry logging
opentelemetry-api~=1.29.0
opentelemetry-sdk~=1.29.0
opentelemetry-instrumentation-logging~=0.50b0
opentelemetry-exporter-otlp~=1.29.0

# Package-specific dependencies
# Generated via: pipreqs /path/to/pkg --force
pyatlan>=8.0.0
pandas>=2.0.0
# Add your specific dependencies here...

# Required for OpenTelemetry logging
opentelemetry-api~=1.29.0
opentelemetry-sdk~=1.29.0
opentelemetry-instrumentation-logging~=0.50b0
opentelemetry-exporter-otlp~=1.29.0

Generating requirements automatically

Usepipreqsto automatically detect and generate your package dependencies:

pipreqs

```bash
 # Install pipreqs if not already installed
 pip install pipreqs

# Generate requirements for your package
 pipreqs /path/to/your/package --force

# Example for a specific script
 pipreqs scripts/designation_based_group_provisioning --force
 ```



#### 4.requirements-test.txt- testing dependenciesÂ¶
(source: https://developer.atlan.com/toolkits/testing/)

requirements-test.txt

Create testing-specific dependencies for CI/CD and local development:

12345678910

# Minimal required for testing
coverage~=7.6.1
pytest>=7.4.0
pytest-order~=1.3.0
pytest-timer[termcolor]~=1.0.0
pytest-sugar~=1.0.0

# Add VCR support if using HTTP mocking
pytest-vcr~=1.0.2
vcrpy~=6.0.2

# Minimal required for testing
coverage~=7.6.1
pytest>=7.4.0
pytest-order~=1.3.0
pytest-timer[termcolor]~=1.0.0
pytest-sugar~=1.0.0

# Add VCR support if using HTTP mocking
pytest-vcr~=1.0.2
vcrpy~=6.0.2



#### 5. Runsnykvulnerability scan:Â¶
(source: https://developer.atlan.com/toolkits/testing/)

snyk

We also recommend running asnykvulnerability scan on your requirements so that any issues can be fixed before doing a GA release.

snyk

Step-by-step security scanning:

Authenticate with Snyk CLI:snykauthFollow the prompts to login via SSO and grant app access

Authenticate with Snyk CLI:snykauthFollow the prompts to login via SSO and grant app access

snykauth

snykauth

Scan project dependencies:# Ensure your virtual environment is active and dependencies are installedsnyktest

Scan project dependencies:# Ensure your virtual environment is active and dependencies are installedsnyktest

# Ensure your virtual environment is active and dependencies are installedsnyktest

# Ensure your virtual environment is active and dependencies are installedsnyktest

Scan Docker image (optional):# After building your Docker image locallysnykcontainertestghcr.io/atlanhq/designation_based_group_provisioning:1.0.0dev-0d35a91--file=Dockerfile

Scan Docker image (optional):# After building your Docker image locallysnykcontainertestghcr.io/atlanhq/designation_based_group_provisioning:1.0.0dev-0d35a91--file=Dockerfile

# After building your Docker image locallysnykcontainertestghcr.io/atlanhq/designation_based_group_provisioning:1.0.0dev-0d35a91--file=Dockerfile

# After building your Docker image locallysnykcontainertestghcr.io/atlanhq/designation_based_group_provisioning:1.0.0dev-0d35a91--file=Dockerfile

Create exceptions policy (if needed):

Create exceptions policy (if needed):

If there are vulnerabilities that don't impact your project, create a.snykpolicy file:

.snyk

# designation_based_group_provisioning/.snyk# Snyk (https://snyk.io) policy file, patches or ignores known issues.version:v1.0.0# ignores vulnerabilities until expiry date; change duration by modifying expiry dateignore:'snyk:lic:pip:certifi:MPL-2.0':-'*':reason:'MPL-2.0licenseisacceptableforthisproject-certifiisawidelyusedcertificatebundle'

# designation_based_group_provisioning/.snyk# Snyk (https://snyk.io) policy file, patches or ignores known issues.version:v1.0.0# ignores vulnerabilities until expiry date; change duration by modifying expiry dateignore:'snyk:lic:pip:certifi:MPL-2.0':-'*':reason:'MPL-2.0licenseisacceptableforthisproject-certifiisawidelyusedcertificatebundle'



### Development workflowÂ¶
(source: https://developer.atlan.com/toolkits/testing/)



#### Testing your containerized packageÂ¶
(source: https://developer.atlan.com/toolkits/testing/)

Use theBuild Package Test Imageworkflow for rapid development and testing:

Steps:

Navigate to the workflow:Go toBuild Package Test Image

Navigate to the workflow:Go toBuild Package Test Image

Trigger the build:Click"Run workflow"and provide the required inputs:

Trigger the build:Click"Run workflow"and provide the required inputs:

APP-001-containerize-dbgp

designation_based_group_provisioning

designation-based-group-provisioning

1.0.0-dev

The workflow will build a dev image with tag format:

ghcr.io/atlanhq/designation-based-group-provisioning:1.0.0-dev-8799072

ghcr.io/atlanhq/designation-based-group-provisioning:1.0.0-dev-8799072

Benefits of development testing

ğŸš€Rapid iteration- Test containerized changes without affecting production

ğŸ”„Environment consistency- Same container environment as production

âœ…Integration validation- Verify your script works in containerized context



### Production release workflowÂ¶
(source: https://developer.atlan.com/toolkits/testing/)



#### Step 1: Prepare for GA releaseÂ¶
(source: https://developer.atlan.com/toolkits/testing/)

Before creating your pull request:

Update version.txt: Ensure the version reflects your changes (final GA version)version.txt1.0.0

Update version.txt: Ensure the version reflects your changes (final GA version)version.txt1.0.0

1.0.0

1.0.0

Update HISTORY.md: Document all changes in this releaseHISTORY.md1234567891011## 1.0.0 (July 1, 2025)### Features...### Bug Fixes...### Breaking Changes...### QOL Improvements-Migrated package to build specific docker image.

Update HISTORY.md: Document all changes in this releaseHISTORY.md1234567891011## 1.0.0 (July 1, 2025)### Features...### Bug Fixes...### Breaking Changes...### QOL Improvements-Migrated package to build specific docker image.

1234567891011

## 1.0.0 (July 1, 2025)### Features...### Bug Fixes...### Breaking Changes...### QOL Improvements-Migrated package to build specific docker image.

## 1.0.0 (July 1, 2025)### Features...### Bug Fixes...### Breaking Changes...### QOL Improvements-Migrated package to build specific docker image.

Verify integration tests: Ensure all tests pass locallypytesttests/-sOR run tests with coverage:coveragerun-mpytesttests&&coveragereport

Verify integration tests: Ensure all tests pass locallypytesttests/-s

pytesttests/-s

pytesttests/-s

OR run tests with coverage:

coveragerun-mpytesttests&&coveragereport

coveragerun-mpytesttests&&coveragereport



#### Step 2: Create pull requestÂ¶
(source: https://developer.atlan.com/toolkits/testing/)

Create PR with your containerization changes:Include all required files (Dockerfile,version.txt,requirements.txt, etc.)Add or update integration tests following thetesting guidelinesUpdate documentation if needed

Create PR with your containerization changes:

Include all required files (Dockerfile,version.txt,requirements.txt, etc.)

Dockerfile

version.txt

requirements.txt

Add or update integration tests following thetesting guidelines

Update documentation if needed

PR validation: The automated CI pipeline will:Run unit and integration testsValidate Docker build processCheck code quality and coverageVerify all required files are present

PR validation: The automated CI pipeline will:

Run unit and integration tests

Validate Docker build process

Check code quality and coverage

Verify all required files are present

Integration tests required

If your package doesn't have integration tests, this is the perfect time to add them following thetesting toolkit guidelines. The CI pipeline expects comprehensive test coverage for production releases.



#### Step 3: Merge and deployÂ¶
(source: https://developer.atlan.com/toolkits/testing/)

Review and approval: Get your PR reviewed and approved

Merge to main: Once merged, this automatically triggers:

GA image build: Creates production image with semantic version tag

Registry publication: Publishes to GitHub Container Registry

Deployment preparation: Image becomes available for Argo template updates

Deployment preparation: Image becomes available for Argo template updates

Final GA image: Your production image will be tagged as:ghcr.io/atlanhq/designation-based-group-provisioning:1.0.0

Final GA image: Your production image will be tagged as:ghcr.io/atlanhq/designation-based-group-provisioning:1.0.0

ghcr.io/atlanhq/designation-based-group-provisioning:1.0.0

ghcr.io/atlanhq/designation-based-group-provisioning:1.0.0



#### Step 4: Update Argo templatesÂ¶
(source: https://developer.atlan.com/toolkits/testing/)

After the GA image is built, you need to update your package's Argo workflow template to use the new containerized image. This involves two main changes:

Remove the git repository artifact(scripts are now embedded in the Docker image)

Update the container configurationto use the new image and module path

Example PR:marketplace-packages/pull/18043



##### Key changes required:Â¶
(source: https://developer.atlan.com/toolkits/testing/)

inputs:artifacts:-   - name: scripts-     path: "/tmp/marketplace-csa-scripts"-     git:-       repo: git@github.com:atlanhq/marketplace-csa-scripts-       insecureIgnoreHostKey: true-       singleBranch: true-       branch: "main"-       revision: "main"-       sshPrivateKeySecret:-         name: "git-ssh"-         key: "private-key"- name: configpath: "/tmp/config"# ... other artifacts remain unchanged

inputs:artifacts:-   - name: scripts-     path: "/tmp/marketplace-csa-scripts"-     git:-       repo: git@github.com:atlanhq/marketplace-csa-scripts-       insecureIgnoreHostKey: true-       singleBranch: true-       branch: "main"-       revision: "main"-       sshPrivateKeySecret:-         name: "git-ssh"-         key: "private-key"- name: configpath: "/tmp/config"# ... other artifacts remain unchanged

container:+ image: ghcr.io/atlanhq/designation-based-group-provisioning:1.0.0imagePullPolicy: IfNotPresentenv:- name: OAUTHLIB_INSECURE_TRANSPORTvalue: "1"# ... other env vars remain unchanged- workingDir: "/tmp/marketplace-csa-scripts"command: [ "python" ]args:- "-m"-   - "scripts.designation_based_group_provisioning.main"+   - "designation_based_group_provisioning.main"

container:+ image: ghcr.io/atlanhq/designation-based-group-provisioning:1.0.0imagePullPolicy: IfNotPresentenv:- name: OAUTHLIB_INSECURE_TRANSPORTvalue: "1"# ... other env vars remain unchanged- workingDir: "/tmp/marketplace-csa-scripts"command: [ "python" ]args:- "-m"-   - "scripts.designation_based_group_provisioning.main"+   - "designation_based_group_provisioning.main"



##### Why these changes are needed:Â¶
(source: https://developer.atlan.com/toolkits/testing/)

No more git clone: Scripts are now embedded in the Docker image, eliminating the need to clone the repository at runtime

Simplified module path: Direct import from the package directory instead of the nestedscripts.path

scripts.

Cleaner execution: Container starts directly in the appropriate working directory (/app)

/app

Better security: No SSH keys needed for git access during workflow execution

Once merged, this will automatically deploy your containerized script across all Atlan tenants via theatlan-updateworkflow

atlan-update

Production deployment complete

Your script is now fully containerized and ready for production deployment across all Atlan tenants with:

âœ…Consistent execution environment

âœ…Proper versioning and rollback capability

âœ…Comprehensive testing coverage

âœ…Automated CI/CD pipeline integration



### Best practices for containerized scriptsÂ¶
(source: https://developer.atlan.com/toolkits/testing/)



#### Development practicesÂ¶
(source: https://developer.atlan.com/toolkits/testing/)

version.txt

requirements.txt

HISTORY.md



#### Security considerationsÂ¶
(source: https://developer.atlan.com/toolkits/testing/)



#### Cookie consent
(source: https://developer.atlan.com/toolkits/testing/)

We use cookies to:Anonymously measure page views, andAllow you to give us one-click feedback on any page.We donotcollect or store:Any personally identifiable information.Any information for any (re)marketing purposes.With your consent, you're helping us to make our documentation better ğŸ’™

Anonymously measure page views, and

Allow you to give us one-click feedback on any page.

Any personally identifiable information.

Any information for any (re)marketing purposes.

Google Analytics
