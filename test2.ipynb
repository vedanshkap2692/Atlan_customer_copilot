{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49fe2cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENROUTER_API_KEY = (\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82094062",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dspy\n",
    "lm = dspy.LM(\"openrouter/meta-llama/llama-4-scout\", api_key=OPENROUTER_API_KEY, cache=False, provider=\"groq\")\n",
    "dspy.configure(lm=lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55c14db9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vedansh.kapoor/atlan_project/.venv/lib/python3.11/site-packages/httpx/_models.py:408: DeprecationWarning: Use 'content=<...>' to upload raw bytes/text content.\n",
      "  headers, stream = encode_request(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Hi! How are you today? Is there something I can help you with, or would you like to chat?']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm(\"hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75ae70b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vedansh.kapoor/atlan_project/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "<frozen importlib._bootstrap>:241: DeprecationWarning: builtin type SwigPyPacked has no __module__ attribute\n",
      "<frozen importlib._bootstrap>:241: DeprecationWarning: builtin type SwigPyObject has no __module__ attribute\n",
      "<frozen importlib._bootstrap>:241: DeprecationWarning: builtin type swigvarlink has no __module__ attribute\n",
      "2025-09-13 23:08:19,904 | INFO | MarkdownRAG | Loading 1 markdown files...\n",
      "2025-09-13 23:08:19,905 | INFO | MarkdownRAG | Reading /Users/vedansh.kapoor/atlan_project/experiments/knowledge_base_resource/atlan_documentation/apps_connectors_data-warehouses_databricks_how-tos_set-up-databricks.md\n",
      "2025-09-13 23:08:19,906 | INFO | MarkdownRAG | File loaded: /Users/vedansh.kapoor/atlan_project/experiments/knowledge_base_resource/atlan_documentation/apps_connectors_data-warehouses_databricks_how-tos_set-up-databricks.md (size=39283 chars)\n",
      "2025-09-13 23:08:19,909 | INFO | MarkdownRAG | Total chunks across all files: 27\n",
      "2025-09-13 23:08:19,910 | INFO | MarkdownRAG | Loading embedding model: BAAI/bge-large-en-v1.5\n",
      "2025-09-13 23:08:19,936 | INFO | sentence_transformers.SentenceTransformer | Use pytorch device_name: mps\n",
      "2025-09-13 23:08:19,936 | INFO | sentence_transformers.SentenceTransformer | Load pretrained SentenceTransformer: BAAI/bge-large-en-v1.5\n",
      "2025-09-13 23:08:32,956 | INFO | MarkdownRAG | Embedding model loaded.\n",
      "2025-09-13 23:08:32,957 | INFO | MarkdownRAG | Preparing texts for embedding...\n",
      "2025-09-13 23:08:32,958 | INFO | MarkdownRAG | Creating embeddings for 27 chunks...\n",
      "Batches: 100%|██████████| 1/1 [00:08<00:00,  8.30s/it]\n",
      "2025-09-13 23:08:41,278 | INFO | MarkdownRAG | Embeddings shape: (27, 1024)\n",
      "2025-09-13 23:08:41,282 | INFO | MarkdownRAG | Creating FAISS index with dimension 1024\n",
      "2025-09-13 23:08:41,285 | INFO | MarkdownRAG | FAISS index populated with embeddings.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "from typing import List\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.text_splitter import MarkdownHeaderTextSplitter\n",
    "import faiss\n",
    "import numpy as np\n",
    "import dspy\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Logging configuration\n",
    "# ---------------------------------------------------------------------\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s | %(levelname)s | %(name)s | %(message)s\",\n",
    "    handlers=[logging.StreamHandler()]   # console only for notebook\n",
    ")\n",
    "logger = logging.getLogger(\"MarkdownRAG\")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 1. Load multiple markdown files\n",
    "# ---------------------------------------------------------------------\n",
    "md_file_paths = [\n",
    "\"/Users/vedansh.kapoor/atlan_project/experiments/knowledge_base_resource/atlan_documentation/apps_connectors_data-warehouses_databricks_how-tos_set-up-databricks.md\"]\n",
    "logger.info(\"Loading %d markdown files...\", len(md_file_paths))\n",
    "\n",
    "all_chunks = []\n",
    "headers_to_split_on = [(\"#\", \"Header1\"), (\"##\", \"Header2\"), (\"###\", \"Header3\")]\n",
    "splitter = MarkdownHeaderTextSplitter(headers_to_split_on, strip_headers=False)\n",
    "\n",
    "for path in md_file_paths:\n",
    "    logger.info(\"Reading %s\", path)\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        md_content = f.read()\n",
    "    logger.info(\"File loaded: %s (size=%d chars)\", path, len(md_content))\n",
    "    chunks = splitter.split_text(md_content)\n",
    "    # Store the source filename in metadata for traceability\n",
    "    for ch in chunks:\n",
    "        ch.metadata[\"source\"] = os.path.basename(path)\n",
    "    all_chunks.extend(chunks)\n",
    "\n",
    "logger.info(\"Total chunks across all files: %d\", len(all_chunks))\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 2. Load embedding model\n",
    "# ---------------------------------------------------------------------\n",
    "embed_model_name = \"BAAI/bge-large-en-v1.5\"\n",
    "logger.info(\"Loading embedding model: %s\", embed_model_name)\n",
    "embed_model = SentenceTransformer(embed_model_name)\n",
    "logger.info(\"Embedding model loaded.\")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 3. Prepare texts for embedding\n",
    "# ---------------------------------------------------------------------\n",
    "logger.info(\"Preparing texts for embedding...\")\n",
    "texts_to_embed = []\n",
    "for idx, chunk in enumerate(all_chunks):\n",
    "    headers = [chunk.metadata[h] for h in [\"Header1\", \"Header2\", \"Header3\"] if h in chunk.metadata]\n",
    "    header_text = \" | \".join(headers)\n",
    "    source_text = chunk.metadata.get(\"source\", \"\")\n",
    "    embed_text = f\"{header_text} | {source_text} | {chunk.page_content}\"\n",
    "    texts_to_embed.append(embed_text)\n",
    "    logger.debug(\"Chunk %d prepared (len=%d)\", idx, len(embed_text))\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 4. Create embeddings\n",
    "# ---------------------------------------------------------------------\n",
    "logger.info(\"Creating embeddings for %d chunks...\", len(texts_to_embed))\n",
    "embeddings = embed_model.encode(texts_to_embed, convert_to_numpy=True)\n",
    "logger.info(\"Embeddings shape: %s\", embeddings.shape)\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 5. Create and populate FAISS index\n",
    "# ---------------------------------------------------------------------\n",
    "embedding_dim = embeddings.shape[1]\n",
    "logger.info(\"Creating FAISS index with dimension %d\", embedding_dim)\n",
    "faiss_index = faiss.IndexFlatL2(embedding_dim)\n",
    "faiss_index.add(embeddings)\n",
    "logger.info(\"FAISS index populated with embeddings.\")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 6. Keep chunk metadata\n",
    "# ---------------------------------------------------------------------\n",
    "metadata_store = all_chunks\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25f00502",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-13 23:08:49,891 | INFO | mem0.vector_stores.qdrant | Inserting 1 vectors into collection mem0migrations\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "config = {\n",
    "    \"llm\": {\n",
    "        \"provider\": \"groq\",\n",
    "        \"config\": {\n",
    "            \"model\": \"openrouter/meta-llama/llama-4-scout\",\n",
    "            \"api_key\": OPENROUTER_API_KEY,   # your notebook variable\n",
    "            \n",
    "        }\n",
    "    },\n",
    "    \"embedder\": {\n",
    "        \"provider\": \"huggingface\",\n",
    "        \"config\": {\n",
    "            \"model\": \"BAAI/bge-large-en-v1.5\"\n",
    "            # no API key needed for local model\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "from mem0 import Memory\n",
    "memory = Memory.from_config(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ce12463f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-13 23:30:23,328 | INFO | MarkdownRAG | MarkdownRAG module initialized for 27 chunks.\n",
      "2025-09-13 23:30:23,330 | INFO | MarkdownRAG | MarkdownRAG ready to answer questions from 1 files.\n",
      "2025-09-13 23:30:23,402 | INFO | MarkdownRAG | Received query: how can i set up databricks with atlan's platform?\n",
      "2025-09-13 23:30:24,550 | INFO | MarkdownRAG | Retrieved 3 chunks for the query.\n",
      "2025-09-13 23:30:28,263 | INFO | MarkdownRAG | Generated answer for query.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To set up Databricks with Atlan's platform, follow these steps:\n",
      "1. Choose an authentication method: \n",
      "   - Personal access token authentication\n",
      "   - AWS service principal authentication\n",
      "   - Azure service principal authentication\n",
      "\n",
      "2. Select a cluster option:\n",
      "   - **Interactive Cluster**: Ensure an all-purpose interactive cluster is configured with Autopilot options enabled and JDBC/ODBC tab fields populated (Server Hostname, Port, and HTTP Path).\n",
      "   - **SQL Warehouse (formerly SQL endpoint)**: Ensure a SQL warehouse is configured and connection details are noted (Server hostname, Port, and HTTP path).\n",
      "\n",
      "3. **Grant Permissions to Crawl Metadata**:\n",
      "   - Ensure a Unity Catalog-enabled Databricks workspace.\n",
      "   - Grant the BROWSE privilege to the user or service principal for the catalog you want to crawl in Atlan.\n",
      "\n",
      "For detailed steps and specific configurations, refer to the [Atlan documentation for setting up Databricks](https://docs.atlan.com/apps/connectors/data-warehouses/databricks/how-tos/set-up-databricks#optional-grant-permissions-to-mine-query-history).\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------------------\n",
    "# 7. DSPy signature\n",
    "# ---------------------------------------------------------------------\n",
    "class GenerateAnswer(dspy.Signature):\n",
    "    context = dspy.InputField(desc=\"Relevant markdown context extracted from chunks\")\n",
    "    question = dspy.InputField(desc=\"User question\")\n",
    "    answer = dspy.OutputField(desc=\"Concise, accurate answer citing sources\")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 8. DSPy RAG Module\n",
    "# ---------------------------------------------------------------------\n",
    "class MarkdownRAG(dspy.Module):\n",
    "    def __init__(self, faiss_index, metadata_store, embed_model):\n",
    "        super().__init__()\n",
    "        self.faiss_index = faiss_index\n",
    "        self.metadata_store = metadata_store\n",
    "        self.embed_model = embed_model\n",
    "        self.k = 3\n",
    "        self.generate_answer = dspy.ChainOfThought(GenerateAnswer)\n",
    "        logger.info(\"MarkdownRAG module initialized for %d chunks.\", len(metadata_store))\n",
    "\n",
    "    def forward(self, question: str):\n",
    "        logger.info(\"Received query: %s\", question)\n",
    "        query_emb = self.embed_model.encode([question], convert_to_numpy=True)\n",
    "        D, I = self.faiss_index.search(query_emb, self.k)\n",
    "        logger.debug(\"FAISS search distances: %s\", D)\n",
    "        retrieved_chunks = [self.metadata_store[i] for i in I[0]]\n",
    "        logger.info(\"Retrieved %d chunks for the query.\", len(retrieved_chunks))\n",
    "\n",
    "        context_pieces = []\n",
    "        for idx, chunk in enumerate(retrieved_chunks):\n",
    "            source = chunk.metadata.get(\"source\", \"\")\n",
    "            logger.debug(\"Chunk %d source: %s\", idx, source)\n",
    "            context_pieces.append(f\"{chunk.page_content}\\n(Source: {source})\")\n",
    "        context = \"\\n---\\n\".join(context_pieces)\n",
    "\n",
    "        result = self.generate_answer(context=context, question=question)\n",
    "        logger.info(\"Generated answer for query.\")\n",
    "        return dspy.Prediction(answer=result.answer)\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 9. Notebook-friendly usage\n",
    "# ---------------------------------------------------------------------\n",
    "rag = MarkdownRAG(faiss_index, metadata_store, embed_model)\n",
    "logger.info(\"MarkdownRAG ready to answer questions from %d files.\", len(md_file_paths))\n",
    "\n",
    "# Example cell to query:\n",
    "prediction = rag(\"how can i set up databricks with atlan's platform?\")\n",
    "print(prediction.answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea795fdc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
